[
  {
    "objectID": "state-space-models.html",
    "href": "state-space-models.html",
    "title": "State space models",
    "section": "",
    "text": "The main idea behind state space models is to express constant coefficient linear differential equations as a first order vector differential equation. This representation allows us to use linear algebra to design controllers. To understand this, let’s start with a homogeneous first-order LDE: \\[\n\\frac{dy(t)}{dt} = a y(t)\n\\] with the initial condition \\(y(0) = y_0\\). We will write this equation as \\[\\bbox[5pt,border: 1px solid]\n{\\dot y(t) = a y(t), \\quad y(0) = y_0}\n\\] By inspection, we know that the solution is \\[y(t) = e^{at} y_0.\\]\nHowever, we lose this simplicity if we go to second or higher order LDEs. For example, consider the LDE: \\[\\frac{d^2y(t)}{dt^2} + a_1 \\frac{dy(t)}{dt} + a_0 y(t) = 0\\] which known initial conditions \\(y(0)\\) and \\(\\dot y(0)\\).\nHow can we find the solution? A commonly used method is to to use (one-sideded) Laplace transforms (with initial conditions) to compute the output. Recall \\[\\begin{align*}\n\\dot x(t) &\\xleftrightarrow{\\quad \\mathcal L\\quad} s X(s) - x(0) \\\\\n\\ddot x(t) &\\xleftrightarrow{\\quad \\mathcal L\\quad} s^2 X(s) - sx(0) - \\dot x(0).\n\\end{align*}\\]\nSo, if we take the LT of the DE, we get \\[\\begin{equation*}\n\\bigl[ s^2 Y(s) - s y(0) - \\dot y(0) \\bigr]\n+\na_1 \\bigl[s Y(s) - y(0) \\bigr]\n+\na_2 Y_0(s) = 0\n\\end{equation*}\\] Hence, \\[\nY(s) = \\frac{y(0) s + (a_1 y(0) + \\dot y(0))}{s^2 + a_1 s + a_0}.\n\\] We can now simplify the above using partial fractions expansion to compute \\(y(t)\\).\nState space models provide a nicer way to solve such DEs. In particular, we will provide a method to convert general \\(2\\)-nd order LDE into a vector equation \\[\\begin{align*}\n\\dot x(t) &= A x(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\] where \\(x(t) \\in \\reals^2\\), \\(A \\in \\reals^{2 × 2}\\) and \\(C \\in \\reals^{1 \\times 2}\\).\nWe will then look at general LTI systems and show that any \\(n\\)-th order TF can be written as a SSM: \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\] where \\(x(t) \\in \\reals^n\\), \\(A \\in \\reals^{n × n}\\), \\(B \\in \\reals^{n \\times 1}\\) and \\(C \\in \\reals^{1 \\times n}\\).\nIn the next lecture, we will study how to solve such differential equations for a particular choice of input."
  },
  {
    "objectID": "state-space-models.html#sec-DE-SSM",
    "href": "state-space-models.html#sec-DE-SSM",
    "title": "State space models",
    "section": "1 Converting DE to SSM",
    "text": "1 Converting DE to SSM\nIn this section, we show how to construct SSM from DE. The key point to realize is that the SSM is not unique, so we will consider different representations. Each representation is useful for a specific purpose, as we will see later in the course.\nWe assume that we are either given a normalized DE (i.e., the leading coefficient is 1): \\[\n\\frac{d^n y(t)}{dt^n} + a_{n-1} \\frac{d^{n-1}y(t)}{dt}\n+ \\cdots + a_0  y(t)\n=\nb_m \\frac{d^m u(t)}{dt^m} + \\cdots + b_0 u(t)\n\\] or a normalized TF \\[G(s) = \\frac{b_m s^m + \\cdots + b_0}{s^n + a_{n-1}s^{n-1} + \\cdots + a_0}.\\]\nWe will restrict attention to proper TFs, i.e., \\(m &lt; n\\). There are three canoncial realizations, which we illustrate via examples first.\n\nControllable Canonical Form (CCF)\nObservable Canonical Form (OCF)\nDiagonal Canonical Form (DCF)"
  },
  {
    "objectID": "state-space-models.html#illustrative-example",
    "href": "state-space-models.html#illustrative-example",
    "title": "State space models",
    "section": "2 Illustrative Example",
    "text": "2 Illustrative Example\n\n2.1 CCF representation\nConsider the homogeneous second order DE considered above: \\[\\begin{equation}\\label{eq:2nd-DE}\n\\frac{d^2y(t)}{dt^2} + a_1 \\frac{dy(t)}{dt} + a_0 y(t) = 0.\n\\end{equation}\\]\nWe consider a state vector: \\[\nx(t) = \\MATRIX{x_1(t) \\\\ x_2(t)} = \\MATRIX{y(t) \\\\ \\dot y(t)}. \\] Then, we can write \\(\\eqref{eq:2nd-DE}\\) as follows: \\[\\begin{align*}\n  \\dot x_1(t) &= x_2(t) \\\\\n  \\dot x_2(t) &= - a_0 x_1(t) - a_1 x_2(t)\n\\end{align*}\\] This equation can be written in vector form as follows: \\[\\begin{align*}\n\\MATRIX{\\dot x_1(t) \\\\ \\dot x_2(t) }\n&=\n\\MATRIX{0 & 1 \\\\ -a_0 & -a_1} \\MATRIX{x_1(t) \\\\ x_2(t) },\n\\\\\ny(t) &= \\MATRIX{\\phantom{-a_0}\\llap{1} & \\phantom{-a_1}\\llap{0} } \\MATRIX{x_1(t) \\\\ x_2(t)}.\n\\end{align*}\\] This is called controllable cannonical form (CCF) realization of \\(\\eqref{eq:2nd-DE}\\). The meaning of the terminology will become clear later.\n\n\n2.2 OCF representation\nThe second method to obtain a SSM from the LDE is as follows. Write the LDE \\(\\eqref{eq:2nd-DE}\\) as an integro-differential equation: \\[\\begin{align*}\n\\ddot y(t) &= - a_1 \\dot y(t) + a_0 y(t)\n\\\\\n\\implies \\quad\n\\dot y(t) &= - a_1  y(t) + a_0 \\int_{0^-}^t y(\\tau)d\\tau\n\\end{align*}\\] Now define \\[\\begin{align*}\nx_1(t) &= -a_0\\int_{0^-}^t y(\\tau) d\\tau ,\n&\nx_2(t) &= y(t)\n\\end{align*}\\] Then, \\[\\begin{align*}\n\\dot x_1(t) &= -a_0 y(t) = -a_0 x_2(t) \\\\\n\\dot x_2(t) &= \\dot y(t) = x_1(t) - a_1 x_2(t)\n\\end{align*}\\] This equation can be written in vector form as follows: \\[\\begin{align*}\n\\MATRIX{\\dot x_1(t) \\\\ \\dot x_2(t) }\n&=\n\\MATRIX{0 & -a_0 \\\\ 1 & -a_1} \\MATRIX{x_1(t) \\\\ x_2(t) },\n\\\\\ny(t) &= \\MATRIX{0 & \\phantom{-a_1}\\llap{1} } \\MATRIX{x_1(t) \\\\ x_2(t)}.\n\\end{align*}\\] This is called observable canoncial form (OCF) realization of \\(\\eqref{eq:2nd-DE}\\). Again, the reason for the terminolgy will become clear later.\n\n\n\n\n\n\nNon-uniqueness of the SSM realization\n\n\n\nThe above example shows that the same DE can have multiple SSM realizations."
  },
  {
    "objectID": "state-space-models.html#general-form",
    "href": "state-space-models.html#general-form",
    "title": "State space models",
    "section": "3 General form",
    "text": "3 General form\nWe now consider a general second order TF: \\[ G(s) = \\frac{b_1 s+b_0}{s^2 + a_1 s + a_0} \\]\nAs an illustration, we will also use the following numerical example:\n\nExample 1 \\[ G(s) = \\frac{s+5}{s^2 + 3s + 2} \\]\n\n\n3.1 CCF representation\nFor the generic second order system, the CCF form is \\[\nA = \\MATRIX{ 0 & 1 \\\\ \\textcolor{red}{-a_0} & \\textcolor{red}{-a_1} },\n\\quad\nB = \\MATRIX{ 0 \\\\ 1},\n\\quad\nC = \\MATRIX{\\textcolor{red}{b_0} & \\textcolor{red}{b_1} }\n\\] This is the general structure of the matrix, where only the red elements have to be filled based on the TF. For example, for the TF in Example 1, we have \\[\nA = \\MATRIX{ 0 & 1 \\\\ -2 & -3 }\n\\quad\nB = \\MATRIX{ 0 \\\\ 1},\n\\quad\nC = \\MATRIX{5 & 1}\n\\]\nIn general, we have the following:\n\n\n\n\n\n\nFigure 1: Structure of CCF representation\n\n\n\n\n\n3.2 OCF representation\nFor the generic second order system, the OCF form is \\[\nA = \\MATRIX{ 0 & \\textcolor{red}{-a_0} \\\\ 1 & \\textcolor{red}{-a_1} },\n\\quad\nB = \\MATRIX{\\textcolor{red}{b_0} \\\\ \\textcolor{red}{b_1} },\n\\quad\nC = \\MATRIX{ 0 & 1}\n\\] This is the general structure of the matrix, where only the red elements have to be filled based on the TF. For example, for the TF in Example 1, we have \\[\nA = \\MATRIX{ 0 & -2 \\\\ 1 & -3 }\n\\quad\nB = \\MATRIX{5 \\\\ 1},\n\\quad\nC = \\MATRIX{ 0 & 1},\n\\]\nIn general, we have the following:\n\n\n\n\n\n\nFigure 2: Structure of OCF representation\n\n\n\n\n\n3.3 DCF representation\nThis representation is special and only applicable when the system has distinct real roots. Then, using the partial fraction expansion, the TF may be written as \\[\nG(s) = \\frac{c_1}{s+p_1} + \\frac{c_2}{s + p_2} + \\dots + \\frac{c_n}{s+p_n}\n\\] In DCF, the system matrices are given by\n\n\n\n\n\n\nFigure 3: Structure of DCF representation\n\n\n\nIn particular, for the TF in Example 1, we have \\[\nG(s) = \\frac{s+5}{s^2 + 3s + 2} = \\frac{4}{s+1} - \\frac{3}{s+2}. \\]\nThus, \\[\nA = \\MATRIX{-1 & 0 \\\\ 0 & -2},\n\\quad\nB = \\MATRIX{ 1 \\\\ 1 },\n\\quad\nC = \\MATRIX{4 & -3}.\n\\]\nIn general, the poles will be complex and have multiplicity. In this setting, a generalization of DCF known as Jordan canconical form (JCF) is used. We will not consider JCF in this course."
  },
  {
    "objectID": "state-space-models.html#ssm-for-circuits",
    "href": "state-space-models.html#ssm-for-circuits",
    "title": "State space models",
    "section": "4 SSMs for circuits",
    "text": "4 SSMs for circuits\nWe will use the following notation in this section: \\(v(t)\\) denotes voltage, \\(i(t)\\) denotes current, and \\(q(t)\\) denotes charge.\nFor the three main circuit elements, we have the following relationship:\n\n\n\n\n\n\n\n\n\nRelationship\nResistor\nCapcitor\nInductor\n\n\n\n\n\n\n\n\n\n\nvoltage-current\n\\(v(t) = R i(t)\\)\n\\(\\displaystyle v(t) = \\frac{1}{C} \\int_{0}^t i(τ) d τ\\)\n\\(\\displaystyle \\bbox[5pt, background-color:lightgray]{v(t) = L \\frac{di(t)}{dt}}\\)\n\n\ncurrent-voltage\n\\(\\displaystyle i(t) = \\frac{1}{R} v(t)\\)\n\\(\\bbox[5pt, background-color:lightgray]{\\displaystyle i(t) = C \\frac{dv(t)}{dt}}\\)\n\\(i(t) = \\displaystyle \\frac 1L \\int_{0}^t v(τ) d τ\\)\n\n\nvoltage-charge\n\\(v(t) = R \\dfrac{dq(t)}{dt}\\)\n\\(v(t) = \\dfrac{1}{C} q(t)\\)\n\\(v(t) = L \\dfrac{d^2q(t)}{dt^2}\\)\n\n\n\nEach energy storage element (i.e., capacitor and inductor) gives rise to a state variable. We want to avoid integral equations, so for capacitor we choose voltage as state and for inductor we choose current as state.\n\n4.1 Example 1\nAs an example, let’s start with a simple example to understand how state space modeling works. Let’s consider an RCL circuit shown in Figure 4.\n\n\n\n\n\n\nFigure 4: A simple circuit\n\n\n\nWe construct the state space model as follows.\n\n4.1.1 Step 1: Select the state variables\nWe write the appropriate relation for the two energy storage elements: \\[\\begin{align*}\nC \\frac{d v_C(t)}{dt} &= i_C(t) = i_L(t) \\\\\nL \\frac{d i_L(t)}{dt} &= v_L(t)\n\\end{align*}\\]\nThus, we use \\(\\MATRIX{ v_C(t) \\\\ i_L(t) }\\) as state.\n\n\n4.1.2 Step 2: Use KCL/KVL to simplify the RHS of step 1\nWe want the RHS of the above equations to be expressed in terms of input \\(u(t)\\) and state \\((v_C(t), i_L(t))\\). Note that \\(dv_C(t)/dt\\) is already in terms of \\(i_L(t)\\). We now use KVL to find an expression for \\(v_L(t)\\) in terms of the other variables: \\[u(t) = v_R(t) + v_L(t) + v_C(t)\\] Thus, \\[\nv_L(t) = -v_R(t) - v_C(t) + u(t) = - R i_L(t) - v_C(t) + u(t).\n\\]\nThus, we have \\[\\begin{align*}\n\\frac{d v_C(t)}{dt} &= \\frac{1}{C} i_L(t) \\\\\n\\frac{d i_L(t)}{dt} &= - \\frac{1}{L} v_C(t) - \\frac{R}{L} i_L(t) + \\frac{1}{L} u(t)\n\\end{align*}\\]\nThus, the SSM is: \\[\n\\def\\1{\\vphantom{\\dfrac{d}{dt}}}\n\\bbox[5pt,background-color: lightgray]\n{\n\\begin{align*}\n\\MATRIX{ \\dfrac{d v_C(t)}{dt} \\\\ \\dfrac {d i_L(t)}{dt} }\n&=\n\\MATRIX{0 & \\dfrac{1}{C} \\\\ -\\dfrac{1}{L} & - \\dfrac{R}{L} }\n\\MATRIX{ \\1 v_C(t) \\\\  \\1 i_L(t) }\n+\n\\MATRIX{\\1 0 \\\\ \\dfrac{1}{L}} u(t)\n\\\\\ny(t) &= \\MATRIX{1 & 0}\n\\MATRIX{ \\1 v_C(t) \\\\  \\1 i_L(t) }\n\\end{align*}\n}\n\\]\nThus, we have \\[\n\\def\\1{\\vphantom{\\dfrac{d}{dt}}}\nA =\n\\MATRIX{0 & \\dfrac{1}{C} \\\\ -\\dfrac{1}{L} & - \\dfrac{R}{L} }\n,\n\\quad\nB =\n\\MATRIX{\\1 0 \\\\ \\dfrac{1}{L}},\n\\quad\nC = \\MATRIX{1 & 0 }\n\\]\n\n\n\n4.2 Example 2\nNow consider the example shown in Figure 5\n\n\n\n\n\n\nFigure 5: An RCL circuit\n\n\n\nWe construct the state space model as follows.\n\n4.2.1 Step 1: Select the state variables\nWe write the appropriate relation for the two energy storage elements: \\[\\begin{align*}\nC \\frac{d v_C(t)}{dt} &= i_C(t) \\\\\nL \\frac{d i_L(t)}{dt} &= v_L(t)\n\\end{align*}\\]\nThus, we use \\(\\MATRIX{ v_C(t) \\\\ i_L(t) }\\) as state.\n\n\n4.2.2 Step 2: Use KCL/KVL to simplify the RHS of step 1\nWe use KCL/KVL to write the right hand side of Step 1 as a linear combination of state and input. From KCL we have \\[\\begin{align*}\ni_L(t) &= i_R(t) + i_C(t) \\\\\n\\implies i_C(t) &= i_L(t) - i_R(t) \\\\\n&= i_L(t) - \\frac{1}{R} v_C(t)\n\\qquad [\\because v_C(t) = v_R(t)]\n\\end{align*}\\]\nMoreover, from KVL we have \\[\\begin{align*}\nu(t) &= v_L(t) + v_C(t) \\\\\n\\implies v_L(t) &= -v_C(t) + u(t)\n\\end{align*}\\]\nThus, we have \\[\\begin{align*}\n\\dfrac{d v_C(t)}{dt} &= \\frac{1}{C} i_c(t)\n= \\frac{1}{C} i_L(t) - \\frac{1}{RC} v_C(t) \\\\\n\\dfrac{d i_L(t)}{dt} &= \\frac{1}{L} v_L(t)\n= -\\frac{1}{L} v_C(t) + \\frac{1}{L} u(t)\n\\end{align*}\\]\nThus, the SSM is: \\[\n\\def\\1{\\vphantom{\\dfrac{d}{dt}}}\n\\bbox[5pt,background-color: lightgray]\n{\n\\begin{align*}\n\\MATRIX{ \\dfrac{d v_C(t)}{dt} \\\\ \\dfrac {d i_L(t)}{dt} }\n&=\n\\MATRIX{-\\dfrac{1}{RC} & \\dfrac{1}{C} \\\\ -\\dfrac{1}{L} & 0 }\n\\MATRIX{ \\1 v_C(t) \\\\  \\1 i_L(t) }\n+\n\\MATRIX{\\1 0 \\\\ \\dfrac{1}{L}} u(t)\n\\\\\ny(t) &= \\MATRIX{1 & 0}\n\\MATRIX{ \\1 v_C(t) \\\\  \\1 i_L(t) }\n\\end{align*}\n}\n\\]\nThus, we have \\[\n\\def\\1{\\vphantom{\\dfrac{d}{dt}}}\nA =\n\\MATRIX{-\\dfrac{1}{RC} & \\dfrac{1}{C} \\\\ -\\dfrac{1}{L} & 0 }\n,\n\\quad\nB =\n\\MATRIX{\\1 0 \\\\ \\dfrac{1}{L}},\n\\quad\nC = \\MATRIX{1 & 0 }\n\\]"
  },
  {
    "objectID": "state-space-models.html#from-state-space-models-to-transfer-functions",
    "href": "state-space-models.html#from-state-space-models-to-transfer-functions",
    "title": "State space models",
    "section": "5 From state space models to transfer functions",
    "text": "5 From state space models to transfer functions\nSo far, we have seen how to write different SSM representations from a TF. It is also possible to go in the other direction.\nSuppose we have a SSM: \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\] Then the transfer function is \\[\n  \\bbox[5pt,border: 1px solid]\n  {G(s) = C(sI - A)^{-1} B }\n\\]\n\n\n\n\n\n\nDerivation of the formula\n\n\n\n\n\nTake LT of the SSM: \\[\\begin{align*}\ns X(s) &= A X(s) + B U(s)\n& \\implies&&\nX(s) &= (sI - A)^{-1} B U(s) \\\\\nY(s) &= C X(s)\n& \\implies&&\nY(s) &= C(sI - A)^{-1} B U(s)\n\\end{align*}\\]\nHence, \\[ G(s) = \\frac{Y(s)}{U(s)} = C(sI - A)^{-1} B.\\]\n\n\n\nWe can think of this in terms of the following block diagram.\n\n\n\n\n\n\nFigure 6: Block diagram of SSM\n\n\n\nThe TF can be “read backwards” from the block diagram.\n\nExample 2 Let \\(A = \\MATRIX{-1 & 2 \\\\ 3 & -1}\\), \\(B = \\MATRIX{1 \\\\ 0}\\), and \\(C = \\MATRIX{2 & 1}\\). Find the TF?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote that \\[sI - A = \\MATRIX{s+1 & -2 \\\\ -3 & s+1}.\\]\nTherefore, \\[\\det(sI-A) = (s+1)^2 - 6 = s^2 + 2s -5. \\]\nHence, \\[(sI - A)^{-1} = \\frac{1}{\\det(sI-A)} \\MATRIX{s+1 & 2 \\\\ 3 & s+1}.\\]\nFinally, \\[G(s) = C(sI-A)^{-1}B\n=\n\\frac{1}{\\det(sI-A)} \\MATRIX{2 & 1} \\MATRIX{s+1 & 2 \\\\ 3 & s+1} \\MATRIX{1 \\\\ 0}\n= \\frac{2s + 5}{s^2 + 2s - 5}.\\]\n\n\n\nRecall that \\(\\det(sI-A)\\) is the characteristic equation of \\(A\\) and the roots of the characteristic equation are the eigenvalues of \\(A\\). Therefore, an important implication of the above formula is \\[\\bbox[5pt,border: 1px solid]\n{\\text{Poles of $G(s)$} = \\text{evals of $A$}}\\] This means that we can control the poles of the TF by controlling the eigenvalues of \\(A\\).\nChanging the eigenvalues of the \\(A\\) matrix will be one of the most simple and the most important design methods that we will learn in this course."
  },
  {
    "objectID": "state-space-models.html#change-of-coordinates",
    "href": "state-space-models.html#change-of-coordinates",
    "title": "State space models",
    "section": "6 Change of coordinates",
    "text": "6 Change of coordinates\nA key idea for controller design using state space methods is change of coordinates. To fix ideas, we first review the change of coordinates for vectors and then we will come back to SSMs.\nConsider a vector space \\(\\reals^2\\). The standard basis for \\(\\reals^2\\) is \\[\ne_1 = \\MATRIX{1 \\\\ 0}\n\\quad\\text{and}\\quad\ne_2 = \\MATRIX{0 \\\\ 1}.\n\\] When we write \\(x = \\MATRIX{x_1 \\\\ x_2} \\in \\reals^2\\), what we really mean is \\[x = x_1 e_1 + x_2 e_2.\\]\nWhat happens if we choose a different basis, say \\(v_1\\) and \\(v_2\\). If the vector \\[\n  x = \\tilde x_1 v_1 + \\tilde x_2 v_2\n\\] then we can say that the vector is equal to \\(\\MATRIX{\\tilde x_1 \\\\ \\tilde x_2}\\) in the coordinate system \\((v_1, v_2)\\). Note that the vector hasn’t changed, only the coodinate system has.\nWe use the following example to illustrate how to move from one coordinate system to another\n\nExample 3 Let \\(x = \\MATRIX{2 \\\\ 3}\\). Find its representation in the coordinate system \\[v_1 = \\MATRIX{1 \\\\ 0}\n\\quad\\text{and}\\quad\nv_2 = \\MATRIX{1 \\\\ 1}.\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet the representation in the new coordinate system be \\(\\tilde x = \\MATRIX{\\tilde x_1 \\\\ \\tilde x_2}\\). Thus, we must have \\[\n2 e_1 + 3 e_2 = \\tilde x_1 v_1 + \\tilde x_2 v_2.\n\\] Or, in vector form, \\[ \\MATRIX{1 & 0 \\\\ 0 & 1} \\MATRIX{2 \\\\ 3}\n=\n\\MATRIX{1 & 1 \\\\ 0 & 1} \\MATRIX{\\tilde x_1 \\\\ \\tilde x_2}\n\\] which can be written as \\[\\bbox[5pt,border: 1px solid]\n{ x = T \\tilde x}\n\\] where \\[T = \\MATRIX{v_1 & v_2} = \\MATRIX{1 & 1 \\\\ 0 & 1}.\\] The matrix \\(T\\) is full rank (which is always the case if \\(\\text{span}(v_1,v_2) = \\reals^2\\). Thus, we have \\[\\bbox[5pt,border: 1px solid]\n{ \\tilde x = T^{-1} x}\n\\]\nSo, for the above example we have \\[\\tilde x = \\MATRIX{1 & 1 \\\\ 0 & 1}^{-1} \\MATRIX{2 \\\\ 3}\n= \\MATRIX{-1 \\\\ 3}.\\]\n\n\n\n\n\n\n\n\n(a) Vector \\(x\\) in standard coordinate system\n\n\n\n\n\n\n\n(b) Vector \\(x\\) in oblique coordinate system\n\n\n\n\n\n\nFigure 7: Vector \\(x\\) in different coordinate systems"
  },
  {
    "objectID": "state-space-models.html#review-of-eigenvalues-and-eigenvectors",
    "href": "state-space-models.html#review-of-eigenvalues-and-eigenvectors",
    "title": "State space models",
    "section": "7 Review of eigenvalues and eigenvectors",
    "text": "7 Review of eigenvalues and eigenvectors\nFor the last several years, eigenvalues and eigenvectors are not covered in CEGEP. Therefore, students who come from CEGEP and don’t take the linear algebra course at McGill haven’t seen this material before. Eigenvalues and eigenvectors are perhaps the most fundamental idea in linear algebra. We present a quick review of this material here, but if you haven’t seen this material before, I strongly study it in detail. Some resources:\n\nUnderstanding linear algebra\n3Blue1Brown’s review\nGilbert Strang’s lecture\n\nWe can think of an \\(n × n\\) square matrix \\(A\\) as a function that takes vectors in \\(\\reals^n\\) and outputs vectors in \\(\\reals^n\\), i.e., \\[ f(x) = A x. \\]\nSuch a function is a linear function, i.e., for any scalars \\(α_1\\) and \\(α_2\\) and any vectors \\(x_1\\) and \\(x_2\\), we have \\[\nf(α_1 x_1 + α_2 x_2) = α_1 f(x_1) + α_2 f(x_2).\n\\]\nSuppose we were free to choose the coordinate system in which we evaluate \\(f\\). Which coordinate system should we choose? This is where eigenvalues and eigenvectors come in.\nAn eigenvector is a special vector \\(x\\) such that \\(f(x)\\) is a scaled (and possibly flipped) version of \\(x\\) (i.e., \\(f(x)\\) lies in the linear subspace spanned by \\(x\\)). Mathematically, there exists a scalar \\(λ\\) such that \\[ A x = λ x \\]\n\nExample 4 Consider \\(A = \\MATRIX{0 & 1 \\\\ 1 & 0}\\). This is a permultation matrix. \\[\nA \\MATRIX{x_1 \\\\ x_2} = \\MATRIX{x_2 \\\\ x_1}.\n\\]\n\nFind an eigenvector with eigenvalue \\(λ = 1\\)?\nWe want a vector \\(\\MATRIX{x_1 \\\\ x_2}\\) such that \\[\n  \\underbrace{\\MATRIX{ x_2 \\\\ x_1 }}_{A x} = \\underbrace{\\MATRIX{x_1 \\\\ x_2}}_{λ x}\n  \\] Thus, any vector of the form \\(\\MATRIX{x_1 \\\\ x_1}\\) (i.e., any vector that lies in \\(\\SPAN\\left(\\MATRIX{1 \\\\ 1}\\right)\\) is an eigenvector with eigenvalue \\(λ = 1\\).\nFind an eigenvector with eigenvalue \\(λ = -1\\)?\nWe want a vector \\(\\MATRIX{x_1 \\\\ x_2}\\) such that \\[\n  \\underbrace{\\MATRIX{ x_2 \\\\ x_1 }}_{A x} = \\underbrace{-\\MATRIX{x_1 \\\\ x_2}}_{λ x}\n  \\] Thus, any vector of the form \\(\\MATRIX{-x_1 \\\\ x_1}\\) (i.e., any vector that lies in \\(\\SPAN\\left(\\MATRIX{-1 \\\\ 1}\\right)\\) is an eigenvector with eigenvalue \\(λ = -1\\).\n\nNote that \\(A\\) has two eigenvalues \\(λ \\in \\{-1, 1\\}\\). From the definition of the eigenvectors it is clear that the eigenvectors corresponding to distinct eigenvalues must be linearly independent.\n\n\n7.1 How to compute eigenvalues and eigenvectors\nThe previous example shows how to find eigenvectors if we know the eigenvalues. But how do we find the eigenvalues? Let’s consider the equation \\[ A x = λ x \\] which can also be written as \\[ Ax = λ I x \\implies (λ I - A) x = 0.\\]\nIf there is a \\((λ, x)\\) with \\(x \\neq 0\\) that satisfies the above equation, then it must be case that \\(λI - A\\) is singular, i.e., \\[\n\\bbox[5pt,border: 1px solid]\n{\\det(λI - A) = 0}\n\\] The above equation is called the characteristic equation of \\(A\\) and the eigenvalues of \\(A\\) are the roots of the characterisitc equation!\n\n\n\n\n\n\nSome special cases\n\n\n\n\nIf \\(A\\) is a diagonal matrix, then the eigenvalues are the diagonal elements! For example, for \\(A = \\MATRIX{3 & 0 \\\\ 0 & 2}\\) we have \\[\n  \\det(λ I - A) = \\DET{ λ -3 & 0 \\\\ 0 & λ -2} = (λ-3)(λ - 2).\n\\]\nIf \\(A\\) is an upper or lower triangular matrix, then the eigenvalues are the diagonal elements! For example, for \\(A = \\MATRIX{3 & 1 \\\\ 0 & 2}\\) we have \\[\n  \\det(λ I - A) = \\DET{ λ -3 & -1 \\\\ 0 & λ -2} = (λ-3)(λ - 2).\n\\]\n\n\n\n\nExample 5 Find the eigenvalues and eigenvectors of \\(A = \\MATRIX{3 & 1 \\\\ 1 & 3 }\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe characterisitc equation of \\(A\\) is \\[\n\\det(λI - A) = \\DET{ λ - 3 & -1 \\\\ -1 & λ - 3 }\n= (λ-3)^2 - 1 = (λ-2)(λ-4).\n\\] Thus, the eigenvalues are \\(\\{2, 4\\}\\).\nTo find the eigenvectors cooresponding to \\(λ = 4\\), observe that \\[\nλ I - A = \\MATRIX{ 1 & -1 \\\\ -1 & 1}.\n\\] Thus, the eigenvector \\(x\\) must satisfy \\[\n(λI - A) x = 0\n\\implies\n\\MATRIX{ 1 & -1 \\\\ -1 & 1} \\MATRIX{ x_1 \\\\ x_2 } = 0\n\\implies\n\\MATRIX{ x_1 - x_2 \\\\ -x_1 + x_2 } = 0.\n\\] Thus, any vector of the form \\(\\MATRIX{ α \\\\ α}\\) is an eigenvector.\nTo find the eigenvector corresponding to \\(λ = 2\\), observe that \\[\nλ I - A = \\MATRIX{ -1 & -1 \\\\ -1 & -1}.\n\\] Thus, the eigenvector \\(x\\) must satisfy \\[\n(λI - A) x = 0\n\\implies\n\\MATRIX{ -1 & -1 \\\\ -1 & -1} \\MATRIX{ x_1 \\\\ x_2 } = 0\n\\implies\n\\MATRIX{ x_1 + x_2 \\\\ x_1 + x_2 } = 0.\n\\] Thus, any vector of the form \\(\\MATRIX{ -α \\\\ α}\\) is an eigenvector.\n\n\n\n\n\n\n\n\n\nWhich eigenvector to pick?\n\n\n\nFor any eigenvalue \\(λ\\), the matrix \\(λ I - A\\) is singular. Therefore, the equation \\((λI - A) x = 0\\) will have multiple solutions. Which solution to pick?\nThe standard convention (especially when you are using a mathematical programming language to compute eigenvectors) is to pick the unit vector. For example, in Example 5, one would pick:\n\nFor \\(λ = 4\\), the eigenvector is \\(\\MATRIX{ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}}\\).\nFor \\(λ = 2\\), the eigenvector is \\(\\MATRIX{ \\frac{-1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}}}\\).\n\nHowever, when doing analysis by hand, it is often more convenient to pick eigenvectors which are easier to represent. For example, in Example 5, one would pick\n\nFor \\(λ = 4\\), the eigenvector is \\(\\MATRIX{1 \\\\ 1}\\).\nFor \\(λ = 2\\), the eigenvector is \\(\\MATRIX{-1 \\\\ 1}\\).\n\n\n\n\n\n\n\n\n\nSome remarks\n\n\n\n\nNote that in Example 5, the matrix \\(A\\) is symmetric. Symmetric matrices have real eigenvalues.\nWe can show that \\[\n  λ_1 + λ_2 + \\cdots + λ_n = \\text{Trace}(A)\n\\] (where \\(\\text{Trace}(A)\\) means the sum of diagonal elements of \\(A\\) and \\[\n  λ_1 λ_2 \\cdots λ_n = \\det(A).\n\\] You can verify these properties for Example 5.\nIn general for any \\(2 × 2\\) matrix, \\[\n  \\det(λI - A) = λ^2 - \\text{Trace}(A) λ + \\det(A).\n\\]\n\n\n\nIf the matrix is not symmetric, then the eigenvalues may be complex.\n\nExample 6 (Complex eigenvalues) Find the eigenvalues of the rotation matrix \\(A = \\MATRIX{0 & -1 \\\\ 1 & 0}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe characterisic equation is \\[\n\\det(λI - A) = \\DET{λ & 1 \\\\ -1 & λ} = λ^2 + 1 = 0.\n\\] Thus, the eigenvalues are \\(\\pm j\\).\n\n\n\n\n\n7.2 Why are eigenvalues useful?\nLet’s consider an example. Suppose \\(A = \\MATRIX{3 & 1 \\\\ 1 & 3}\\). We have seen in Example 5 that the eigenvalues are \\(λ_1 = 4\\) and \\(λ_2 = 2\\) and the corresponding eigenvectors are \\[\nv_1 = \\MATRIX{1 \\\\ 1}, \\quad\nv_2 = \\MATRIX{-1 \\\\ 1}.\n\\]\nSuppose we want to compute \\(y = A x\\). Instead of working with the standard basis, let’s pick the basis as \\[\nT = \\MATRIX{v_1 & v_2} = \\MATRIX{1 & -1 \\\\ 1 & 1 }\n\\]\nWhat is the matrix \\(A\\) in the new coordinate system? We have \\(\\tilde y = T^{-1} y\\) and \\(\\tilde x = T^{-1} x\\). Thus, we have \\[\ny = A x\n\\implies\nT \\tilde y = A T \\tilde x\n\\implies\n\\tilde y = T^{-1} A T \\tilde x.\n\\] Thus, the matrix \\(A\\) in the new coordinate system is \\[\n\\tilde A = T^{-1} A T.\n\\]\nWhen \\(T = \\MATRIX{ v_1 & v_2}\\), the matrix \\(T^{-1} A T\\) has a very special form. Recall that we have \\[\nA v_1 = λ_1 v_1\n\\quad\nA v_2 = λ_2 v_2.\n\\] In matrix form, we have \\[\nA \\MATRIX{v_1 & v_2} = \\MATRIX{v_1 & v_2} \\MATRIX{λ_1 & 0 \\\\ 0 & λ_2}.\n\\] Thus, \\[\nT^{-1} A T = \\MATRIX{λ_1 & 0 \\\\ 0 & λ_2} \\Eqqcolon Λ\n\\] Thus, \\(\\tilde A = T^{-1} A T\\) is a diagonal matrix. So computing \\(\\tilde A \\tilde x\\) in the new coordinate system is trivial.\nBut there is more. Suppose we wanted to compute \\(A^{100}\\). Directly computing this by matrix multiplication is going to be very very tedious However, we can do the following. We know that \\[\nA = T Λ T^{-1}\n\\] Thus, \\[\nA^2 = T Λ T^{-1} T Λ T^{-1} = T Λ^2 T^{-1}.\n\\] Continuing this way, we have \\[\nA^3 = T Λ^3 T^{-1}\n\\] and so on. Therefore, \\[\nA^{100} = T Λ^{100} T^{-1}.\n\\] We can read this equation from right-to-left. To compute \\(A^{100}\\) we first change to the “eigen-coordinate system” (by multiplying by \\(T^{-1}\\)), we do the computation “take the 100-th power” in the eigen-coordinate system (by computing \\(Λ^{100}\\)) and then we translate the result back to the original coordinate system (by multiplying by \\(T\\)).\n\n\n7.3 Where have you seen this before?\nThe basic idea that we should always work in the eigen-coordinate system is at the heart of all we do in linear algebra, signals and systems, and differential equations (and as we shall see in this course, in control systems).\nHow are eigenvalues and eigenvectors related to Signals and Systems? Consider an LTI system shown in Figure 8\n\n\n\n\n\n\nFigure 8: An LTI system with input \\(x(t)\\), impulse response \\(g(t)\\) and output \\(y(t)\\).\n\n\n\nWe know that \\[\ny(t) = x(t) * g(t) = \\int_{-∞}^{∞} g(τ) x(t-τ) d τ.\n\\] Note that an LTI system is also a “linear transform”. So it has eigenvalues and eigenvectors. What are they?\nClaim. Complex exponentials (i.e., \\(e^{st}\\)) are eigenfunctions of LTI systems (see Figure 9)\n\n\n\n\n\n\nFigure 9: \\(G(s)\\) is the eigenvalue corresponding to \\(e^{st}\\)\n\n\n\nNote that we use the term eigenfunction instead of eigenvector because LTI systems are infinite dimensional so “vector” becomes a “function”.\nProof of claim. \\[\\begin{align*}\ny(t) &= \\int_{-∞}^{∞} g(τ) e^{s(t-τ)} d τ \\\\\n&= \\left[ \\int_{-∞}^{∞} g(τ) e^{-s τ} d τ \\right] e^{st}\n\\end{align*}\\] where the term is the square brackets is \\(G(s)\\), the Laplace transform of \\(g(t)\\).\nSo, in LTI systems, when we need to find the output corresponding to an input \\(x(t)\\), we follow the same recipe of “always work in the eigen-coordinate system”\n\nConvert \\(x(t)\\) to the new coordinate system. By the inverse Laplace transform formula, we know \\[\n  x(t) = \\int_{-∞}^{∞} X(s) e^{st} ds\n\\] (where the integral is along a straight-line contour, but we simply write \\(s\\) for notational simplicity) This is similar to the equation \\(x = T \\tilde x\\). Thus, in the new coordinate system, \\(x(t)\\) has the representation \\(X(s)\\).\nCompute the output in the new coordinate system. Since \\[\n   e^{st} \\longrightarrow G(s) e^{st}\n\\] we have by linearity\n\\[\n   X(s) e^{st} \\longrightarrow X(s) G(s) e^{st}\n\\] and therefore \\[\n   \\int_{-∞}^{∞} X(s) e^{st} \\longrightarrow \\int_{-∞}^{∞} X(s) G(s) e^{st}\n\\]\nTherefore, the output \\(y(t)\\) is the inverse Laplace transform of \\(X(s) G(s)\\). That’s all of Signals and Systems in a nutshell!"
  },
  {
    "objectID": "state-space-models.html#change-of-coordinates-in-ssms",
    "href": "state-space-models.html#change-of-coordinates-in-ssms",
    "title": "State space models",
    "section": "8 Change of coordinates in SSMs",
    "text": "8 Change of coordinates in SSMs\nConsider a SSM: \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\]\nLet \\(T\\) be any invertible \\(n × n\\) matrix. Such a matrix is called a similarity matrix. Think of columns of \\(T\\) as the basis of a new coordinate system. Then, the state can be written as \\[ \\tilde x(t) = T^{-1} x(t) \\] in the new coordinate system.\n\nWhat are the state space equations in the new coordinate system?\nHow does the change of coordinates affect the TF?\n\nWe look at both these questions in detail.\nSince \\(\\tilde x(t) = T^{-1} x(t)\\), we have \\(x(t) = T \\tilde x(t)\\). Substituting this in the SSM, we have \\[\\begin{align*}\nT \\dot {\\tilde x}(t) &= A T \\tilde x(t) + B u(t) \\\\\ny(t) &= C T \\tilde x(t)\n\\end{align*}\\]\nPre-multiply the first equation by \\(T^{-1}\\): \\[\\begin{align*}\n\\dot {\\tilde x}(t) &= T^{-1} A T \\tilde x(t) + T^{-1} B u(t) \\\\\ny(t) &= C T \\tilde x(t)\n\\end{align*}\\]\nSo, we can think of this as a new SSM: \\[\\begin{align*}\n\\dot {\\tilde x}(t) &= \\tilde A \\tilde x(t) + \\tilde B u(t) \\\\\ny(t) &= \\tilde C \\tilde x(t)\n\\end{align*}\\] where \\[\\bbox[5pt,border: 1px solid]\n{\\tilde A = T^{-1} A T,\n\\quad\n\\tilde B = T^{-1} B,\n\\quad\n\\tilde C = CT}\n\\]\nThe coordinate transformation has the following properties:\n\nCoordinate transformation does not change the characteristic function\nNote that \\[\\begin{align*}\n(sI - \\tilde A) &= sI - T^{-1} A T \\\\\n&= s T^{-1} I T - T^{-1} A T \\\\\n&= T^{-1} (sI - A) T.\n\\end{align*}\\] Therefore, \\[\n\\det(sI - \\tilde A) = \\det(T^{-1}) \\det(sI - A) \\det(T)\n= \\det(sI - A)\n\\] because \\(\\det(T^{-1}) \\det(T) = \\det(T^{-1} T) = \\det(I) = 1\\).\nCoordinate transformation does not change the TF\nThe transfer function in the new coordinate system is \\[\\begin{align*}\n\\tilde G(s) &= \\tilde C(sI - \\tilde A)^{-1} \\tilde B \\\\\n&= C T T^{-1} (sI - A)^{-1} T T^{-1} B \\\\\n&= C(sI - A)^{-1} B = G(s)\n\\end{align*}\\]\n\nThese properties hold because the change of coordinates (or similarity transformation) does not physically change the state; it only changes the coordinate system that we are using to represent the state.\nChange of coordinates (or similarity transformation) is a very useful tool for control system syntehsis. As we will see later, it is easy to design controllers for systems in CCF. So, when we have to design controllers for systems not in CCF, we first find a similarity transformation that converts the system to CCF; then we design a controller for the system in CCF; and finally we convert the controller back to the original coordinate system. Don’t worry if this doesn’t make sense notw. We will discuss this in detail in the next few lectures."
  },
  {
    "objectID": "steady-state-errors.html",
    "href": "steady-state-errors.html",
    "title": "Steady-state errors",
    "section": "",
    "text": "So far, we have investigated system stability and transient response. We now look at steady-state error.\nSteady-state error is the difference between the input (or reference) and the output for a prescribed test input as \\(t \\to ∞\\). We are typically interested in the following test signals shown below:\nSignalPlot = function(points, xdomain, ydomain) {\n  return Plot.plot({\n    grid: true,\n    x: { domain: xdomain},\n    y: { domain: ydomain},\n\n    marks: [\n      // Axes\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      // Data\n      Plot.line(points, {x: \"x\", y: \"y\"}),\n    ]\n  })\n}\n\nparabola_points = {\n  var points = new Array()\n  var idx = 0;\n  points[idx++] = { x: -1, y: 0}\n  var Δ = 0.1;\n  for(var n=0; n &lt;= 40; n++) {\n    points[idx++] = { x: n*Δ, y : 0.5*(n*Δ)**3 }\n  }\n  return points\n}\nSince we are interested in steady state errors, we restrict attention to stable systems. The formulas that we derive will not be applicable to unstable systems."
  },
  {
    "objectID": "steady-state-errors.html#steady-state-errors-for-general-systems",
    "href": "steady-state-errors.html#steady-state-errors-for-general-systems",
    "title": "Steady-state errors",
    "section": "1 Steady-state errors for general systems",
    "text": "1 Steady-state errors for general systems\n\n\n\n\n\n\nFigure 2: An open loop system\n\n\n\nConsider a system with input \\(r(t)\\), TF \\(T(s)\\) and output \\(y(t)\\). The steady state error is defined as the difference between the reference and the ouput, i.e., \\[\n  e(t) = r(t) - y(t).\n\\] We assume that \\(T(s)\\) is stable. Therefore, from the final value theorem, we have \\[\ne(∞) = \\lim_{t \\to ∞} e(t) = \\lim_{s \\to 0} s E(s)\n\\] Now observe that \\[\nE(s) = R(s) - Y(s) = R(s) - R(s)T(s) = R(s)\\bigl[ 1 - T(s) \\bigr].\n\\] Hence, \\[\n\\bbox[5pt,border: 1px solid]\n{e(∞) = \\lim_{s \\to 0} s R(s) (1 - T(s))}\n\\]\n\nExercise 1 Find the steady-state error of the open loop system with TF \\[\nT(s) = \\frac{2s + 10}{s^2 + 3s + 15}.\n\\] to a step input.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first check if the open loop system is stable. From Routh Hurwitz criteria we have\n\n\n\n1\n15\n\n\n3\n\n\n\n44/3\n\n\n\n\nSince all entries in the first column are positive, the system is stable. Therefore, we have \\[\n  e(∞) = \\lim_{s \\to 0} s R(s) (1 - T(s))\n  = 1 - T(0)\n  = \\frac{5}{15} = 0.333\n\\]\nTo confirm, we plot the step response of the above system below. From the step response, we see that the system settles around \\(0.67\\). Thus, the steady-state error is indeed \\(0.33\\) as computed.\n\nusing ControlSystems, Plots\n\nG = tf([2,10],[1,3,15])\n\nplt = plot(size=(600,300), gridalpha=0.75, minorgridalpha=0.25)\nplot!(plt, step(G))"
  },
  {
    "objectID": "steady-state-errors.html#system-type",
    "href": "steady-state-errors.html#system-type",
    "title": "Steady-state errors",
    "section": "2 System Type",
    "text": "2 System Type\nBefore looking at more general systems, we discuss the notion of a system type. A general TF can be written as \\[\n\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\nG(s) = K \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n      {\\textcolor{red}{s^k}\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\n\\] Here \\(k\\) denotes the number of poles at origin and is called the type of the system. Thus, we have\n\nType 0: \\(\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\n\\quad\nG(s) = K_p \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n    {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\\)\nType 1: \\(\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\n\\quad\nG(s) = \\dfrac{K_v}{s} \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n    {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\\)\nType 2: \\(\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\n\\quad\nG(s) = \\dfrac{K_a}{s^2} \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n    {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\\)\n\nFor historical reasons, the gain corresponding to the different system types are called:\n\nposition constant \\(K_p = \\lim_{s \\to 0} G(s)\\)\nvelocity constant \\(K_v = \\lim_{s \\to 0} s G(s)\\)\nacceleration constant \\(K_a = \\lim_{s \\to 0}s^2 G(s)\\)\n\nNote that we have the following\n\n\n\nType\n\\(K_p\\)\n\\(K_v\\)\n\\(K_a\\)\n\n\n\n\n0\nfinite\n0\n0\n\n\n1\n\\(∞\\)\nfinite\n0\n\n\n2\n\\(∞\\)\n\\(∞\\)\nfinite\n\n\n\nThese constants play an important role in understanding the steady-state error for unity feedback systems, as explained below."
  },
  {
    "objectID": "steady-state-errors.html#steady-state-errors-for-unity-feedback-systems",
    "href": "steady-state-errors.html#steady-state-errors-for-unity-feedback-systems",
    "title": "Steady-state errors",
    "section": "3 Steady-state errors for unity feedback systems",
    "text": "3 Steady-state errors for unity feedback systems\n\n\n\n\n\n\nFigure 3: A unity feedback system\n\n\n\nConsider a unity feedback system as shown in Figure 3. The closed loop transfer function is given by \\[\nT(s) = \\frac{G(s)}{1 + G(s)} \\]\nThus, from the previous formula we get\n\\[\n\\bbox[5pt,border: 1px solid]\n{e(∞) = \\lim_{s \\to 0} s R(s)(1 - T(s))\n= \\lim_{s \\to 0} \\frac{s R(s)}{1 + G(s)}}\n\\] Note that this formula is valid only if the closed loop system \\(G(s)/(1 + G(s))\\) is stable.\nNow, we specialize this expression for the three test signals described before.\n\n3.1 Step Input\nConsider the steady-state error to step input for which \\(R(s) = 1/s\\). Thus, \\[\ne_{\\rm step}(∞) = \\lim_{s \\to ∞} \\frac{s R(s)}{1 + G(s)}\n= \\frac{1}{1 + \\lim_{s \\to 0} G(s)} = \\frac{1}{1 + K_p}.\n\\]\nTherefore, we have the following:\n\n\n\nType\n\\(K_p\\)\n\\(e_{\\rm step}(∞)\\)\n\n\n\n\n0\nfinite\n\\(\\dfrac{1}{1 + K_p}\\)\n\n\n1\n\\(∞\\)\n0\n\n\n2\n\\(∞\\)\n0\n\n\n\n\n\n3.2 Ramp input\nNow consider the steady-state error to ramp input for which \\(R(s) = 1/s^2\\). Thus, \\[\ne_{\\rm ramp}(∞) = \\lim_{s \\to 0} \\frac{s R(s)}{1 + G(s)}\n= \\frac{1}{\\lim_{s \\to 0} s G(s)} = \\frac{1}{K_v}.\n\\]\nTherefore, we have the following:\n\n\n\nType\n\\(K_v\\)\n\\(e_{\\rm ramp}(∞)\\)\n\n\n\n\n0\n0\n\\(∞\\)\n\n\n1\nfinite\n\\(\\dfrac{1}{K_v}\\)\n\n\n2\n\\(∞\\)\n0\n\n\n\n\n\n3.3 Parabola input\nNow consider the steady-state error to parabola input for which \\(R(s) = 1/s^3\\). Thus, \\[\ne_{\\rm para}(∞) = \\lim_{s \\to 0} \\frac{s R(s)}{1 + G(s)}\n= \\frac{1}{\\lim_{s \\to 0} s^2 G(s)} = \\frac{1}{K_a}.\n\\]\nTherefore, we have the following:\n\n\n\nType\n\\(K_a\\)\n\\(e_{\\rm para}(∞)\\)\n\n\n\n\n0\n0\n\\(∞\\)\n\n\n1\n0\n\\(∞\\)\n\n\n2\nfinite\n\\(\\dfrac{1}{K_a}\\)\n\n\n\n\nExercise 2 Consider a unity feedback system with open-loop transfer function \\[\nG(s) = \\dfrac{100(s+3)}{(s+1)(s+6)}\n\\] Find the steady state errors to step, ramp, and parabola inputs.\n\n\n\n3.4 Summary\nIn summary, the steady-state error of different types of system for different types of inputs is shown in Table 1\n\n\n\nTable 1: Steady-state error of different types of systems for different inputs\n\n\n\n\n\nType\n\\(e_{\\rm step}(∞)\\)\n\\(e_{\\rm ramp}(∞)\\)\n\\(e_{\\rm para}(∞)\\)\n\n\n\n\n0\n\\(\\dfrac{1}{1+K_p}\\)\n\\(∞\\)\n\\(∞\\)\n\n\n1\n\\(0\\)\n\\(\\dfrac 1{K_v}\\)\n\\(∞\\)\n\n\n2\n\\(0\\)\n\\(0\\)\n\\(\\dfrac{1}{K_a}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first use Routh-Hurwitz to verify that the closed loop system is stable (left as an exercise).\nNote that \\(G(s)\\) is a type 0 system. Therefore, \\[\nK_p = \\lim_{s \\to 0} G(s) = \\frac{100 \\cdot 3 }{1 \\cdot 6 } = 50,\n\\quad\nK_v = 0,\n\\quad\nK_a = 0.\n\\] Thus, \\[\ne_{\\rm step}(∞) = \\frac{1}{1 + K_p} = \\frac{1}{51},\n\\quad\ne_{\\rm ramp}(∞) = \\frac{1}{K_v} = ∞,\n\\quad\ne_{\\rm para}(∞) = \\frac{1}{K_a} = ∞.\n\\]\n\n\n\n\nExercise 3 Consider a unity feedback system with open-loop transfer function \\[\nG(s) = \\dfrac{100(s+3)}{s(s+1)(s+6)}\n\\] Find the steady state errors to step, ramp, and parabola inputs.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first use Routh-Hurwitz to verify that the closed loop system is stable (left as an exercise).\nNote that \\(G(s)\\) is a type 0 system. Therefore, \\[\nK_p =  ∞,\n\\quad\nK_v = \\lim_{s \\to ∞} s G(s) = \\frac{100 \\cdot 3 }{1 \\cdot 6 } = 50,\n\\quad\nK_a = 0.\n\\] Thus, \\[\ne_{\\rm step}(∞) = \\frac{1}{1 + K_p} = 0,\n\\quad\ne_{\\rm ramp}(∞) = \\frac{1}{K_v} = \\frac{1}{50},\n\\quad\ne_{\\rm para}(∞) = \\frac{1}{K_a} = ∞.\n\\]\n\n\n\nSteady-state errors are often part of the system specification. Depending on the system type, a constraint on the steady state error can be translated to a constraint on the appropriate error constant, which in turn can be used as a constraint on the tuneable parameters of the controller.\n\nExercise 4 Consider the system\n\nFind the value of \\(K\\) such that the steady state error to a ramp signal is less than \\(10\\%\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe constraint that \\(r_{\\rm ramp}(∞) \\le 0.1\\) implies that \\(K_v \\ge 10\\).\nThe velocity constant of the system is given by \\[\nK_v = \\lim_{s \\to 0} s \\cdot \\frac{K}{s} \\cdot G(s) =\n\\frac{K \\cdot 5}{2 \\cdot 10} = \\frac{K}{4}.\n\\]\nThus, \\[ K_v \\ge 10 \\implies K \\ge 40. \\]\nWe now test if the specified value of the gain works.\n\nusing ControlSystems, Plots\nusing Printf\n\nK = 40\n\nP = zpk([-5],[-2,-10],1.0)\nC = tf(K,[1.0,0])\n\nT_cl = feedback(C*P)\nramp(x,t) = [t] # Input needs to be a vector\n\nres = lsim(T_cl, ramp, 5)\ne   = res.u .- res.y\n\nplt = plot(size=(600,300))\nplot!(plt, res.t, [res.y' res.u'], label=[\"y(t)\" \"u(t)\"])\nplot!(plt, res.t[end]*ones(2), [res.y[end], res.u[end]], \n           label=@sprintf(\"e(∞) = %.2f%%\", e[end]*100))"
  },
  {
    "objectID": "steady-state-errors.html#steady-state-errors-for-non-unity-feedback-systems",
    "href": "steady-state-errors.html#steady-state-errors-for-non-unity-feedback-systems",
    "title": "Steady-state errors",
    "section": "4 Steady-state errors for non-unity feedback systems",
    "text": "4 Steady-state errors for non-unity feedback systems\nConsider a system with non-unity feedback as shown above. We can compute the steady-state error using the generic formula \\[\ne(∞) = \\lim_{s \\to 0} s R(s)(1 - T(s))\n= \\lim_{s \\to 0) s R(s) \\biggl[ \\frac{1 + G(s)H(s) - G(s)}{1 + G(s)H(s)} \\biggr].\n\\] However, in doing so, we lose the intuition that we have when using error constants with unity feedback system. In this section, we show that we can get back that intuition by converting a non-unity feedback system into a unity feedback system.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nFigure 4: Unity feedback system equivalent to a non-unity feedback system\n\n\n\nFirst we observe that (a) and (b) in Figure 4 are equivalent. Then observe that (b) is equivalent to (c), which in turn is equivalent to (d) with \\[\n\\bbox[5pt,border: 1px solid]\n{G_e(s) = \\frac{G(s)}{1 + G(s)H(s) - G(s)}}\n\\]\n\nExercise 5 Consider the system\n\nFind the steady state errors to step, ramp, and parabola inputs.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case, the forward gain is \\(G(s) = 2/(s (s+2))\\) and the feedback gain \\(H(s) = 2\\). Thus,\n\\[\nG_e(s) = \\frac{ \\dfrac{2}{s(s+2)} }{1 + \\dfrac{4}{s(s+2)} - \\dfrac{2}{s(s+2)}}\n= \\frac{2}{s^2 + 2s + 2}.\n\\]\nThus, the equivalent system is a type 0 system with \\[\nK_p = \\lim_{s \\to 0} G(s) = 1,\n\\quad\nK_v = 0,\n\\quad\nK_a = 0.\n\\]\nThus, \\[\ne_{\\rm step}(∞) = \\frac{1}{1+K_p} = \\frac {1}{2},\n\\quad\ne_{\\rm ramp}(∞) = \\frac{1}{K_v} = ∞,\n\\quad\ne_{\\rm para}(∞) = \\frac{1}{K_a} = ∞.\n\\]"
  },
  {
    "objectID": "steady-state-errors.html#steady-state-errors-for-disturbances",
    "href": "steady-state-errors.html#steady-state-errors-for-disturbances",
    "title": "Steady-state errors",
    "section": "5 Steady-state errors for disturbances",
    "text": "5 Steady-state errors for disturbances\nTo be written"
  },
  {
    "objectID": "steady-state-errors.html#disturbance-rejection-for-non-unity-feedback-system",
    "href": "steady-state-errors.html#disturbance-rejection-for-non-unity-feedback-system",
    "title": "Steady-state errors",
    "section": "6 Disturbance rejection for non-unity feedback system",
    "text": "6 Disturbance rejection for non-unity feedback system\nTo be written"
  },
  {
    "objectID": "output-feedback.html",
    "href": "output-feedback.html",
    "title": "State observers and output feedback",
    "section": "",
    "text": "So far, we have assumed that the controller has access to the state of the system. In many systems, it is either too expensive or physically impossible to place enough sensors to observe all the components of the state. In such cases, one option is to estimate the state from the inputs and outputs of the system and use the estimated state to chose the control input. The system which generates a state estimate from inputs and outputs is called an observer.\nAn estimate \\(\\hat x(t)\\) of the state \\(x(t)\\) should have two properties:\n\nIt should be asymptotitcally consistent, i.e., \\(\\lim_{t \\to ∞} \\NORM{x(t) - \\hat x(t)} = 0\\).\nWe should be able to control the rate of convergence.\n\nWe state with a naive observer to show that it is possible to obtain asymptotically consistent estimators. We then consider how to modify the naive observer to be able to to control the rate of convergence.\n\n\nThe main idea of building an observer is to start with a (physical or digital) replica of the original system, provide it the same inputs as the original system, and measure the internal state of the replica. In particular, we start with an initial guess \\(\\hat x(0)\\) and then update the estimate according to \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t).\n\\]\nA feature of this observer is that if our initial guess \\(\\hat x(0)\\) is equal to the true initial conditions of the system then \\(\\hat x(t) = x(t)\\) for all \\(t\\). If the initial guess is wrong, then there is an error \\(e(t) = x(t) - \\hat x(t)\\) in measuring the state of the system. The evolution of the error is given by: \\(e(0) = x(0) - \\hat x(0)\\) and \\[\n  \\dot e(t) = \\dot x(t) - \\dot {\\hat x}(t) = A e(t).\n\\] This is a vector linear differential equation. Hence, \\[\n  e(t) = \\exp(At) e(0).\n\\] Thus, if \\(A\\) is internally stable (i.e., all the eigenvalues of \\(A\\) lie in the OLHP), then the error \\(e(t)\\) will converge to zero. However, we have no ability to influence the rate of convergence (which depends on the eigenvalues of \\(A\\)).\n\n\n\nThe Luenberger observer is given as follows. Start with an initial guess \\(\\hat x(0)\\) and update the estimate according to \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + L( y(t) - C \\hat x(t))\n\\] where \\(L = \\MATRIX{ \\ell_0 & \\cdots & \\ell_{n-1}}^\\TRANS\\) is called the observer gain.\nIn this case, the error \\(e(t) = x(t) - \\hat x(t)\\) evolves as follows. The initial state \\(e(0) = x(0) - \\hat x(0)\\) and \\[\n\\dot e(t) = \\dot x(t) - \\dot {\\hat x}(t) = (A - LC) e(t).\n\\] This is a vector linear differential equation.Hence, \\[\n  e(t) = \\exp( (A-LC)t) e(0).\n\\] Thus, if \\(A - LC\\) is stable, then the error will converge to zero with a rate that is determined by the eigenvalues of \\(A - LC\\).\nThus, designing an observer gain is similar in spirit to designing a feedback gain for state feedback. In state feedback design, we are given \\(A\\) and \\(B\\) matrices and we want to choose a gain \\(K\\) such that the eigenvalues of \\(A - BK\\) take desired values; in observer desgin, we are given \\(A\\) and \\(C\\) matrices and we want to choose a gain \\(:\\) such that the eigenvalues of \\(A - LC\\) take desired values.\nAs was the case for state feedback, we will start with system in OCF and show that observer design for such systems can be done by simply comparing the coefficients of the characteristic polynomials of the open-loop system and the desired characteristic polynomial of \\(A - LC\\). We then look at systems that are not in OCF and show that if the system satisfies a condition known as observability, we can find a transformation that converts the system to OCF, design the observer gain for the OCF representation, and translate it back to the original coordinate system."
  },
  {
    "objectID": "output-feedback.html#state-observer",
    "href": "output-feedback.html#state-observer",
    "title": "State observers and output feedback",
    "section": "",
    "text": "So far, we have assumed that the controller has access to the state of the system. In many systems, it is either too expensive or physically impossible to place enough sensors to observe all the components of the state. In such cases, one option is to estimate the state from the inputs and outputs of the system and use the estimated state to chose the control input. The system which generates a state estimate from inputs and outputs is called an observer.\nAn estimate \\(\\hat x(t)\\) of the state \\(x(t)\\) should have two properties:\n\nIt should be asymptotitcally consistent, i.e., \\(\\lim_{t \\to ∞} \\NORM{x(t) - \\hat x(t)} = 0\\).\nWe should be able to control the rate of convergence.\n\nWe state with a naive observer to show that it is possible to obtain asymptotically consistent estimators. We then consider how to modify the naive observer to be able to to control the rate of convergence.\n\n\nThe main idea of building an observer is to start with a (physical or digital) replica of the original system, provide it the same inputs as the original system, and measure the internal state of the replica. In particular, we start with an initial guess \\(\\hat x(0)\\) and then update the estimate according to \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t).\n\\]\nA feature of this observer is that if our initial guess \\(\\hat x(0)\\) is equal to the true initial conditions of the system then \\(\\hat x(t) = x(t)\\) for all \\(t\\). If the initial guess is wrong, then there is an error \\(e(t) = x(t) - \\hat x(t)\\) in measuring the state of the system. The evolution of the error is given by: \\(e(0) = x(0) - \\hat x(0)\\) and \\[\n  \\dot e(t) = \\dot x(t) - \\dot {\\hat x}(t) = A e(t).\n\\] This is a vector linear differential equation. Hence, \\[\n  e(t) = \\exp(At) e(0).\n\\] Thus, if \\(A\\) is internally stable (i.e., all the eigenvalues of \\(A\\) lie in the OLHP), then the error \\(e(t)\\) will converge to zero. However, we have no ability to influence the rate of convergence (which depends on the eigenvalues of \\(A\\)).\n\n\n\nThe Luenberger observer is given as follows. Start with an initial guess \\(\\hat x(0)\\) and update the estimate according to \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + L( y(t) - C \\hat x(t))\n\\] where \\(L = \\MATRIX{ \\ell_0 & \\cdots & \\ell_{n-1}}^\\TRANS\\) is called the observer gain.\nIn this case, the error \\(e(t) = x(t) - \\hat x(t)\\) evolves as follows. The initial state \\(e(0) = x(0) - \\hat x(0)\\) and \\[\n\\dot e(t) = \\dot x(t) - \\dot {\\hat x}(t) = (A - LC) e(t).\n\\] This is a vector linear differential equation.Hence, \\[\n  e(t) = \\exp( (A-LC)t) e(0).\n\\] Thus, if \\(A - LC\\) is stable, then the error will converge to zero with a rate that is determined by the eigenvalues of \\(A - LC\\).\nThus, designing an observer gain is similar in spirit to designing a feedback gain for state feedback. In state feedback design, we are given \\(A\\) and \\(B\\) matrices and we want to choose a gain \\(K\\) such that the eigenvalues of \\(A - BK\\) take desired values; in observer desgin, we are given \\(A\\) and \\(C\\) matrices and we want to choose a gain \\(:\\) such that the eigenvalues of \\(A - LC\\) take desired values.\nAs was the case for state feedback, we will start with system in OCF and show that observer design for such systems can be done by simply comparing the coefficients of the characteristic polynomials of the open-loop system and the desired characteristic polynomial of \\(A - LC\\). We then look at systems that are not in OCF and show that if the system satisfies a condition known as observability, we can find a transformation that converts the system to OCF, design the observer gain for the OCF representation, and translate it back to the original coordinate system."
  },
  {
    "objectID": "output-feedback.html#observer-design-for-systems-in-ocf",
    "href": "output-feedback.html#observer-design-for-systems-in-ocf",
    "title": "State observers and output feedback",
    "section": "2 Observer design for systems in OCF",
    "text": "2 Observer design for systems in OCF\nConsider a state feedback system in OCF with \\[\nA = \\MATRIX{ 0 & \\cdots & \\cdots & 0 & - a_0 \\\\\n             1 & 0 & \\cdots & 0 & - a_1 \\\\\n             0 & 1 & \\ddots & 0 & -a_2 \\\\\n             \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n             0 & \\cdots & \\cdots & 1 & -a_{n-1} }\n,\\quad\nC = \\MATRIX{0 & \\cdots & 0 & 1}.\n\\]\nWe want to design a Luenberger observer \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + L (y(t) - C \\hat x(t)),\n\\quad \\text{where }\nL = \\MATRIX{\\ell_0 \\\\ \\vdots \\\\ \\ell_{n-1}}\n\\] such that the characteristic polynomial of \\(A - LC\\) is \\[\n\\det(sI _ A + LC) = s^n + α_{n-1} s^{n-1} + \\cdots + α_0.\n\\]\nObserve that \\[\nLC = \\MATRIX{\\ell_0 \\\\ \\vdots \\ell_{n-1}}\n\\MATRIX{0 & \\cdots & 0 & 1 }\n= \\MATRIX{0  & \\cdots & 0 & - \\ell_0 \\\\\n          \\vdots & \\ddots & \\vdots & \\vdots \\\\\n          0  & \\cdots & 0 & - \\ell_{n-1}}\n\\] Thus, \\[\nA - LC = \\MATRIX{ 0 & \\cdots & \\cdots & 0 & -(a_0 + \\ell_0) \\\\\n             1 & 0 & \\cdots & 0 & - (a_1 + \\ell_1) \\\\\n             0 & 1 & \\ddots & 0 & -(a_2 + \\ell_2)\\\\\n             \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n             0 & \\cdots & \\cdots & 1 & -(a_{n-1} + \\ell_{n-1}) }\n\\] which has the OCF structure. Thus, \\[\n\\det(sI - (A - LC)) =\ns^n + (a_{n-1} + \\ell_{n-1}) s^{n-1} + \\cdots + (a_1 + \\ell_1) s + (a_0 + \\ell_0).\n\\] Observe that each component of the observer gain matrix \\(L\\) affects only one coefficient of the characteristic polynomial. Thus, we can pick \\[\n\\bbox[5pt,border: 1px solid]\n{L = \\MATRIX{ α_0 - a_0 \\\\ \\vdots \\\\ α_{n-1} - a_{n-1}} }\n\\] to place the eigenvalues of \\(A - LC\\) in the desired location.\n\nExample 1 Consider the system \\[\nA = \\MATRIX{ 0 & -1 \\\\ 1 & -2 }\n\\quad\nC = \\MATRIX{ 0 & 1}\n\\] Determine the observer gain \\(L\\) such that the eigenvalues of \\(A - LC\\) are at \\(-10 \\pm 5 j\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe desired characteristic polynomial is \\[ (s+10)^2 + 5^2 = s^2 + 20s + 125. \\]\nThe current characteristic polynomial is (since the system is in OCF, we can determine it by inspection) \\[ \\det(sI - A) = s^2 + 2s + 1. \\]\nTherefore, the required output feedback gain is \\[ L = \\MATRIX{ 125 - 1 \\\\ 20 - 2 } = \\MATRIX{ 124 \\\\ 18 }. \\]\nWe verify the solution below.\n\nA = [0 -1 ; 1 -2]\nC = [0  1]\np = [-10 + 5im, -10 - 5im]\nL = place(A',C',p)'\n\n2×1 adjoint(::Matrix{ComplexF64}) with eltype ComplexF64:\n 124.0 - 0.0im\n  18.0 - 0.0im"
  },
  {
    "objectID": "output-feedback.html#observer-design-for-systems-not-in-ocf",
    "href": "output-feedback.html#observer-design-for-systems-not-in-ocf",
    "title": "State observers and output feedback",
    "section": "3 Observer design for systems not in OCF",
    "text": "3 Observer design for systems not in OCF\nWhen a system is not in OCF, we can still use the design of systems in OCF if there is a transformation (or change of coordinates) \\(x_o(t) = T^{-1} x(t)\\) such that the transformed matrices \\((A_o, B_o, C_o)\\) are in OCF. If such a transformation can be found, then we can find the observer gain \\(L_o\\) for the system in OCF using the method of the previous section and use1 \\[\n  L = T L_o\n\\] as the observer gain for the original system. So, as was the case for state feedback, the main question is: how do we find such a transformation?\n1 The Luenberger observer for the new coordinate system is \\[\n\\dot {\\hat x_o}(t) = A_o \\hat x_o(t) + B_o u(t) + L_o(y(t) - C_o \\hat x_o(t)).\n\\] Substituting the values of \\((A_o, B_o, C_o)\\), we get \\[\\begin{align*}\n\\dot {\\hat x_o}(t)\n&= T^{-1} A T \\hat x_o(t) + T^{-1} B u(t) + L_o( y(t) - C T \\hat x_o(t)) \\\\\n&= T^{-1} A \\hat x(t) + T^{-1} B u(t) + L_o(y(t) - C \\hat x(t))\n\\end{align*}\\] where we have used the fact that \\(\\hat x_o(t) = T \\hat x(t)\\). Multiplying both sides by \\(T\\), we get \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + T L_o(y(t) - C \\hat x(t)).\n\\] Hence, \\(L = T L_o\\).\n3.1 Observability matrix\nThe observability matrix of a state space system \\((A,C)\\) is defined as \\[\n\\mathcal O_{(A,C)} = \\MATRIX{C \\\\ CA \\\\ \\vdots \\\\ C A^{n-1}}\n\\] Now consider a change of coordinates \\(x_o(t) = T^{-1} x(t)\\). Recall that the state space representation in the new coordinate system is \\[\nA_o = T^{-1} A T,\n\\quad\nC_o = C T.\n\\]\nObserve that\n\n\\(A_o^2 = T^{-1} A T T^{-1} A T = T^{-1} A^2 T\\)\n\\(A_o^3 = T^{-1} A^2 T T^{-1} A T = T^{-1} A^3 T\\)\netc.\n\nTherefore,\n\n\\(C_o A_o = C T T^{-1} A T = C A T\\)\n\\(C_o A_o^2 = C T T^{-1} A^2 T = C A^2 T\\)\netc.\n\nTherefore, the observability matrix of the system in the new coordinate system is \\[\\begin{align*}\n  \\mathcal O_{(A_o, B_o)}\n  % &= \\MATRIX{ C_o \\\\ C_o A_o \\\\ \\vdots \\\\ C_o A_o^{n-1} } \\\\\n  &= \\MATRIX{ C T \\\\ C A T \\\\ \\vdots \\\\ C A^{n-1} T } \\\\\n  &= \\MATRIX{ C  \\\\ C A  \\\\ \\vdots \\\\ C A^{n-1}  } T \\\\\n  &= \\mathcal O_{(A,C)} T.\n\\end{align*}\\]\nThus, we can go from SSM \\((A,C)\\) to SSM \\((A_o,C_o)\\) using the transformation \\[\n  T = \\mathcal O_{(A,C)}^{-1} \\mathcal O_{(A_o, C_o)}\n\\] provided \\(\\mathcal O_{(A,C)}\\) is invertible (i.e., has full rank). When that is the case, we say that the system is observable.\n\n\n3.2 Observer design\nThe general observer design problem is as follows. We are given a system \\((A,C)\\) not in OCF and we want to design a Luenberger observer \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + L (y(t) - C \\hat x(t)),\n\\quad \\text{where }\nL = \\MATRIX{\\ell_0 \\\\ \\vdots \\\\ \\ell_{n-1}}\n\\] such that the characteristic polynomial of \\(A - LC\\) is \\[\n\\det(sI _ A + LC) = s^n + α_{n-1} s^{n-1} + \\cdots + α_0.\n\\] We present two methods for observer design.\n\nMethod 1: Using observability matrix\n\nFind the observability matrix \\(\\mathcal O_{(A,C)}\\) of the system.\nIf the system is not observable, i.e., the observability matrix is not full rank (which can be tested by checking that the determinant of the observability matrix is non-zero), observer design is not possible and we quit the procedure.\nIf the system is observable, find the characteristic polynomial of \\(A\\): \\[\\det(s I - A) = s^n + a_{n-1} s^{n-1} + \\cdots + a_0 \\]\nUsing the above characteristic polynomial, write the SSM \\((A_o, C_o)\\) in OCF.\nRecall that the characteristic polynomial is the denominator of the transfer function\nCompute the observability matrix \\(\\mathcal O(A_o, C_o)\\) of the system \\((A_o, B_o)\\) in OCF.\nCompute the transformation \\[\\bbox[5pt,border: 1px solid]{\nT = \\mathcal O_{(A, C)}^{-1} \\mathcal O_{(A_o,C_o)}\n}\\]\nCompute the observer gain \\(L_o\\) for the system in OCF: \\[ L_o = \\MATRIX{ α_0 - a_0 \\\\ \\vdots \\\\ α_{n-1} - a_{n-1}}. \\]\nThe controller gain for the original system is \\[L = T L_o.\\]\n\n\n\nMethod 2: A slightly more efficient approach\nIt is possible to avoid the computation of the observability matrix of the system in OCF by replacing steps 4–6 of Method 1 by the following:\n\nCompute the matrix \\[ W = \\MATRIX{ a_1 & a_2 & \\cdots & a_{n-1} & 1 \\\\\n                a_2 & \\iddots & \\iddots & 1 & 0 \\\\\n                \\iddots & \\iddots & \\iddots & \\iddots & \\vdots \\\\\n               a_{n-1} & 1 & 0 & \\cdots & 0 \\\\\n               1 & 0 & 0 & \\cdots & 0\n              }\n\\]\nCompute the transformation: \\[\\bbox[5pt,border: 1px solid]{\nT = \\bigl[ W \\mathcal O_{(A,C)} \\bigr]^{-1}\n}\\]\n\nThis method relies on the fact that for a system \\((A_o,C_o)\\) in OCF, \\(\\mathcal O_{(A_o,C_o)} = W^{-1}\\). For a proof, see Lemma 4.5 of William and Lawrence.\n\nExample 2 Consider the following SSM: \\[ A = \\MATRIX{3 & 1 \\\\ 1 & 2}, B = \\MATRIX{2 \\\\1} \\text{ and } C = \\MATRIX{ 3 & 2}. \\] Design a Luenberger observer (i.e., compute the observer gain \\(L\\)) such that the eigenvalues of \\(A - LC\\) are at \\(-10 \\pm 5j\\)."
  },
  {
    "objectID": "output-feedback.html#output-feedback",
    "href": "output-feedback.html#output-feedback",
    "title": "State observers and output feedback",
    "section": "4 Output feedback",
    "text": "4 Output feedback\nConsider a SSM \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\]\nIn this section, we will learn how to control such a system when the controller does not have access to the internal state \\(x(t)\\), but can only observe the output \\(y(t)\\). This is called output feedback.\n\n\n\n\n\n\nFigure 1: Block diagram for output feedback\n\n\n\nThe basic idea of output feedback is to first estimate the state of the system via a Luenberger observer \\[\n\\dot {\\hat x}(t) = A \\hat x(t) + B u(t) + L (y(t) - C \\hat x(t)),\n\\quad\\text{where }\nL = \\MATRIX{\\ell_0 \\\\ \\vdots \\\\ \\ell_{n-1}}\n\\] and then use a linear feedback controller that uses the estimated state: \\[\n  u(t) = - K \\hat x(t),\n\\quad\\text{where }\nK = \\MATRIX{k_0 & \\cdots & k_{n-1}}.\n\\]\nAs before, we will first consider the system without any reference signal, i.e., \\(r(t) = 0\\). In tihs case, there are \\(2n\\) tunable gains \\(k_0, \\dots, k_{n-1}\\) and \\(\\ell_0, \\dots, \\ell_{n-1}\\) and the hope is that we can find these gain to arbitrarily choose the eigenvalues of the closed loop system.\nWriting the above equations in state space form, we get \\[\n\\MATRIX{ \\dot x(t) \\\\ \\dot {\\hat x}(t) } =\n\\MATRIX{ A & -BK \\\\ LC & A - BK - LC }\n\\MATRIX{ x(t) \\\\ \\hat x(t) }.\n\\] By using the transformation \\(T = \\MATRIX{I & 0 \\\\ I & -I}\\), we get \\[\n\\MATRIX{ \\dot x(t) \\\\ \\dot e(t) } =\n\\MATRIX{ A - BK & BK \\\\ 0 & A - LC }\n\\MATRIX{ x(t) \\\\ e(t) }.\n\\] Thus, the characteristic polynomial of the closed loop system is \\[\n\\DET{ A - BK & -BK \\\\ 0 & A - LC }  =\n\\det(sI - (A - BK)) \\det(sI - (A-LC))\n\\] where we have used the fact that the determiniant of a block diagonal upper triangular matrix is the product of the determinants of the diagonal sub-matrices.\nThus, \\[\n\\{\\text{eigenvalues of closed loop system}\\}\n= \\{\\text{eigenvalues of $(A - BK)$}\\} \\cup\n  \\{\\text{eigenvalues of $(A - LC)$}\\}.\n\\] Thus, we can separately design the controller gain \\(K\\) and the observer gain \\(L\\) by using the methods described earlier. This is called the separation principle.\nIf we choose observer poles several times faster than controller poles, the controller poles will dominate. Thus, output feedback gives essentially the same performance as (the non-implementable) state feedback."
  },
  {
    "objectID": "matrix-exponential.html",
    "href": "matrix-exponential.html",
    "title": "Time response of state space models",
    "section": "",
    "text": "Recall that for a scalar differential equation (i.e., \\(x(t) \\in \\reals\\)): \\[\n\\dot x(t) = a x(t), \\quad\n\\hbox{initial condition } x(0)=x_0\n\\] has the solution \\[ x(t) = e^{at} x_0. \\]\nCan we say the same for vector systems? That is, can we define matrix exponential \\(e^{At}\\) in such a way that the matrix differential equation (i.e., \\(x(t) \\in \\reals^n\\)): \\[\n\\dot x(t) = A x(t), \\quad\n\\hbox{initial condition } x(0) = x_0\n\\] where \\(A \\in \\reals^{n × n}\\) has the solution \\[ x(t) = e^{At} x_0? \\]\nMathematically, it turns out that this is straight forward. Recall that for a scalar \\(a\\), we have \\[e^{at} = 1 + at + \\frac{(at)^2}{2!} + \\frac{(at)^3}{3!} + \\cdots\\] So, a natural choice is to define matrix exponential as \\[\\begin{align*}\ne^{At} &= I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots\\\\\n       &= I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\n\\end{align*}\\] where the second equation uses the fact that \\((At)^2 = At At = A^2 t^2\\) because \\(t\\) is a scalar.\nWith this definition, we have \\[\\begin{align*}\n\\frac{d}{dt} e^{At} &= A + A^2 t + \\frac{A^3 t^2}{2!} + \\cdots \\\\\n&= A[I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots] \\\\\n&= A e^{At}.\n\\end{align*}\\]\nThus, if we take the candidate solution \\(x(t) = e^{At} x_0\\), we have that \\[\n\\frac{d}{dt} x(t) =\n\\left[ \\frac{d}{dt} e^{At} \\right] x_0 =\nA e^{At} x_0 = A x(t).\n\\] Thus, our candidate solution satisfies the matrix differential equation!\n\n\n\n\n\n\nIf we define \\(e^{At}\\) as \\[\\begin{equation}\\label{eq:matrix-exponential}\ne^{At} = I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\n\\end{equation}\\]\nThen, we can write the solution of the matrix differential equation (i.e., \\(x(t) \\in \\reals^n\\)) \\[\n\\dot x(t) = A x(t), \\quad\n\\hbox{initial condition } x(0) = x_0\n\\] where \\(A \\in \\reals^{n × n}\\) as \\[ x(t) = e^{At} x_0. \\]\n\n\n\nWe now present some examples where the matrix exponential can be computed easily.\n\nExample 1 Compute \\(e^{At}\\) for \\[A = \\MATRIX{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0}. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe compute the terms of \\(\\eqref{eq:matrix-exponential}\\):\n\n\\(A^2 = \\MATRIX{0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\\).\n\\(A^3 = \\MATRIX{0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\\).\n\nwhich means that \\(A^k = 0\\) for \\(k \\ge 0\\). Thus, the right hand side of \\(\\eqref{eq:matrix-exponential}\\) contains only a finite number of nonzero terms: \\[\\begin{align*}\ne^{At} &= I + At + \\frac 12 A^2 t^2 \\\\\n&=\n\\MATRIX{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 }\n+\n\\MATRIX{0 & t & 0 \\\\ 0 & 0 & t \\\\ 0 & 0 & 0}\n+\n\\frac 12\n\\MATRIX{0 & 0 & t^2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\n\\\\\n&=\\MATRIX{1 & t & \\frac 12 t^2 \\\\ 0 & 1 & t \\\\ 0 & 0 & 1}.\n\\end{align*}\\]\n\n\n\n\nExample 2 Compute \\(e^{At}\\) for \\[A = \\MATRIX{λ_1 & 0 & 0 \\\\ 0 & λ_2 & 0 \\\\ 0 & 0 & λ_3}. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe compute the terms of \\(\\eqref{eq:matrix-exponential}\\):\n\n\\(A^2 = \\MATRIX{λ_1^2 & 0 & 0 \\\\ 0 & λ_2^2 & 0 \\\\ 0 & 0 & λ_3^2}.\\)\n\\(A^3 = \\MATRIX{λ_1^3 & 0 & 0 \\\\ 0 & λ_2^3 & 0 \\\\ 0 & 0 & λ_3^3}.\\)\nand so on.\n\nThus, we have \\[\\begin{align*}\ne^{At} &= I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots \\\\\n&= \\MATRIX{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1}\n+ \\MATRIX{λ_1 t & 0 & 0 \\\\ 0 & λ_2 t & 0 \\\\ 0 & 0 & λ_3 t}\n+ \\frac 12 \\MATRIX{λ_1^2 t^2 & 0 & 0 \\\\ 0 & λ_2^2 t^2 & 0 \\\\ 0 & 0 & λ_3^2 t^2}\n+ \\cdots \\\\[10pt]\n&= \\MATRIX{1 + λ_1 t + \\frac 12 λ_1^2 t^2 + \\cdots & 0 & 0 \\\\\n           0 & 1 + λ_2 t + \\frac 12 λ_2^2 t^2 + \\cdots & 0 \\\\\n           0 & 0 & 1 + λ_3 t + \\frac 12 λ_3^2 t^2 + \\cdots} \\\\[10pt]\n&= \\MATRIX{e^{λ_1t} & 0 & 0 \\\\ 0 & e^{λ_2 t} & 0 \\\\ 0 & 0 & e^{λ_3 t}}\n\\end{align*}\\]\n\n\n\nBut outside of such few special cases, computing \\(e^{At}\\) via definition \\(\\eqref{eq:matrix-exponential}\\) is not computationally feasible. We now present computationally efficient methods to compute the matrix exponential."
  },
  {
    "objectID": "matrix-exponential.html#solution-to-matrix-differential-equations",
    "href": "matrix-exponential.html#solution-to-matrix-differential-equations",
    "title": "Time response of state space models",
    "section": "",
    "text": "Recall that for a scalar differential equation (i.e., \\(x(t) \\in \\reals\\)): \\[\n\\dot x(t) = a x(t), \\quad\n\\hbox{initial condition } x(0)=x_0\n\\] has the solution \\[ x(t) = e^{at} x_0. \\]\nCan we say the same for vector systems? That is, can we define matrix exponential \\(e^{At}\\) in such a way that the matrix differential equation (i.e., \\(x(t) \\in \\reals^n\\)): \\[\n\\dot x(t) = A x(t), \\quad\n\\hbox{initial condition } x(0) = x_0\n\\] where \\(A \\in \\reals^{n × n}\\) has the solution \\[ x(t) = e^{At} x_0? \\]\nMathematically, it turns out that this is straight forward. Recall that for a scalar \\(a\\), we have \\[e^{at} = 1 + at + \\frac{(at)^2}{2!} + \\frac{(at)^3}{3!} + \\cdots\\] So, a natural choice is to define matrix exponential as \\[\\begin{align*}\ne^{At} &= I + At + \\frac{(At)^2}{2!} + \\frac{(At)^3}{3!} + \\cdots\\\\\n       &= I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\n\\end{align*}\\] where the second equation uses the fact that \\((At)^2 = At At = A^2 t^2\\) because \\(t\\) is a scalar.\nWith this definition, we have \\[\\begin{align*}\n\\frac{d}{dt} e^{At} &= A + A^2 t + \\frac{A^3 t^2}{2!} + \\cdots \\\\\n&= A[I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots] \\\\\n&= A e^{At}.\n\\end{align*}\\]\nThus, if we take the candidate solution \\(x(t) = e^{At} x_0\\), we have that \\[\n\\frac{d}{dt} x(t) =\n\\left[ \\frac{d}{dt} e^{At} \\right] x_0 =\nA e^{At} x_0 = A x(t).\n\\] Thus, our candidate solution satisfies the matrix differential equation!\n\n\n\n\n\n\nIf we define \\(e^{At}\\) as \\[\\begin{equation}\\label{eq:matrix-exponential}\ne^{At} = I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots\n\\end{equation}\\]\nThen, we can write the solution of the matrix differential equation (i.e., \\(x(t) \\in \\reals^n\\)) \\[\n\\dot x(t) = A x(t), \\quad\n\\hbox{initial condition } x(0) = x_0\n\\] where \\(A \\in \\reals^{n × n}\\) as \\[ x(t) = e^{At} x_0. \\]\n\n\n\nWe now present some examples where the matrix exponential can be computed easily.\n\nExample 1 Compute \\(e^{At}\\) for \\[A = \\MATRIX{0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0}. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe compute the terms of \\(\\eqref{eq:matrix-exponential}\\):\n\n\\(A^2 = \\MATRIX{0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\\).\n\\(A^3 = \\MATRIX{0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\\).\n\nwhich means that \\(A^k = 0\\) for \\(k \\ge 0\\). Thus, the right hand side of \\(\\eqref{eq:matrix-exponential}\\) contains only a finite number of nonzero terms: \\[\\begin{align*}\ne^{At} &= I + At + \\frac 12 A^2 t^2 \\\\\n&=\n\\MATRIX{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 }\n+\n\\MATRIX{0 & t & 0 \\\\ 0 & 0 & t \\\\ 0 & 0 & 0}\n+\n\\frac 12\n\\MATRIX{0 & 0 & t^2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 }\n\\\\\n&=\\MATRIX{1 & t & \\frac 12 t^2 \\\\ 0 & 1 & t \\\\ 0 & 0 & 1}.\n\\end{align*}\\]\n\n\n\n\nExample 2 Compute \\(e^{At}\\) for \\[A = \\MATRIX{λ_1 & 0 & 0 \\\\ 0 & λ_2 & 0 \\\\ 0 & 0 & λ_3}. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe compute the terms of \\(\\eqref{eq:matrix-exponential}\\):\n\n\\(A^2 = \\MATRIX{λ_1^2 & 0 & 0 \\\\ 0 & λ_2^2 & 0 \\\\ 0 & 0 & λ_3^2}.\\)\n\\(A^3 = \\MATRIX{λ_1^3 & 0 & 0 \\\\ 0 & λ_2^3 & 0 \\\\ 0 & 0 & λ_3^3}.\\)\nand so on.\n\nThus, we have \\[\\begin{align*}\ne^{At} &= I + At + \\frac{A^2t^2}{2!} + \\frac{A^3t^3}{3!} + \\cdots \\\\\n&= \\MATRIX{1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1}\n+ \\MATRIX{λ_1 t & 0 & 0 \\\\ 0 & λ_2 t & 0 \\\\ 0 & 0 & λ_3 t}\n+ \\frac 12 \\MATRIX{λ_1^2 t^2 & 0 & 0 \\\\ 0 & λ_2^2 t^2 & 0 \\\\ 0 & 0 & λ_3^2 t^2}\n+ \\cdots \\\\[10pt]\n&= \\MATRIX{1 + λ_1 t + \\frac 12 λ_1^2 t^2 + \\cdots & 0 & 0 \\\\\n           0 & 1 + λ_2 t + \\frac 12 λ_2^2 t^2 + \\cdots & 0 \\\\\n           0 & 0 & 1 + λ_3 t + \\frac 12 λ_3^2 t^2 + \\cdots} \\\\[10pt]\n&= \\MATRIX{e^{λ_1t} & 0 & 0 \\\\ 0 & e^{λ_2 t} & 0 \\\\ 0 & 0 & e^{λ_3 t}}\n\\end{align*}\\]\n\n\n\nBut outside of such few special cases, computing \\(e^{At}\\) via definition \\(\\eqref{eq:matrix-exponential}\\) is not computationally feasible. We now present computationally efficient methods to compute the matrix exponential."
  },
  {
    "objectID": "matrix-exponential.html#computing-matrix-exponential",
    "href": "matrix-exponential.html#computing-matrix-exponential",
    "title": "Time response of state space models",
    "section": "2 Computing matrix exponential",
    "text": "2 Computing matrix exponential\n\n2.1 Method 1: Eigenvalue diagonalization method\nAs illustrated by Example 2, compute matrix exponential is easy for diagonal matrix. So, if the matrix \\(A\\) is diagonalizable (i.e., has distinct eigenvalues) we can do a change of coordinates and compute the matrix exponential in the eigen-coordinates.\nIn particular, suppose \\(A\\) has distinct eivenvalues \\(λ_1, \\dots, λ_n\\) and \\(v_1, \\dots, v_n\\) are the corresponding eigvenvectors. Thus, $ A v_k = λ_k v_k$ for all \\(k \\in \\{1,\\dots, n\\}\\). Writing this in matrix form, we have \\[\\begin{align*}\nA \\MATRIX{v_1 & v_2 & \\cdots & v_n} &=\n\\MATRIX{A v_1 & A v_2 & \\cdots & A v_n} \\\\\n&= \\MATRIX{λ_1 v_1 & λ_2 v_2 & \\cdots & λ_n v_n} \\\\\n&= \\MATRIX{v_1 & v_2 & \\cdots & v_n}\n\\MATRIX{\nλ_1 & 0 & \\cdots & 0 \\\\\n0 & λ_2 &  \\ddots & 0 \\\\\n0 & \\cdots & \\ddots & 0 \\\\\n0 & \\cdots & \\cdots & λ_n}\n\\end{align*}\\] Now define \\[T = \\MATRIX{v_1 & v_2 & \\cdots & v_n}\n\\quad\\text{and}\\quad\nΛ = \\MATRIX{\nλ_1 & 0 & \\cdots & 0 \\\\\n0 & λ_2 &  \\ddots & 0 \\\\\n0 & \\cdots & \\ddots & 0 \\\\\n0 & \\cdots & \\cdots & λ_n}.\n\\] So, the above equation can be writen as \\(AT = T Λ\\) or \\[\n\\bbox[5pt,border: 1px solid]\n{T^{-1} A T = Λ}\n\\] Observe that\n\n\\(A = T Λ T^{-1}\\)\n\\(A^2 = T Λ^2 T^{-1}\\)\n\\(A^3 = T Λ^3 T^{-1}\\)\nand so on.\n\nTherefore, \\[\\begin{align*}\ne^{At} &= I + At + \\frac{A^2 t^2}{2!} + \\frac{A^3 t^3}{3!} + \\cdots \\\\\n&= I + T Λ T^{-1} t + \\frac{T Λ^2 T^{-1} t^2}{2!} + \\frac{T Λ^3 T^{-1} t^3}{3!} + \\cdots \\\\\n&= T\\bigl( I + Λt + \\frac{Λ^2 t^2}{2!} + \\frac{Λ^3 t^3}{3!} + \\cdots\\bigr) T^{-1} \\\\\n&= T e^{Λ t} T^{-1}\n\\end{align*}\\]\nThus, once we know the eigenvalues and eigenvectors of \\(A\\) (and if all eigenvalues are distinct), then \\[\n\\bbox[5pt,border: 1px solid]\n{e^{At} = T e^{Λt} T^{-1}}\n\\]\n\nExercise 1 Use the eigenvalue diagonalizable method to compute \\(e^{At}\\) for \\(A = \\MATRIX{-6 & 4 \\\\ -5 & 3 }\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe start by computing the eigenvalues and eigenvectors of \\(A\\).\nTo compute the eigenvalues: \\[sI - A = \\MATRIX{s + 6 & -4 \\\\ 5 & s-3 }\\] Therefore, the characteristic equation is \\[\\det(sI - A) = s^2 + 3s + 2 = (s+1)(s+2) \\] Hence, the eigenvalues of \\(A\\) are \\(λ_1 = -1\\) and \\(λ_2 = -2\\).\nWe now compute the eigenvalues. Recall that for any eigenvalue \\(λ\\), the eigen-vector satisfies \\((λI - A) v = 0\\). We start wtih \\(λ_1 = -1\\). Then, \\[\n\\MATRIX{5 & -4 \\\\ 5 & -4 } \\MATRIX{ v_{11} \\\\ v_{12} } = 0\n\\] We set \\(v_{11} = 4\\). Then, \\(v_{12} = 5\\). Thus, the eigenvector \\[ v_1 = \\MATRIX{ 4 \\\\ 5}. \\]\nSimilarly, for \\(λ_2 = -2\\), we have \\[\n\\MATRIX{4 & -4 \\\\ 5 & -5 } \\MATRIX{ v_{21} \\\\ v_{22} }\n= 0\n\\] We set \\(v_{21} = 1\\). Then, \\(v_{22} = 1\\). Thus, the eigenvector \\[ v_2 = \\MATRIX{ 1 \\\\ 1}. \\]\nThus, \\(T = \\MATRIX{ 4 & 1 \\\\ 5 & 1 }\\) and therefore \\(T^{-1} = \\MATRIX{-1 & 1 \\\\ 5 & -4 }\\). Hence, \\[\ne^{At} = \\MATRIX{ 4 & 1 \\\\ 5 & 1 }\n\\MATRIX{e^{-t} & 0 \\\\ 0 & e^{-2t} }\n\\MATRIX{-1 & 1 \\\\ 5 & -4 } =\n\\MATRIX{ -4 e^{-t} + 5 e^{-2t} & 4 e^{-t} - 4 e^{-2t} \\\\\n         -5 e^{-t} + 5 e^{-2t} & 5 e^{-t} - 4 e^{-2t} }\n\\]\n\n\n\n\n\n2.2 Method 2: Laplace transform method\nRecall that we have \\[ \\dot x(t) = A x(t). \\] Taking (unilateral) Laplace transforms of both sides gives \\[ s X(s) - x_0 = A X(s). \\] Rearranging terms, we get \\[ (sI - A)X(s) = x_0\n\\implies X(s) = (sI-A)^{-1} x_0. \\] Taking inverse Laplace transforms, we get \\[x(t) = \\mathcal L^{-1}( (sI - A)^{-1} ) x_0. \\] Comparing it with the solution obtained earlier, we get \\[\n\\bbox[5pt,border: 1px solid]\n{ e^{At} = \\mathcal L^{-1}( (sI - A)^{-1} ).}\n\\] In the above, we interpret the inverse Laplace transform of a matrix to mean the inverse Laplace transform of each entry.\n\nExercise 2 Use the Laplace transform method to compute \\(e^{At}\\) for \\(A = \\MATRIX{-6 & 4 \\\\ -5 & 3 }\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute \\[sI - A = \\MATRIX{s + 6 & -4 \\\\ 5 & s-3 }\\] Therefore, \\[\\det(sI - A) = s^2 + 3s + 2 = (s+1)(s+2) \\] and \\[ (sI - A)^{-1} =\n\\MATRIX{\n\\dfrac{s-3}{(s+1)(s+2)} &\n\\dfrac{4}{(s+1)(s+2)} \\\\\n\\dfrac{-5}{(s+1)(s+2)} &\n\\dfrac{s+6}{(s+1)(s+2)}\n}\n\\] We now do partial fraction expansion of each term: \\[ (sI - A)^{-1} =\n\\MATRIX{\n\\dfrac{-4}{s+1} + \\dfrac{5}{s+2} &\n\\dfrac{ 4}{s+1} - \\dfrac{4}{s+2} \\\\\n\\dfrac{-5}{s+1} + \\dfrac{5}{s+2} &\n\\dfrac{5}{s+1} - \\dfrac{4}{s+2} }\n\\] Taking the inverse Laplace transform of each term, we get \\[ e^{At} = \\mathcal L^{-1}(sI - A)^{-1} =\n\\MATRIX{\n-4 e^{-t} + 5 e^{-2t} &\n4 e^{-t} - 4 e^{-2t} \\\\\n-5 e^{-t} + 5 e^{-2t} &\n5 e^{-t} - 4 e^{-2t}}\n\\]"
  },
  {
    "objectID": "matrix-exponential.html#time-response",
    "href": "matrix-exponential.html#time-response",
    "title": "Time response of state space models",
    "section": "3 Time response of state space models",
    "text": "3 Time response of state space models\nNow consider a SSM given by \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\]\nSuppose the system starts at \\(t=0\\) with an initial state \\(x(0) = x_0\\) and we apply the input \\(u(t)\\). How do we find the output?\nTaking Laplace transform of the SSM, we get: \\[\\begin{align*}\nsX(s) - x_0 &= A X(s) + B U(s) \\\\\nY(s) &= C X(s)\n\\end{align*}\\] Solving for \\(X(s)\\), we get \\[X(s) = (sI - A)^{-1} x_0 + (sI - A)^{-1} B U(s). \\] Substituting in \\(Y(s) = C X(s)\\), we get \\[\n\\bbox[5pt,border: 1px solid]\n{Y(s) = \\underbrace{C (sI - A)^{-1} x_0}_{\\text{zero-input response}}\n+ \\underbrace{C (sI - A)^{-1} B U(s)}_{\\text{zero-state response}}}\n\\] We can the use the above expression compute \\(y(t)\\) by taking the inverse Laplace transform.\nIt is sometimes useful to write the expression in time domain (but we will not use this expression for computations). To do so, recall that \\(C(sI - A)^{-1}B\\) is the transfer function \\(G(s)\\) of the system. Therefore, its inverse Laplace transform is the impulse response: \\[ g(t) = C e^{At} B. \\] Then, we can compute the inverse Laplace transform of \\(G(s) U(s)\\) using the convolution formula and write \\[\n\\bbox[5pt,border: 1px solid]\n{y(t) =\n\\underbrace{C e^{At} x_0}_{\\text{zero-input response}}\n+ \\underbrace{\\int_{0}^t C e^{A (t-τ) } B u(τ) d τ}_{\\text{zero-state response}}}\n\\]"
  },
  {
    "objectID": "bode-and-nyquist.html",
    "href": "bode-and-nyquist.html",
    "title": "Bode and Nyquist plots",
    "section": "",
    "text": "The frequency response refers to the Fourier transform of the impulse response; or equivalently, the trasfer function evaluated at \\(jω\\), which we will denote by \\(G(jω)\\). We can think of it as a function that maps the frequency \\(ω\\) to a complex number \\(G(jω)\\).\nFrequency response can be determined experimentally. Recall that complex exponentials are eigenfunctions of LTI systems. Thus, when the input to a LTI system is \\(e^{jω}\\), the output is \\(G(jω)e^{jω}\\). Therefore, when the input is \\(A\\cos ωt = A(e^{jω} + e^{-jω})/2\\), the output is \\[\\begin{align*}\n\\frac A2 G(jω)e^{jω} + \\frac A2 G(-jω)e^{-jω} &=\n\\frac A2 |G(jω)|e^{jω + \\angle G(jω)} + \\frac A2 |G(jω)|e^{-jω - \\angle G(jω)} \\\\\n&= A|G(jω)| \\cos (ωt + \\angle G(jω))\n\\end{align*}\\] where the first equality uses the fact that for a real signal \\(g(t)\\), the Fourier transform has the property that \\(G(-jω) = G^*(jω)\\).\n\n\n\n\n\n\nIf we input a cosine at frequency \\(ω\\), the output is also a cosine of the same frequency with magnitude scaled by \\(|G(jω)|\\) and shifted by \\(\\angle G(jω)\\).\n\n\n\nThere are two ways to plot the frequency response:\n\nBode plot where we plot two separate plots: a magitude plot which is the plot of \\(|G(jω)|\\) vs \\(ω\\) and a phase plot which is a plot of \\(\\angle G(jω)\\) vs \\(ω\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Bode plot of \\(G(s) = \\dfrac{1}{s^2 + s + 1}\\).\n\n\n\n\nNyquist plot which is a parametric polar plot of \\(G(jω)\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nyquist plot of \\(G(s) = \\dfrac{1}{s^2 + s + 1}\\).\n\n\n\n\n\nWe will first understand how to draw these plots and then use them as a tool to design controllers."
  },
  {
    "objectID": "bode-and-nyquist.html#frequency-response",
    "href": "bode-and-nyquist.html#frequency-response",
    "title": "Bode and Nyquist plots",
    "section": "",
    "text": "The frequency response refers to the Fourier transform of the impulse response; or equivalently, the trasfer function evaluated at \\(jω\\), which we will denote by \\(G(jω)\\). We can think of it as a function that maps the frequency \\(ω\\) to a complex number \\(G(jω)\\).\nFrequency response can be determined experimentally. Recall that complex exponentials are eigenfunctions of LTI systems. Thus, when the input to a LTI system is \\(e^{jω}\\), the output is \\(G(jω)e^{jω}\\). Therefore, when the input is \\(A\\cos ωt = A(e^{jω} + e^{-jω})/2\\), the output is \\[\\begin{align*}\n\\frac A2 G(jω)e^{jω} + \\frac A2 G(-jω)e^{-jω} &=\n\\frac A2 |G(jω)|e^{jω + \\angle G(jω)} + \\frac A2 |G(jω)|e^{-jω - \\angle G(jω)} \\\\\n&= A|G(jω)| \\cos (ωt + \\angle G(jω))\n\\end{align*}\\] where the first equality uses the fact that for a real signal \\(g(t)\\), the Fourier transform has the property that \\(G(-jω) = G^*(jω)\\).\n\n\n\n\n\n\nIf we input a cosine at frequency \\(ω\\), the output is also a cosine of the same frequency with magnitude scaled by \\(|G(jω)|\\) and shifted by \\(\\angle G(jω)\\).\n\n\n\nThere are two ways to plot the frequency response:\n\nBode plot where we plot two separate plots: a magitude plot which is the plot of \\(|G(jω)|\\) vs \\(ω\\) and a phase plot which is a plot of \\(\\angle G(jω)\\) vs \\(ω\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Bode plot of \\(G(s) = \\dfrac{1}{s^2 + s + 1}\\).\n\n\n\n\nNyquist plot which is a parametric polar plot of \\(G(jω)\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Nyquist plot of \\(G(s) = \\dfrac{1}{s^2 + s + 1}\\).\n\n\n\n\n\nWe will first understand how to draw these plots and then use them as a tool to design controllers."
  },
  {
    "objectID": "bode-and-nyquist.html#bode-plots",
    "href": "bode-and-nyquist.html#bode-plots",
    "title": "Bode and Nyquist plots",
    "section": "2 Bode Plots",
    "text": "2 Bode Plots\nAs mentioned earlier, Bode plot refers to a combination of two plots: magnitude vs frequency and phase vs frequency. It is named after :Hendrik Bode\nThere are two features of these plots:\n\nIn the magnitude plot, we plot \\(\\log |G(jω)|\\) vs \\(ω\\) (rather than \\(|G(jω)|\\) vs \\(ω\\). This is useful for the following reason. Consider a series interconnection: \\(G(s) = G_1(s)G_2(s)\\). Then, we have \\[\n  \\log |G(jω)| = \\log |G_1(jω)| + \\log |G_2(jω)|\n\\] and \\[\n  \\angle |G(jω)| = \\angle |G_1(jω)| + \\angle |G_2(jω)|.\n\\] Thus, the Bode plot of \\(G(s)\\) is simply the sum of the Bode plots of \\(G_1(s)\\) and \\(G_2(s)\\).\nFor historical reasosn, we plot the magnitude square (which corresponds to power at a particular frequency) on the :decibel scale, i.e., we plot \\(20 \\log |G(jω)|\\) vs \\(ω\\).\nThe key insight of Bode was that if we plot the frequency plot on the logarithmic scale on the \\(ω\\)-axis, then both the magnitude and phase plot can be easily approximated. This allows us to easily plot the Bode plot of any transfer function and understand the impact of adding a controller on the Bode plot.\n\nFrom properties of Fourier transforms, we know that if the impulse response \\(g(t)\\) is real, then \\(|G(jω)|\\) is even and \\(\\angle G(jω)\\) is an odd function of \\(ω\\).\nFor this reason, Bode plots are drawn only for \\(ω &gt; 0\\). We now illustrate how to draw Bode plots of simple terms.\n\n\n\n\n\n\nIn this course, we only consider Bode plot of minimum phase systems (i.e., all poles and zeros are in the LHP). Non-minimum phase systems have the same magnitude plot, but the phase plots are inverted.\n\n\n\n\n2.1 Bode plot of a constant term\nConsider the constant term \\[ G(s) = K \\] Clearly, the magnitude is a constant \\[ 20 \\log |G(jω)| = 20 \\log K \\] and the phase is a constant \\[ \\angle G(jω) = \\begin{cases}\n0^∘ & K &gt; 0 \\\\\n\\pm 180 ^∘ & K &lt; 0\n\\end{cases}\\]\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = 10\\).\n\n\n\n\n\n\n\n\n\nFor a constant term, the magnitude plot is a straight line.\nThe phase plot is also a striaght line, either at \\(0 ^∘\\) (when the gain is positive) or at \\(\\pm 180 ^∘\\) (when the gain is negative).\n\n\n\n\n\n\n2.2 Bode plot of first order term\nConsider a normalized first order system \\[\n    G(s) = \\frac{a}{s + a} = \\frac{1}{1 + \\dfrac{s}{a}}.\n\\] The frequency \\(a\\) is called the corner frequency (the reason for the name will become clear when we plot the magnitude plot).\nThe magnitude and phase are: \\[|G(jω)|^2 = \\frac{1}{1 + \\dfrac{ω^2}{a^2}}\n\\quad\\text{and}\\quad\n\\angle G(jω) = -\\tan^{-1}\\frac{ω}{a}.\\]\nObserve the following:\n\nFor \\(ω/a \\ll 1\\), (i.e., \\(ω \\ll a\\)) \\[20 \\log |G(jω)| \\approx 10 \\log \\frac{1}{1} = 0\n\\quad\\text{and}\\quad\n\\angle \\log G(jω) \\approx -\\tan^{-1} 0 = 0^∘\\]\nThus, the magnitude is \\(0\\) dB and the phase is \\(0^∘\\).\nFor \\(ω/a \\gg 1\\), (i.e., \\(ω \\gg a\\)) \\[20 \\log |G(jω)| \\approx 10 \\log \\frac{1}{\\dfrac{ω}{a}} = -20 \\log ω + 20 \\log ω\n\\quad\\text{and}\\quad\n\\angle \\log G(jω) \\approx -\\tan^{-1} ∞ = 90^∘\\]\nThus, the magnitude is a straight line with a slope of \\(-20\\) dB/decade and the phase is \\(-90^∘\\).\n\nThus, the Bode plot is a straight line for low and high frequencies. We will approximate it by a straight line in the intermediate range as well as shown below.\n\nBode PlotStraight line approximation\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = \\dfrac{1}{1 + \\dfrac{s}{a}}\\).\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = \\dfrac{1}{1 + \\dfrac{s}{a}}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFor a pole at \\(a\\), the straight line approximation of the magnitude Bode plot starts at 0 dB until the corner frequency \\(a\\) and then decreases at a slope of -20 dB/decade.\nThe phase plot starts at 0° until one decade before the corner frequency (i.e., until \\(a/10\\), then goes down at a slope of -45°/decade for two decades (i.e., until \\(10a\\)) and then remains constant at -90°.\nIf the pole has multiplicity \\(n\\), then the slopes are \\(-20n\\) dB/decade and \\(-45n^∘\\)/decade.\n\n\n\n\n\n\n2.3 Bode plot of a second order term\nConsider a normalized second order term is of the form:\n\\[\nG(s) = \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}.\n\\]\nIn this course, we will ignore the impact of damping coefficient on the Bode plot approximate any second order system as \\[\nG(s) = \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2} \\approx\n\\frac{ω_n^2}{(s+ω_n)^2}.\n\\]\nThe reason for ignoring the damping coefficient is as follows. In this course, we are using the straight line approximation of Bode plot to understand how to use them for control design. It is possible to develop rules that capture the impact of damping coefficient on the Bode plot (and the textbook presents such rules!), but in my opinion, there is very little value in remembering such rules. In practice, if you need an accurate Bode plot, it is much simpler to just plot it via MATLAB.\n\nBode PlotStraight line approximation\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = \\dfrac{1}{s^2+s+1}\\).\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = \\dfrac{1}{s^2+s+1}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFor the purpose of this course, we will treat a second order term \\(ω_n^2/(s^2 + 2 ζ ω_n s + ω_n^2)\\) as a repeated root at \\(ω_n\\).\n\n\n\n\n\n\n2.4 Poles at origin\nConsider the TF with a pole at origin: \\[\nG(s) = \\frac 1s\n\\]\nThus, \\[\n|G(jω)|^2 = \\frac{1}{ω^2}\n\\quad\\hbox{hence}\\quad\n20 \\log |G(jω)| = -20 \\log ω\n\\] and \\[\n\\angle G(jω) = - 90^∘\n\\]\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = \\dfrac{1}{s}\\).\n\n\n\n\n\n\n\n\n\nFor a pole at origin, the magnitude Bode plot is a straight line with -20 dB/decade slope. The line passes through 0 dB at \\(ω=1\\) rad/sec.\nThe phase is constant at -90°.\nIf the pole has multiplicity \\(n\\), then the slope of the magnitude plot is \\(-20n\\) dB/decade and the constant phase is \\(-90n^∘\\).\n\n\n\n\n\n\n2.5 The impact of zeros\nConsider the transfer function \\[\nG(s) = 1 + \\frac{s}{a}.\n\\]\nThe Bode plot of this is simply the “flipped” version of \\(1/(1 + s/a)\\).\n\nBode PlotStraight line approximation\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = {1 + \\dfrac{s}{a}}\\).\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBode plot of \\(G(s) = {1 + \\dfrac{s}{a}}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nA zero is similar to a pole but the plots are mirrored around 0dB or 0°.\n\n\n\n\n\n\n2.6 Bode plot of general transfer functions\nWe now explain how to plot the Bode plots of a general transfer function via an example. Consider \\[\nG(s) = \\frac{100 (s+10)}{ (s+1)(s+100) }\n\\]\nStep 1: Normalize the transfer function and break it into first or second order terms.\n\\[\nG(s) = \\frac{1}{10} \\left( \\frac{1}{1 + s} \\right)\n\\left( 1 + \\frac{s}{10} \\right)\n\\left( \\frac 1{1 + \\dfrac{s}{100}} \\right)\n\\]\nStep 2 Separately plot the straight-line Bode plot approximation for each of the terms\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Add all the individual terms to plot the combined Bode plot.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.7 System type and steady state errors\nIt is possible to identify the system type and the corresponding error constant from Bode plots. Recall that a general TF of type \\(k\\) can be written as \\[\n\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\nG(s) = \\dfrac{K}{s^k} \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n      {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\n\\]\n\n2.7.1 Position constant of type 0 system\nThe TF of a type 0 system is: \\[\n\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\nG(s) = K_p \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n      {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\n\\]\nThus, \\(\\lim_{s \\to 0} G(s) = K_p\\). This means that at low frequency (i.e., \\(s \\to 0\\)), the magnitude Bode plot is straight line with 0 dB/decade slope and has a y-offset of \\(20 \\log K_p\\). So, we can find the position constant by inspection.\nAn illustration of the Bode plot of a type 0 system is shown in Figure 3.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Bode plot of type 0 system. The magnitude at low frequencies is equal to \\(20 \\log K_p\\).\n\n\n\n\n\n\n2.7.2 Velocity constant of type 1 system\nThe TF of a type 1 system is: \\[\n\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\nG(s) = \\dfrac{K_v}{s} \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n      {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\n\\]\nThus, \\(\\lim_{s \\to 0} s G(s) = K_v\\). This means that at low frequency (i.e., \\(s \\to 0\\)), the magnitude Bode plot is straight line with -20 dB/decade slope. One can identify the velocity constant \\(K_v\\) by extending the initial -20 dB line until it hits 0 dB. The frequency at which it happens is \\(ω = K_v\\) (because at that frequency |K_v/j ω| = 1). So, we can find the velocity constant by inspection.\nAn illustration of the Bode plot of a type 1 system is shown in Figure 4.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Bode plot of type 1 system. The initial slope of -20 dB/dec intercepts the 0 dB line at \\(K_v\\).\n\n\n\n\n\n\n2.7.3 Acceleration constant of type 2 system\nThe TF of a type 1 system is: \\[\n\\def\\1#1{\\Bigl(1 + \\dfrac{s}{#1}\\Bigr)}\nG(s) = \\dfrac{K_a}{s^2} \\frac{\\1{z_1}\\1{z_2}\\cdots\\1{z_m}}\n      {\\1{p_1}\\1{p_2}\\cdots\\1{p_n}}\n\\]\nThus, \\(\\lim_{s \\to 0} s^2 G(s) = K_a\\). This means that at low frequency (i.e., \\(s \\to 0\\)), the magnitude Bode plot is straight line with -40 dB/decade slope. One can identify the acceleration constant \\(K_v\\) by extending the initial -40 dB line until it hits 0 dB. The frequency at which it happens is \\(ω = \\sqrt{K_a}\\) (because at that frequency |K_a/(j ω)^2| = 1). So, we can find the acceleration constant by inspection.\nAn illustration of the Bode plot of a type 2 system is shown in Figure 5.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Bode plot of type 2 system. The initial slope of -40 dB/dec intercepts the 0 dB line at \\(\\sqrt{K_a}\\).\n\n\n\n\n\nExercise 1 For each of the following systems, identify the system type and the corresponding error constant.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.8 System identification via Bode plots\nSince there is a one-to-one correspondence between transfer functions and Bode plots, we can identify a system transfer function from its Bode plot. We illustrate this via an example.\n\nExercise 2 Find the minimum phase transfer function which has a magnitude Bode plot as shown below.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the Bode plot of a type 0 systems, since the magnitude Bode plot is a constant at low frequencies. The position constant is \\(K_p = -10\\)dB or equivalently \\(\\sqrt{10}\\). There is then a zero at \\(10\\) rad/s, a pole at \\(100\\) rad/sec, and a second pole at \\(1000\\) rad/sec. Thus, the TF is \\[\n  G(s) = \\sqrt{10} \\frac{ \\biggl(1+\\dfrac{s}{10}\\biggr) }\n{ \\biggl(1 + \\dfrac{s}{100} \\biggr) \\biggl(1 + \\dfrac{s}{1000} \\biggr) } =\n\\frac{1}{1000\\sqrt{10}} \\frac{ (s+10) }{ (s + 100) (s + 1000) }.\n\\]"
  },
  {
    "objectID": "bode-and-nyquist.html#nyquist-plots",
    "href": "bode-and-nyquist.html#nyquist-plots",
    "title": "Bode and Nyquist plots",
    "section": "3 Nyquist plots",
    "text": "3 Nyquist plots\nNquist plot is a polar plot of the frequency response, that is, we plot \\(G(jω)\\) as a complex number for different values of \\(ω\\). Unlike the Bode plot, we plot the Nyquist plot for all \\(ω \\in (-∞, ∞)\\) and usually also draw an arrow indicating the direction. An example is shown in Figure 2. In the next lecture, we will show how to use Nyquist plot to determine stability of a unity feedback system."
  },
  {
    "objectID": "state-feedback.html",
    "href": "state-feedback.html",
    "title": "Pole placement via statement feedback",
    "section": "",
    "text": "Consider a SSM \\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t)\n\\end{align*}\\]\nIn this section, we will learn how to control such a system when the controller has access to the internal state \\(x(t)\\). This is called state feedback. In the next section, we will learn how to design a system when the controller has access to only the output \\(y(t)\\). This is called output feedback.\nThe basic idea of state feedback is shown in Figure 1. The system has two tunable blocks: the feedback gain \\(K\\) which is a \\(1 \\times n\\) matrix and the pre-compensator \\(N\\), which is a scalar. The equation for the input-output system may be written as: \\[\\begin{align*}\n  \\dot x(t) &= A x(t) + B \\bigl( N r(t) - K x(t) \\bigr),  \\\\\n  y(t) &= C x(t),\n\\end{align*}\\] or, equivalently, \\[\\begin{align*}\n  \\dot x(t) &= (A - BK) x(t) + B N r(t),  \\\\\n  y(t) &= C x(t),\n\\end{align*}\\] where \\(N \\in \\reals\\) and \\[\nK = \\MATRIX{k_0 & \\cdots & k_{n-1}}.\n\\]\nWe will first consider the system without any input (i.e., \\(r(t) = 0\\)) and consider the zero input response. In this case, we only need to design \\(K\\), which has \\(n\\) tunable gains \\(k_0, \\dots, k_{n-1}\\). The hope is that we can find these gain to arbitrarily choose the eigenvalues of the closed loop system. This is called pole placement.\nWe will start by looking at system in CCF and show that for such systems pole placement can be achieved by simply comparing coefficients of the characteristic polynomial of the open loop systems and the designed characteristic polynomial of the closed loop system. We will then look at systems that are not in CCF and show that if the system satisfies a condition known as controllability, we can find a transformation that converts the to CCF and use the results from state transformation to find \\(K\\)."
  },
  {
    "objectID": "state-feedback.html#state-feedback-for-systems-in-ccf",
    "href": "state-feedback.html#state-feedback-for-systems-in-ccf",
    "title": "Pole placement via statement feedback",
    "section": "1 State feedback for systems in CCF",
    "text": "1 State feedback for systems in CCF\nConsider a state space system in CCF with \\[\nA = \\MATRIX{\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\cdots & 1 \\\\\n-a_0 & -a_1 & \\cdots & \\cdots & - a_{n-1}\n},\n\\quad\nB = \\MATRIX{0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 1}.\n\\]\nWe want to desgin a state feedback controller \\[\nu(t) = -K x(t)\n\\quad \\text{where }\nK = \\MATRIX{k_0 & \\cdots & k_{n-1}}\n\\] such that the characteristic polynomial of the closed loop system is \\[\n\\det(sI - (A - BK)) =\ns^n + α_{n-1} s^{n-1} + \\cdots + α_1 s + α_0.\n\\]\nObserve that \\[\nB K  = \\MATRIX{0 \\\\ \\vdots \\\\ 0 \\\\ 1}\n\\MATRIX{k_0 & \\cdots & k_{n-1}} =\n\\MATRIX{\n0 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & 0 \\\\\nk_0 & k_1 & \\cdots & k_{n-1}\n}.\n\\] Thus, \\[\nA - BK = \\MATRIX{\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\cdots & 1 \\\\\n-(a_0+k_0) & -(a_1+k_1) & \\cdots & \\cdots & -(a_{n-1} + k_{n-1})\n},\n\\] which has the CCF structure. Thus, \\[\n\\det(sI - (A - BK)) =\ns^n + (a_{n-1} + k_{n-1}) s^{n-1} + \\cdots + (a_1 + k_1) s + (a_0 + k_0).\n\\]\nObserve that each component of the gain matrix \\(K\\) affects only one coefficient of the characteristic polynomial. Thus, we can pick \\[\n\\bbox[5pt,border: 1px solid]\n{K = \\MATRIX{ α_0 - a_0 & \\cdots & α_{n-1} - a_{n-1}}}\n\\] to place the poles of the closed loop system at any desired location.\n\nExample 1 Consider the system \\[\nA = \\MATRIX{0 & 1 \\\\ -1 & -3},\n\\quad\nB = \\MATRIX{0 \\\\ 1}.\n\\] Determine a state feedback controller such that the eigenvalues of the closed loop system are at \\(-3 \\pm 2j\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe desired characteristic polynomial is \\[(s + 3)^2 + 2^2 = s^2 + 6s + 13.\\]\nThe current characteristic polynomial is (since the system is in CCF, we can determine it by inspection) \\[\\det(sI - A) = s^2 + 3s + 1.\\]\nThus, the required state feedback matrix is \\[K = \\MATRIX{13 - 1 & 6 - 3} = \\MATRIX{12 & 3}.\\]\nWe verify the solution below:\n\nA = [0 1; -1 -3]\nB = [0; 1]\np = [-3+2im -3-2im]\nK = place(A,B,p)\n\n1×2 Matrix{ComplexF64}:\n 12.0+0.0im  3.0+0.0im"
  },
  {
    "objectID": "state-feedback.html#state-feedback-for-systems-not-in-ccf",
    "href": "state-feedback.html#state-feedback-for-systems-not-in-ccf",
    "title": "Pole placement via statement feedback",
    "section": "2 State feedback for systems not in CCF",
    "text": "2 State feedback for systems not in CCF\nWhen a system is not in CCF, we can still use the design idea for systems in CCF if there exists a transformation (or change of coordinates) \\(x_c(t) = T^{-1} x(t)\\) such that the system matrices for the system \\((A_c, B_c, C_c)\\) for \\(x_c(t)\\) are in CCF. If such a transformation can be found, then we can find the gain matrix \\(K_c\\) for the system in CCF using the method of the previous section and use1 \\[ K = K_c T^{-1} \\] as the gain for the original system. So, the main question is: how do we find such a transformation?\n1 The state feedback in the new coordinate system is \\[u(t) = - K_c x_c(t).\\] Since \\(x_c(t) = T^{-1} x(t)\\), we have \\[ u(t) = - K_c T^{-1} x(t). \\] Therefore \\(K = K_c T^{-1}\\).\n2.1 Controllability matrix\nThe controllability matrix of a state space system \\((A,B)\\) is defined as \\[\\mathcal C_{(A,B)} = \\MATRIX{B & AB & \\cdots & A^{n-1} B }.\\]\nNow consider a change of coordinates \\(x_c(t) = T^{-1} x(t)\\). Recall that the state space representation in the new coordinate system is \\[\nA_c = T^{-1} A T \\quad B_c = T^{-1} B.\n\\] Observe that\n\n\\(A_c^2 = T^{-1} A T T^{-1} A T = T^{-1} A^2 T\\)\n\\(A_c^3 = T^{-1} A^2 T T^{-1} A T = T^{-1} A^3 T\\)\netc.\n\nTherefore,\n\n\\(A_c^2 B_c =  T^{-1} A^2 T T^{-1} B = T^{-1} A^2 B\\)\n\\(A_c^3 B_c =  T^{-1} A^3 T T^{-1} B = T^{-1} A^3 B\\)\netc.\n\nTherefore, the controllability matrix of the system in the new coordinates is \\[\\begin{align*}\n\\mathcal C_{(A_c, B_c)} &=\n\\MATRIX{ T^{-1} B & T^{-1} A B & \\cdots & T^{-1} A^{n-1} B  }\n\\\\\n&= T^{-1} \\MATRIX{B & AB & \\cdots A^{n-1} B }\n\\\\\n&= T^{-1} \\mathcal C_{(A,B)}.\n\\end{align*}\\]\nThus, we can go from the SSM \\((A,B)\\) to the SSM \\((A_c, B_c)\\) using the transformation \\[\nT^{-1} = \\mathcal C_{(A_c, B_c)} \\mathcal C_{(A,B)}^{-1}\n\\] provided \\(\\mathcal C_{(A,B)}\\) is invertible (i.e., has full rank). When that is the case, we say that the system is controllable.\nThere are different ways in which we can get uncontrollable models as is illustrated in the following example.\n\nExample 2 Check if the following systems are controllable.\n\n\\(A = \\MATRIX{2 & 0 \\\\ 0 & 3}\\) and \\(B = \\MATRIX{0 \\\\ 1}\\).\n\\(A = \\MATRIX{-4 & 5 \\\\ 0 & 9}\\) and \\(B = \\MATRIX{-2 \\\\ 0}\\).\n\\(A = \\MATRIX{1 & 5 \\\\ 8 & 4}\\) and \\(B = \\MATRIX{-2 \\\\ 2}\\).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWe have \\[ A B = \\MATRIX{2 & 0 \\\\ 0 & 3} \\MATRIX{ 0 \\\\ 1} = \\MATRIX{0 \\\\ 3}.\\] Thus, \\[ \\mathcal C_{(A,B)} = \\MATRIX{ 0 & 0 \\\\ 1 & 3}. \\] We have \\(\\det \\mathcal C_{(A,B)} = 0\\). So, \\((A,B)\\) is not controllable.\nWe have \\[ A B = \\MATRIX{-4 & 5 \\\\ 0 & 9 } \\MATRIX{ -2 \\\\ 0} = \\MATRIX{8 \\\\ 0}.\\] Thus, \\[ \\mathcal C_{(A,B)} = \\MATRIX{ -2 & -8 \\\\ 0 & 0}. \\] We have \\(\\det \\mathcal C_{(A,B)} = 0\\). So, \\((A,B)\\) is not controllable.\nWe have \\[ A B = \\MATRIX{ 1 & 5 \\\\ 8 & 4} \\MATRIX{ -2 \\\\ 2} = \\MATRIX{8 \\\\ -8}.\\] Thus, \\[ \\mathcal C_{(A,B)} = \\MATRIX{ -2 & 8 \\\\ 2 & -8}. \\] We have \\(\\det \\mathcal C_{(A,B)} = 0\\). So, \\((A,B)\\) is not controllable.\n\n\n\n\n\n\n2.2 Pole Placement\nWe can use the above idea to design state feedback controller for systems not in CCF. In particular, the pole placement problem is as follows.\n\n\n\n\n\n\nPole placement\n\n\n\nWe are given a SSM \\((A,B)\\) not in CCF and we want to find a state feedback controller \\[\nu(t) = - K x(t), \\quad\n\\text{where } K = \\MATRIX{k_0 & \\cdots & k_{n-1} }\n\\] such that the characteristic polynomial of the closed loop system is \\[\n\\det(sI - A +  BK) = s^n + α_{n-1} s^{n-1} + \\cdots + α_1 s + α_0.\n\\]\n\n\nWe present two methods for pole placement.\n\nMethod 1: Using controllability matrix\n\nFind the controllability matrix \\(\\mathcal C_{(A,B)}\\) of the system.\nIf the system is not controllable, i.e., the controllability matrix is not full rank (which can be tested by checking that the determinant of the controllability matrix is non-zero), pole placement is not possible and we quit the procedure.\nIf the system is controllable, find the characteristic polynomial of \\(A\\): \\[\\det(s I - A) = s^n + a_{n-1} s^{n-1} + \\cdots + a_0 \\]\nUsing the above characteristic polynomial, write the SSM \\((A_c, B_c)\\) in CCF.\nRecall that the characteristic polynomial is the denominator of the transfer function\nCompute the controllability matrix \\(\\mathcal C(A_c, B_c)\\) of the system \\((A_c, B_c)\\) in CCF.\nCompute the transformation \\[\\bbox[5pt,border: 1px solid]{\nT^{-1} = \\mathcal C_{(A_c, B_c)} \\mathcal C_{(A,B)}^{-1}\n}\\]\nCompute the controller gain \\(K_c\\) for the system in CCF: \\[ K_c = \\MATRIX{ α_0 - a_0 & \\cdots & α_{n-1} - a_{n-1}}. \\]\nThe controller gain for the original system is \\[K = K_c T^{-1}.\\]\n\n\n\nMethod 2: A slightly more efficient approach\nIt is possible to avoid the computation of the controllability matrix of the system in CCF by replacing steps 4–6 of Method 1 by the following:\n\nCompute the matrix \\[ W = \\MATRIX{ a_1 & a_2 & \\cdots & a_{n-1} & 1 \\\\\n                a_2 & \\iddots & \\iddots & 1 & 0 \\\\\n                \\iddots & \\iddots & \\iddots & \\iddots & \\vdots \\\\\n               a_{n-1} & 1 & 0 & \\cdots & 0 \\\\\n               1 & 0 & 0 & \\cdots & 0\n              }\n\\]\nCompute the transformation: \\[\\bbox[5pt,border: 1px solid]{\nT^{-1} = \\bigl[ \\mathcal C_{(A,B)} W \\bigr]^{-1}\n}\\]\n\nThis method relies on the fact that for a system \\((A_c,B_c)\\) in CCF, \\(\\mathcal C_{(A_c,B_c)} = W^{-1}\\). For a proof, see Lemma 3.5 of William and Lawrence.\n\nExample 3 Consider the following SSM: \\[ A = \\MATRIX{3 & 1 \\\\ 1 & 2}, B = \\MATRIX{2 \\\\1} \\text{ and } C = \\MATRIX{ 3 & 2}. \\] Design a state feedback controller (i.e., compute the feedback gain \\(K\\)) such that the eigenvalues of the closed loop system are at \\(-2 \\pm 2j\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe will show the solution via both method 1 and method 2. For method 1, we have the following.\n\nWe have \\[ A B = \\MATRIX{3 & 1\\\\ 1 & 2} \\MATRIX{2 \\\\ 1} = \\MATRIX{7 \\\\ 4}. \\] Therefore, \\[ \\mathcal C_{(A,B)} = \\MATRIX{ 2 & 7 \\\\ 1 & 4}. \\]\n\\(\\det \\mathcal C_{(A,B)} = 8 - 7 = 1 \\neq 0\\). Thus, \\((A,B)\\) is controllable and we can design a state feedback controller.\nThe characteristic polynomial is given by \\[s I - A = \\MATRIX{s & 0 \\\\ 0 & s} - \\MATRIX{3 & 1 \\\\ 1 & 2} = \\MATRIX{ s-3 & -1 \\\\ -1 & s - 2}.\\] Therefore, \\[\n\\det(sI-A) = \\DET{ s-3 & -1 \\\\ -1 & s - 2} = (s-3)(s-1) - 1 = s^2 - 5s + 5.\n\\]\nThe SSM in CCF is \\[ A_c = \\MATRIX{0 & 1 \\\\ -5 & 5}, \\quad B_c = \\MATRIX{0 \\\\ 1}.\\]\nWe have \\[ A_c B_c = \\MATRIX{ 0 & 1 \\\\ -5 & 5} \\MATRIX{0 \\\\ 1} = \\MATRIX{1 \\\\ 5}. \\] Therefore, \\[ \\mathcal C_{(A_c, B_c)} = \\MATRIX{0 & 1 \\\\ 1 & 5}. \\]\nThus, we have \\[\\begin{align*}\nT^{-1} &= \\mathcal C_{(A_c,B_c)} \\mathcal C_{(A,B)}^{-1}\n\\\\\n&= \\MATRIX{0 & 1 \\\\ 1 & 5} \\MATRIX{2 & 7 \\\\ 1 & 4}^{-1}\n\\\\\n&= \\MATRIX{0 & 1 \\\\ 1 & 5} \\MATRIX{4 & -7 \\\\ -1 & 2}\n\\\\\n&= \\MATRIX{-1 & 2 \\\\ -1 & 3}.\n\\end{align*}\\]\nThe desired characteristic polynomial is \\[ (s+2-2j)(s+2+2j) = (s+2)^2 + 2^2 = s^2 + 4s + 8. \\] Thus, \\[ K_c = \\MATRIX{8 - 5 & 4 - (-5)} = \\MATRIX{3 & 9}.\\]\nThus, \\[ K = K_c T^{-1} = \\MATRIX{3 & 9} \\MATRIX{-1 & 2 \\\\ -1 & 3} = \\MATRIX{-12 & 33}. \\]\n\nWe can verify that the gain is correct.\n\nA = [3 1;1 2]\nB = [2; 1]\np = [-2+2im -2-2im]\nK = place(A,B,p)\n\n1×2 Matrix{ComplexF64}:\n -12.0+0.0im  33.0+0.0im\n\n\nFinally, we can redo the calculations using Method 2. In particular, instead of steps 4–6, we have\n\nDefine \\[ W = \\MATRIX{ a_1 & 1 \\\\ 1 & 0 } = \\MATRIX{ -5 & 1 \\\\ 1 & 0 }. \\]\nTherefore, \\[\\begin{align*}\n  T^{-1} &= \\bigl[ \\mathcal C_{(A,B)} W \\bigr]^{-1} \\\\\n  &= \\left( \\MATRIX{2 & 7 \\\\ 1 & 4} \\MATRIX{-5 & 1 \\\\ 1 & 0 } \\right)^{-1} \\\\\n  &= \\MATRIX{-1 & 2 \\\\ -1 & 3},\n  \\end{align*}\\]\n\nThus, we get the same \\(T\\) as using Method 1. The remaining steps 7 and 8 are the same as before."
  },
  {
    "objectID": "state-feedback.html#state-feedback-for-reference-tracking",
    "href": "state-feedback.html#state-feedback-for-reference-tracking",
    "title": "Pole placement via statement feedback",
    "section": "3 State-feedback for reference tracking",
    "text": "3 State-feedback for reference tracking\nWe now return to the architecture shown in Figure 1 and consider the design of the pre-compensator \\(N\\). The idea is to choose the pre-compensator \\(N\\) to ensure that steady state error is zero. In particular, we know that the DC-gain of the system is \\[\n\\text{DC-gain} = N G(0) = N C (sI - (A - BK) )^{-1} B \\bigg|_{s = 0} = - N C (A-BK)^{-1} B.\n\\] To ensure that steady-state error when tracking a step input is zero, we pick the pre-compensator gain \\(N\\) as follows: \\[\n  N = - \\frac{1}{ C(A-BK)^{-1} B }.\n\\]"
  },
  {
    "objectID": "state-feedback.html#understanding-lack-of-controllability",
    "href": "state-feedback.html#understanding-lack-of-controllability",
    "title": "Pole placement via statement feedback",
    "section": "4 Understanding lack of controllability",
    "text": "4 Understanding lack of controllability\nIn this section, we provide some intuition to understand why controllability plays a critical role in state feedback. Consider the SSM\n\\[\\begin{align*}\n\\dot x(t) &= A x(t) + B u(t) \\\\\ny(t) &= C x(t).\n\\end{align*}\\]\nAs we saw in the last section, the zero-state response of the system is \\[\nx(t) = \\int_{0}^t e^{A τ} B u(t-τ)\\, d τ.\n\\]\nWe already say that \\[\n  e^{At} = I + At + \\frac{A^2 t^2}{2!} + \\frac{A^3 t^3}{3!} + \\cdots\n\\] From :Caley-Hamilton Theorem, we know that any \\(A^k\\) for \\(k \\ge n\\) can be expressed in terms of \\(A^0, \\dots, A^{n-1}\\). Thus, we can write \\[\n  e^{At} = \\sum_{k=0}^{n-1} w_k(t) A^k\n\\] for some weight functions \\(\\{w_k(t)\\}_{k=0}^n\\).\nHence, we can write the state \\(x(t)\\) as \\[\\begin{align*}\nx(t) &= \\int_{0}^t \\left( \\sum_{k=0}^{n-1} w_k(t) A^k \\right) B u(t - τ)\\, d τ \\\\\n&= \\sum_{k=0}^{n-1} A^k B\n\\underbrace{\\left[\\int_{0}^t w_k(t) u(t - τ)\\, d τ\\right]}\n_{\\eqqcolon v_k(t)} \\\\\n&= \\sum_{k=0}^{n-1} A^{k-1} B v_k(t)\n\\end{align*}\\] where \\(v_k(t)\\) denote on the control input \\(u(t)\\).\nLet \\(M_c\\) denote the range space of \\(\\mathcal C_{(A,B)}\\). Then, by definition, for any input \\(u(t)\\), \\[\nx(t) = \\sum_{k=0}^{n-1} A^{k-1} B v_k(t) = \\mathcal C_{(A,B)} v(t) \\in M_c.\n\\]\nSo, no matter what control inputs we use, we can only reach states belonging to \\(M_c\\). If \\[\\text{rank}(\\mathcal C_{(A,B)}) = \\text{rank}(M_c) \\neq n,\\] then there is a subspace that cannot be reached by any control input! This is actually the formal definition of controllability.\n\n\n\n\n\n\nControllability\n\n\n\nA SSM is said to be controllable if for every initial condition \\(x_0\\), time \\(t\\), and state \\(x\\) at time \\(t\\), there exists a control input \\(u\\) that moves the system starting at initial state \\(x_0\\) to state \\(x\\) at time \\(t\\).\nThe argument presented above shows that a system is controllable if and only if \\(\\text{rank}(\\mathcal C_{(A,B)} = n\\).\n\n\n\n4.1 Uncontrollable modes of a system\n\nExample 4 Consider \\[\nA = \\MATRIX{-2 & 0 \\\\ 0 & -1}\n\\quad\\text{and}\\quad\nB = \\MATRIX{2 \\\\ 0}.\n\\]\nThen, \\[\\mathcal C_{(A,B)} = \\MATRIX{2 & -4 \\\\ 0 & 0 }.\\] Therefore, \\[\n\\text{range}(\\mathcal C_{(A,B)}) =\n\\left\\{ \\MATRIX{ α \\\\ 0 } : α \\in \\reals \\right\\}.\n\\] Thus, no matter what input we choose, we cannot reach vectors belonging to the subspace \\[\n\\left\\{ \\MATRIX{ 0 \\\\ β } : β \\in \\reals \\right\\}.\n\\]\n\nThe above example shows that for systems where \\(A\\) matrix is diagonal, it is easy to interpret controllability: a system \\((A,B)\\) where \\(A\\) is diagonal is uncontrollable if some element of \\(B\\) is zero, which means that there is a mode (i.e., eigen-direction) such that the input cannot effect the evolution of the state in that eigen-direction.\nLet’s revisit the examples in Example 2. Let \\((λ_1, v_1)\\) and \\((λ_2, v_2)\\) be the eigenvalue-eigenvector pairs for each model. Take \\(T = [v_1, v_2]\\) and consider the change of variables \\(\\tilde x = T^{-1} x\\). In this new coordinate system \\[ \\tilde A = T^{-1} A T, \\quad \\tilde B = T^{-1} B. \\] We have that\n\nWe take \\(T = \\MATRIX{1 & 0 \\\\ 0 & 1}\\). Thus, we have \\[\\tilde A = \\MATRIX{2 & 0 \\\\ 0 & 3}, \\quad \\tilde B = \\MATRIX{0 \\\\ 1}.\\]\nWe take \\(T = \\MATRIX{1 & 5 \\\\ 0 & 13}\\). Thus, we have \\[\\tilde A = \\MATRIX{-4 & 0 \\\\ 0 & 3}, \\quad \\tilde B = \\MATRIX{-2 \\\\ 0}.\\]\nWe take \\(T = \\MATRIX{-1 & 5 \\\\ 1 & 8}\\). Thus, we have \\[\\tilde A = \\MATRIX{-4 & 0 \\\\ 0 & 3}, \\quad \\tilde B = \\MATRIX{2 \\\\ 0}.\\]\n\nThus, in all cases, there is an eigen-direction that cannot be controlled!\nFinally, we consider the following circuit to see why a physical system may be uncontrollable.\n\n\n\n\n\n\nFigure 2: A RC-bridge\n\n\n\nWe will follow the approach presented in previous section to find a SSM for the circuit in Figure 2. We take the voltage across the two capcitors (denoted by \\(v_1\\) and \\(v_2\\)) as the two components of the SSM. From KCL, we have \\[\\begin{align*}\n  C_1 \\dot v_1(t) &= \\frac{u(t) - v_1(t)}{R_1} - \\frac{v_1(t) - v_2(t)}{R_3} \\\\\n  C_2 \\dot v_2(t) &= \\frac{u(t) - v_2(t)}{R_2} + \\frac{v_1(t) - v_2(t)}{R_3}\n\\end{align*}\\] which can be re-written in state-space form as: \\[\\def\\1{\\vphantom{\\dfrac{1}{R_1C_1}}}\n  \\MATRIX{ \\1\\dot v_1(t) \\\\ \\1\\dot v_2(t) } =\n  \\MATRIX{ -\\left(\\dfrac{1}{R_1C_1} + \\dfrac{1}{R_3C_1}\\right) & \\dfrac{1}{R_3C_1} \\\\\n  \\dfrac{1}{R_3C_2} & -\\left(\\dfrac{1}{R_2C_2} + \\dfrac{1}{R_3C_2}\\right)}\n  \\MATRIX{ \\1v_1(t) \\\\ \\1v_2(t) }\n  +\n  \\MATRIX{\\dfrac{1}{R_1C_1} \\\\ \\dfrac{1}{R_2C_2}} u(t).\n\\] Consider the difference \\(\\tilde v(t) = v_1(t) - v_2(t) = \\MATRIX{I & -I} \\MATRIX{v_1(t) \\\\ v_2(t)}\\), which satisfies \\[\n\\dot {\\tilde v(t)} =\n  -\\left[\\frac{1}{R_1C_1} + \\frac{2}{R_3C_1} \\right] v_1(t)\n  +\\left[\\frac{1}{R_2C_2} + \\frac{2}{R_3C_2} \\right] v_2(t)\n+ \\left[ \\frac{1}{R_1C_1} - \\frac {1}{R_2C_2} \\right] u(t).\n\\] When the bridge is balanced, i.e., \\(R_1 = R_2  = R\\) and \\(C_1 = C_2 = C\\), we have \\[\n\\dot {\\tilde v(t)} = -\\left[ \\frac{1}{RC} + \\frac{2}{R_3 C} \\right] \\tilde v(t)\n\\] which is a homogeneous equation with no input. Thus, in this case, the control input \\(u(t)\\) has no impact on the subspace \\(v_1(t) - v_2(t)\\) of the state space. Thus, the system is not controllable.\nWe can also verify this via the controllability matrix, whic is given by \\[\n\\mathcal C = \\MATRIX{ \\dfrac{1}{RC} & -\\dfrac{1}{(RC)^2} \\\\\n\\dfrac{1}{RC} & -\\dfrac{1}{(RC)^2}}\n\\] which is not full rank.\n\n\n4.2 Uncontrollability and pole-zero cancellation\nUncontrollability is often related to pole zero cancelling. Consider the following transfer function: \\[ G(s) = \\frac{s - z}{s^2 + 3s + 2}. \\] Let’s compare the CCF and OCF representations of the system:\n\nCCF representation \\[ A_c = \\MATRIX{0 & 1 \\\\ -2 & -3 }, \\quad\n   B_c = \\MATRIX{0 \\\\ 1}, \\quad\n   C_c = \\MATRIX{-z & 1}.\n\\]\nOCF representation \\[ A_o = \\MATRIX{0 & -2 \\\\ 1 & -3 }, \\quad\n   B_o = \\MATRIX{-z \\\\ 1}, \\quad\n   C_o = \\MATRIX{0 & 1}.\n\\]\n\nObserve that in the CCF representation the matrices \\((A_c, B_c)\\) do not depend on the location \\(z\\) of the zero. Therefore, neither does the controllability matrix, which is given by \\[\n  \\mathcal C_{(A_c, B_c)} = \\MATRIX{ 0 & 1 \\\\ 1 & -3 }\n\\] On the other hand, in the OCF representation the matrix \\(B_o\\) depends on the location \\(z\\) of the zero. Therefore, so does the controllability matrix, which is given by \\[\n  \\mathcal C_{(A_o, B_o)} = \\MATRIX{ -z & -2 \\\\ 1 & -3 - z}.\n\\] Thus, \\(\\det \\mathcal C_{(A_o, B_o)} = z^2 + 3z + 2 = (z+1)(z+2).\\)$ Thus, the OCF representation is not controllable when \\(z = -1\\) or \\(z = -2\\)!\nConsider z = -1$. For this value of \\(z\\), the TF is given by \\[\n  G(s) = \\frac{s + 1}{(s+1)(s+2)} = \\frac{1}{s+2}.\n\\] Thus, we have a pole-zero cancellation and the OCF representation is not controllable because of that!\nThis also suggests that we need to refine the relationship between characteristic polynomial of \\(A\\) and the poles of the TF. The two are equal when there is no pole-zero cancellation. This also leads to the following definition.\n\n\n\n\n\n\nInternal stability\n\n\n\nA SSM model \\((A,B,C)\\) is called internally stable if all the eigenvalues of \\(A\\) lie in the OLHP.\nThis is equivalent to having all open loop poles like in the OLHP and no pole-zero cancellations taking place.\n\n\n\n\n4.3 Properties of controllability\nWe conclude the discussion with some properties of controllability.\n\n\n\n\n\n\nProperties of controllability\n\n\n\n\nChange of coordinates does not change controllability.\nLet \\(T\\) be an inverible matrix that changes the SSM \\((A,B,C)\\) to \\((\\tilde A, \\tilde B, \\tilde C)\\). Then, the system \\((A,B)\\) is controllable if and only if \\((\\tilde A, \\tilde B)\\) is controllable (we have seen this before when we derived a formula for \\(T^{-1}\\).)\nInvariance under state feedback\nThe system \\((A,B)\\) is controllable if and only if the system \\((A - BK, B)\\) is controllable for all state feedback gains \\(K\\).\n\n\n\nIs there a contradiction between the first claim (change of variables doesn’t change controllability) and the pole-zero cancellation example above? No. Note that the claim is that for an invertible matrix \\(T\\). In the example considered in the pole-zero cancellation section, the change of coordinates that take us from CCF to OCF is given by: \\[\n  T^{-1} = \\mathcal C_{(A_o, B_o)} \\mathcal C_{(A_c, B_c)}^{-1}\n  = \\MATRIX{ -2 - 3z & - z \\\\ -z & 1 }.\n\\] For \\(z = -1\\) or \\(z = -2\\), the change of coordinates is not full rank!"
  },
  {
    "objectID": "freq-modeling.html",
    "href": "freq-modeling.html",
    "title": "Frequency domain modeling",
    "section": "",
    "text": "A comment about notation\n\n\n\nIn Signals and Systems, we used \\(u(t)\\) to denote the step function. In this course, we will use \\(u(t)\\) to denote the control input. So, to avoid confusion, we will use \\(\\mathbb{1}(t)\\) to denote the step function. Note that the book uses \\(u(t)\\), which can get confusing, in my opinion.\nIn this lecture, we will review frequency domain modeling of LTI (linear and time invariant) systems. You have seen this material in Signals and Systems. The purpose of this review is to simply introduce the notation used in this course, in case it happens to be different from the notation used in Signals and Systems. You are strongly encouraged to review your notes on this topic."
  },
  {
    "objectID": "freq-modeling.html#unilateral-laplace-transforms",
    "href": "freq-modeling.html#unilateral-laplace-transforms",
    "title": "Frequency domain modeling",
    "section": "1 Unilateral Laplace Transforms",
    "text": "1 Unilateral Laplace Transforms\nRecall that there are two types of Laplace transforms (LTs): bi-lateral or two-sided LTs and unilateral or one-sided LTs. In this course, we will work exclusively with unilateral LTs.\nAlso recall that for two-sided LTs, we need to worry about the ROC (region of convergence). For one-sided LTs, we never explicitly mention the ROC since it is always the right hand plane right to the rightmost pole.\nWe will use the notation \\[f(t) \\xleftrightarrow{\\quad\\mathcal L\\quad} F(s)\\] to denote a LT-pair. We will also use \\[F(s) = \\mathcal{L}\\{ f(t)\\}\\] or \\[f(t) = \\mathcal L^{-1}\\{F(s)\\}\\] when we want to explicitly write the LT or the inverse LT of a signal.\n\nThe basic formula for a unilateral LT is \\[F(s) = \\int_{0-}^{\\infty} f(t)e^{-st} dt.\\] The inverse Laplace transform is given by \\[f(t) = \\frac{1}{2\\pi j} \\int_{\\sigma - j \\infty}^{\\sigma + j \\infty} F(s) e^{st} ds\\] where \\(\\sigma\\) is chosen such that \\((\\sigma,0)\\) is in the ROC.\nIn this course, we will never explicitly use these formulas to compute LTs. We will always use LT tables. You do not need to memorize the LT table; one will be provided during the examples. What you do need is the understanding of how to use the LT tables to compute LTs of complicated expressions and to compute inverse LTs using partial fraction expansion.\nI will not review this material in class. Please go back to your SS notes or review Sec 2.1–2.2 of the textbook. In is important to understand the following three cases:\n\nRoots of the denominator are real and distinct\nRoots of the denominator are complex and distinct\nRoots of the denominator are real and repeated."
  },
  {
    "objectID": "freq-modeling.html#the-transfer-function",
    "href": "freq-modeling.html#the-transfer-function",
    "title": "Frequency domain modeling",
    "section": "2 The Transfer Function",
    "text": "2 The Transfer Function\nIn this course, we will work exclusively with LTI systems. Such systems arise in all branches of engineering, e.g., electrical circuits, spring-mass systems, gear systems, and thermodynamics. See Chapter 2 of Nice for detailed modeling examples.\nConsider an LTI system with input \\(u(t)\\) (also called the reference signal sometimes) and output \\(y(t)\\). We can also represent this as the following block diagram:\n\n\n\n\n\n\nFigure 1: An LTI System\n\n\n\nAll such systems can be represented by constant coefficient linear differential equations (LDE) of the form \\[\na_n \\frac{d^n y(t)}{dt^n}  + a_{n-1} \\frac{d^{n-1} y(t)}{dt^{n-1}} + \\cdots  a_0 y(t)\n= b_m \\frac{d^m u(t)}{dt^m} + b_{m-1} \\frac{d^{m-1} u(t)}{dt^{m-1}} + \\cdots + b_0 u(t).\n\\]\n\nThis differential equation is called linear because there are no non-linear or multiplicative terms of the form \\(\\displaystyle \\left(\\frac{d^3 y(t)}{dt^3}\\right)\\left(\\frac{d^2 y(t)}{dt^2}\\right)\\).\nIt is called constant coefficient because the coefficients \\(a_n\\) and \\(b_m\\) are constants that do not depend on time.\n\nThere is a one-to-one correspondence between constant coefficient LDE and an LTI system. That is, every LTI system can be described by a constant coefficient LDE and vice-versa.\nRecall that a key defining property of an LTI system is its impulse response, which we will typically denote by \\(g(t)\\) and the transfer function, which we denote by \\(G(s)\\). Recall that \\(g(t) \\xleftrightarrow{\\quad \\mathcal L \\quad} G(s)\\).\nNow an interesting feature of a constant coefficient LDE is that we can identify the transfer function simply by inspection.\n\n\n\n\n\n\nFigure 2: An LTI System\n\n\n\nGiven an LTI system with input \\(u(t)\\) and output \\(y(t)\\), we know that \\[Y(s) = G(s)U(s).\\] Therefore, if we know the input and output, we can identify the transfer function using \\[ G(s) = \\frac{Y(s)}{U(s)}.\\] Now, if we go back to the general formula of a constant coefficient LDE \\[\na_n \\frac{d^n y(t)}{dt^n}  + a_{n-1} \\frac{d^{n-1} y(t)}{dt^{n-1}} + \\cdots  a_0 y(t)\n= b_m \\frac{d^m u(t)}{dt^m} + b_{m-1} \\frac{d^{m-1} u(t)}{dt^{m-1}} + \\cdots + b_0 u(t).\n\\] and assume that the system starts from zero-initial state and take the LT of both sides, we get \\[\na_n s^n Y(s) + a_{n-1} s^{n-1} Y(s) + \\cdots + a_0 Y(s)\n=\nb_m s^m U(s) + b_{m-1} s^{m-1} U(s) + \\cdots + b_m U(s).\n\\] Rearranging terms, we get \\[\n(a_n s^n + a_{n-1} s^{n-1} + \\cdots + a_0) Y(s)\n=\n(b_m s^m + b_{m-1}s^{m-1} + \\cdots + b_0) U(s).\n\\] Therefore, \\[\nG(s) = \\frac{Y(s)}{U(s)}\n= \\frac{b_m s^m + b_{m-1} s^{m-1} + \\cdots + b_m}\n{a_n s^n + a_{n-1} s^{n-1} + \\cdots + a_0}.\n\\] Thus, we can easily go back and forth between the DE and the transfer function.\n\nExercise 1 Find the transfer function corresponding to the following LDE \\[\n\\frac{d}{dt}y(t) + 2 y(t) = u(t)\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom inspection, we have \\[G(s) = \\frac{1}{s + 2}\\]\n\n\n\n\nExercise 2 Find a LDE that implements the following TF \\[\nG(s) = \\frac{2s+1}{s^2 + 2s + 3}.\n\\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom inspection, we have \\[\n\\frac{d^2c(t)}{dt} + 2\\frac{d y(t)}{dt} + 3c(t) = 2\\frac{d u(t)}{dt} + u(t).\n\\]\n\n\n\nIn this course, we will assume that the system is specified either as a LDE or as a TF. The textbook provides detailed examples of how to derive either the LDE or the TF from a physical system such as an electric circuit or a spring-mass system.\nWe present an alternative representation of the TF (pole-zero plot) in the next section. Later in the course we will also study other equivalent forms of representing the system such as state space equations and Bode plots.\nA final remark. In all the systems that we consider in this course, we will assume that \\(m &lt; n\\). So, the denominator of the TF has a strictly higher degree than the numerator. Such transfer functions are called proper. It is possible to have causal systems where \\(m=n\\) but we will not consider that in this course. As a consequence, when we consider state space representation in future lectures, we will get formulas which are simpler than more general formulas which you may find at other sources."
  },
  {
    "objectID": "freq-modeling.html#poles-and-zeros-of-a-transfer-function",
    "href": "freq-modeling.html#poles-and-zeros-of-a-transfer-function",
    "title": "Frequency domain modeling",
    "section": "3 Poles and Zeros of a Transfer Function",
    "text": "3 Poles and Zeros of a Transfer Function\nThe TF of an LTI system is always of the form: \\[\nG(s)\n= \\frac{b_m s^m + b_{m-1} s^{m-1} + \\cdots + b_m}\n{a_n s^n + a_{n-1} s^{n-1} + \\cdots + a_0}.\n\\]\nBoth the numerator and the denominator are polynomials in \\(s\\). So, we can factorize them and write the TF as \\[\nG(s)\n= K\n\\frac{(s-z_1)(s-z_2) \\cdots (s - z_m)}\n{(s-p_1)(s-p_2)\\cdots (s-p_n)}.\n\\] The roots of the numerator are called the zeros (because \\(G(z_i) = 0\\)). The roots of the denominator are called poles (because \\(G(p_i) = \\infty\\) and if we plot \\(G(s)\\) it will have a peak going to \\(∞\\) at \\(p_i\\); this peak looks like a pole). The constant \\(K\\) is called the gain (because, when none of the poles and zeros are at origin, the step response of \\(G(s)\\) will have a steady state value of \\(K\\)).\nWe often represent the poles and zeros using a pole-zero plot.\n\nPZplot = function(zeros, poles, xdomain, ydomain) {\n  return Plot.plot({\n    grid: true,\n    x: { domain: xdomain},\n    y: { domain: ydomain},\n\n    marks: [\n      // Axes\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      // Data\n      Plot.dot(zeros, {x:\"σ\", y:\"jω\", r: 5}),\n      Plot.dot(poles, {x:\"σ\", y:\"jω\", symbol: \"times\", r: 5}),\n    ]\n  })\n}\n\n\n\n\n\n\n\nExample 1 (Pole-zero plot) Consder \\[G(s) = \\frac{s+2}{(s+1)^2 + 1^2}.\\] The poles are \\(-1 \\pm j\\) and the zero is \\(-2\\). The pole-zero plot is shown below, where the location of the pole is represented by a “cross” and the location of the zero is represented by a “circle”.\n\nPZplot(\n       [ {σ: -2, jω: 0} ], \n       [ { σ: -1, jω: -1 }, {σ: -1, jω: 1 } ],\n       [-3, 3],\n       [-3, 3],\n      )\n\n\n\n\n\n\n\nSince the polynomials in the numerator and denominator of \\(G(s)\\) have real coefficients, the roots are either real or occur in complex conjugate pairs. So, the poles and zeros either lie on the \\(σ\\)-axis or are symmetric about the \\(σ\\)-axis.\nNote that the pole-zero plot does not capture the gain of the TF."
  },
  {
    "objectID": "freq-modeling.html#bibo-stability",
    "href": "freq-modeling.html#bibo-stability",
    "title": "Frequency domain modeling",
    "section": "4 BIBO Stability",
    "text": "4 BIBO Stability\nRecall from Signals and Systems that an LTI system with impulse response \\(g(t)\\) is BIBO stable if \\[\n\\int_{-∞}^{∞} |g(t)| dt &lt; \\infty.\n\\]\nFor a causal system (i.e., a system for which \\(g(t) = 0\\) for \\(t &lt; 0\\)), this is equivalent to \\[\n\\int_{0^{-}}^{∞} |g(t)| dt &lt; ∞.\n\\]\nThis implies that for any \\(σ \\in \\reals\\), \\(σ &gt; 0\\), we have \\[\n\\int_{0^{-}}^{∞} |g(t)| e^{-σt} dt &lt; ∞.\n\\]\nTherefore, there can be no pole in the open right hand plane \\(\\{ (σ + j ω) : σ &gt; 0 \\}\\). Thus, we have the following:\n\nA causal LTI system is stable if all poles are in the open left hand plane.\n\nPZplot(\n       [ {σ: 1.5, jω: 0} ], \n       [ { σ: -1, jω: -1 }, {σ: -1, jω: 1 }, {σ: -1.5, jω: 0} ],\n       [-2, 2],\n       [-2, 2],\n      )\nPZplot(\n       [ {σ: -1.5, jω: 0} ], \n       [ { σ: -1, jω: -1 }, {σ: -1, jω: 1 }, { σ: 1.5, jω: 0 } ],\n       [-2, 2],\n       [-2, 2],\n      )\nPZplot(\n       [ {σ: 1.5, jω: 0} ], \n       [ { σ: -1, jω: -1 }, {σ: -1, jω: 1 }, {σ: 0, jω: 0} ],\n       [-2, 2],\n       [-2, 2],\n      )\nPZplot(\n       [ {σ: 1.5, jω: 0} ], \n       [ { σ: -1, jω: -1 }, {σ: -1, jω: 1 }, {σ: -0.02, jω: 0}, {σ: 0.02, jω: 0} ],\n       [-2, 2],\n       [-2, 2],\n      )\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Stable systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Unstable systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Marginally systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Unstable systems (double pole at origin)\n\n\n\n\n\n\n\nFigure 3: Illustration of stable and unstable systems\n\n\n\nIf there are poles on the \\(j ω\\) axis and the poles have multiplicity 1, then the system is marginally stable; if poles have multiplicity greater than 1, then the system is unstable.\nIn the next lecture, we will see how to use the poles to identify the step response of the system."
  },
  {
    "objectID": "freq-modeling.html#step-response",
    "href": "freq-modeling.html#step-response",
    "title": "Frequency domain modeling",
    "section": "5 Step Response",
    "text": "5 Step Response\nOne of the configurations that we will really focus on in the course is the output of a stable system when the input is a step function. This is often the case when we specify a reference input such as the desired temperature of a room or the desired speed in cruise control of a car, and are interested in seeing if we get a “good” output. Note that this question is only of interest for stable systems. If the system is unstable, then the output will go to infinity.\nAs an example, consider the system of Exercise 2. What is the output when the input is a step function (assuming that the system starts from a zero initial state)?\nTo compute this, recall \\[\n\\mathbb{1}(t) \\xleftrightarrow{\\quad \\mathcal L\\quad} \\frac 1s.\n\\]\nThus, \\(U(s) = 1/s\\). We had already identified that \\(G(s) = 1/(s+2)\\). Thus, \\[\\begin{align*}\nY(s) &= G(s) U(s) \\\\\n     &= \\frac{1}{s(s+2)} \\\\\n     &= \\frac{ \\frac 12 }{s} - \\frac{ \\frac 12 }{ s + 2 }\n     \\quad \\text{[By partial fraction expansion]}.\n\\end{align*}\\] Thus, \\[\n  y(t) = \\left[ \\frac 12 - \\frac 12 e^{-2t} \\right] \\mathbb{1}(t).\n\\]"
  },
  {
    "objectID": "freq-modeling.html#interconnection-of-lti-systems",
    "href": "freq-modeling.html#interconnection-of-lti-systems",
    "title": "Frequency domain modeling",
    "section": "6 Interconnection of LTI Systems",
    "text": "6 Interconnection of LTI Systems\nThere are three common ways to inter connect LTI systems shown below.\n\n\n\n\n\n\nFigure 4: Series Connection equivalent to \\(G_1(s)G_2(s)\\).\n\n\n\n\n\n\n\n\n\nFigure 5: Parallel Connection equivalent to \\(G_1(s) + G_2(s)\\).\n\n\n\n\n\n\n\n\n\nFigure 6: Feedback Connection equivalent to \\(\\displaystyle \\frac{G(s)}{1 + G(s)H(s)}\\)."
  },
  {
    "objectID": "freq-modeling.html#time-delay-systems",
    "href": "freq-modeling.html#time-delay-systems",
    "title": "Frequency domain modeling",
    "section": "7 Time-delay systems",
    "text": "7 Time-delay systems\nIn a real system, there is often a delay between input and output, which is often modeled by a delay block whose output \\(y(t)\\) is a delayed version of the input \\(u(t)\\), by a delay time \\(τ\\), that is \\[\n  y(t) = u(t-τ).\n\\] Taking the Laplace transform of both sides, we get \\[\nY(s) = e^{-τs}U(s).\n\\] Therefore, the TF of time-delay unit is \\(e^{-τs}\\), which is different from the TFs studied so far as it is not a rational polynomial of \\(s\\). It is possible to approximate \\(e^{-τs}\\) using a rational polynomial using what is called Padé approximation:\n\nFirst-order Padé approximation: \\[ e^{-τs} \\approx \\dfrac{1 - τs/2}{1 + τs/2}. \\]\nSecond-order Padé approximation: \\[ e^{-τs} \\approx \\dfrac{1 - τs/2 + (τs)^2/12}{1 + τs/2 + (τs)^2/12}. \\]"
  },
  {
    "objectID": "step-response.html",
    "href": "step-response.html",
    "title": "Step Response",
    "section": "",
    "text": "In the previous lecture, we saw that we can use the pole zero plot to determine if a system is BIBO stability. We also saw that we can compute the step reponse using inverse LTs. In this lecture, we will develop a heuristic method to approximate the step response from a pole-zero plot."
  },
  {
    "objectID": "step-response.html#step-response-from-pole-zero-plot",
    "href": "step-response.html#step-response-from-pole-zero-plot",
    "title": "Step Response",
    "section": "1 Step response from pole-zero plot",
    "text": "1 Step response from pole-zero plot\nTo understand how to infer the step response from the pole-zero plot, let’s revisit the example from last lecture:\n\n\n\n\n\n\nFigure 1: Block diagram for a simple LTI system\n\n\n\nRecall that we simplified the partial fraction expansion as \\[ \\frac{\\frac12}{s} - \\frac{\\frac12}{s+2}.\\]\nConsequently, the step response (which is the inverse LT of the above) is: \\[\n\\Bigl[ \\underbrace{\\frac 12}_{\\text{Forced response}}\n-\n\\underbrace{\\frac 12 e^{-2t}}_{\\text{Natural response}}\n\\Bigr] \\mathbb{1}(t).\n\\]\nThe salient features of the step response are:\n\nThe pole of the input generates a forced response.\nThe pole of the system TF generates a natural response.\nThe amplitude of the response depends on the exact values of the poles and zeros, but the form of the response only depends on the location of the poles.\n\nThus, even without doing any exact partial fraction expansion, we know that \\[\n\\dfrac{1}{s(s+2)} = \\frac{K_1}{s} + \\frac{K_2}{s+2}.\n\\] Thus, we know that the output will be of the form: \\[\ny(t) = \\bigl[ K_1 + K_2 e^{-2t} \\bigr] \\mathbb{1}(t).\n\\]\nThus, we can obtain the form of the output without any explicit calculations.\n\nExercise 1 Find the general form of the step response of the following systems:\n\n\\(\\displaystyle\nG(s) = \\frac{ s + 2 } { (s+5)(s+10) }.\\)\n\\(\\displaystyle\nG(s) = \\frac{ 5(s+3)(s+10) }{ (s+1)(s+5)(s+20) }.\\)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(y(t) = \\bigl[\nK_1 + K_2 e^{-5t} + K_3 e^{-10t}\n\\bigr] \\mathbb{1}(t).\\)\n\\(y(t) = \\bigl[\nK_1 + K_2 e^{-t} + K_3 e^{-5t}  + K_4 e^{-20t}\n\\bigr] \\mathbb{1}(t).\\)"
  },
  {
    "objectID": "step-response.html#dc-gain",
    "href": "step-response.html#dc-gain",
    "title": "Step Response",
    "section": "2 DC Gain",
    "text": "2 DC Gain\nIn the above examples, the system does not have a pole at origin. Such systems are called type 0 systems. For stable type 0 systems, the step response always settles to a steady state value known as the DC gain. For a type 0 system with TF \\(G(s)\\), the DC gain is given by \\[\n\\text{DC-gain} = G(0).\n\\]\nWe will expand more on this point later in the course."
  },
  {
    "objectID": "step-response.html#dominant-poles-and-approximate-system-response",
    "href": "step-response.html#dominant-poles-and-approximate-system-response",
    "title": "Step Response",
    "section": "3 Dominant poles and approximate system response",
    "text": "3 Dominant poles and approximate system response\nNow we compare the step response of two systems\n\n\\(G_1(s) = \\dfrac{1}{(s+2)}\\)\n\\(G_2(s) = \\dfrac{20}{(s+2)(s+20)}\\).\n\nNote that both systems are stable type 0 systems with a DC gain of \\(0.5\\). So, we expect them to settle at a steady state value of \\(0.5\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Step response of \\(G_1(s)\\) and \\(G_2(s)\\).\n\n\n\n\nThe step responses of both systems are very close. Thus, we can approximate \\(G_2(s)\\) by \\(G_1(s)\\). Let’s look at the partial fraction expansion to see what is happening.\n\n\\(\\displaystyle\nY_1(s) = \\frac{1}{s(s+2)} = \\frac{0.5}{s} - \\frac{0.5}{s+2}.\\)\n\\(\\displaystyle\nY_2(s) = \\frac{20}{s(s+2)(s+20)}\n= \\frac{0.5}{s} - \\frac{0.55}{s+2} + \\frac{0.055}{s+20}.\\)\n\nNote that the first two terms of \\(Y_2(s)\\) are almost the same as \\(Y_1(s)\\). Why is the third term \\(0.055/(s+20)\\) negligible? The factor \\(0.055\\) is small, but that is not the main reason for the third term to be small. To see what is happening, let’s compute the inverse LTs:\n\n\\(y_1(t) = [ 0.5 - 0.5 e^{-2t} ] \\mathbb{1}(t).\\)\n\\(y_2(t) = [0.5 - 0.55 e^{-2t} + 0.055 e^{-20t}.\\)\n\nThe reason that we can ignore \\(0.55/(s+20)\\) is that \\(e^{-20t}\\) decays much faster than \\(e^{-2t}\\). In particular, at \\(t=1\\),\n\n\\(e^{-2t} \\approx 0.1353\\)\n\\(e^{-20t} \\approx 2.06 \\times 10^{-9}\\).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Decaying exponentials\n\n\n\n\nIn this example, we say that in system \\(G_2(s)\\) the pole at \\(p_1 = -2\\) dominates the pole at \\(p_2 = -20\\). In this course, we will follow the heuristic that pole \\(p_1\\) dominates pole \\(p_2\\) if \\[\\begin{equation}\\label{eq:dominant-pole}\n\\ABS{\\text{Re}(p_2)} \\ge 5 \\ABS{\\text{Re}(p_1)}\n\\end{equation}\\]\nDominant pole approximation refers to the approximate system where the dominated poles are removed. In particular, to obtain a dominant pole approximation of a system with poles \\(\\{p_1, \\dots, p_n\\}\\), we split the poles into two sets \\(\\mathcal P_d = \\{p_1, \\dots, p_k\\}\\) and \\(\\mathcal P_r = \\{p_{k+1}, \\dots, p_n\\}\\) which has the following property: every pole \\(p_i\\) in \\(\\mathcal P_d\\) dominates every pole \\(p_j\\) in \\(\\mathcal P_r\\). Then, the dominant pole approximation ignores all the poles in the set \\(\\mathcal P_r\\).\n\nExample 1 Consider the system \\[\n  G_1(s) = \\frac{10}{(s+1)(s+2)(s+10)}.\n\\] In this case, the pole at \\(-10\\) is dominated by both poles \\(\\{-1,-2\\}\\). Thus, the pole at \\(-10\\) can be ignored and the simplified system is \\[\n  \\tilde G_1(s) = \\frac{1}{(s+1)(s+2)}.\n\\]\nNote: Note that we have picked the gain of \\(\\tilde G_1(s)\\) so that \\(G_1(s)\\) and \\(\\tilde G_1(s)\\) have the same DC gain.\nWe can confirm this result by comparing the step responses:\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Step response of \\(G(s)\\) and \\(\\tilde G(s)\\) in Example 1\n\n\n\n\n\n\nExample 2 Consider the system \\[\n  G_2(s) = \\frac{10}{(s+1)(s+4)(s+10)}.\n\\] In this case, we cannot ignore any pole. Note that the pole at \\(-1\\) does not dominate the pole at \\(-4\\), and the pole at \\(-4\\) does not dominate the pole at \\(-10\\). Thus, we cannot partition the poles into two sets such that one set dominates the other. Thus, the above system cannot be simplied to \\[\n  \\tilde G_2(s) = \\frac{1}{(s+1)(s+4)}.\n\\] We can confirm this result by comparing the step responses:\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Step response of \\(G(s)\\) and \\(\\tilde G(s)\\) in Example 2\n\n\n\n\n\nWait what? Why is Figure 4 a good approximation while Figure 5 is not? Both of them look equally good or bad. This example highlights that we have to be careful with interpretting the dominant pole approximation heuristic. It says that if \\(\\eqref{eq:dominant-pole}\\) is satisfied then we expect the approximate system to behave the same as the original system. If \\(\\eqref{eq:dominant-pole}\\) is not satisfied, then there might be instances where the approximate system is different from the original system. For example, let’s consider: \\[\n  G_1(s) = \\frac{10}{( (s+2)^2 + 4^2 )(s + 10) }\n  \\stackrel{?}{\\approx}\n  \\tilde G_1(s) = \\frac{1}{((s+2)^2 + 4^2)}\n\\] versus \\[\n  G_2(s) = \\frac{10}{( (s+4)^2 + 8^2 )(s + 10) }\n  \\stackrel{?}{\\approx}\n  \\tilde G_2(s) = \\frac{1}{((s+4)^2 + 8^2)}\n\\] The dominant pole approximation heuristic says that the first approximation should a good approximation while the second may not be. We plot the two step responses to evalaute:\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Checking the quality of approximations"
  },
  {
    "objectID": "step-response.html#high-level-system-design-idea",
    "href": "step-response.html#high-level-system-design-idea",
    "title": "Step Response",
    "section": "4 High-level system design idea",
    "text": "4 High-level system design idea\nIn the next few weeks, we will learn techniques to design system controllers so that we can place the poles of a closed loop system at any desired location. But how do we choose where we want to place the poles of the closed loop system?\nOf course, the first objective is to make sure that the closed loop system is stable. However, a practical design goes beyond stability. Suppose we are designing the cruise controller of a car and have an option of the following three controllers.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Step responses of three different systems\n\n\n\n\nIt is clear from the above plots that the transient response of the system matters as well. For a higer order system, finding the relationship between the location of the poles and the transient response can be difficult. To circumvent this difficulty, our general design philosophy will be as follows:\n\nWe will first understand the relationship between the location of the poles and the transient behavior of a first and second order system.\nLater, when it comes to system design, we will place the poles of the closed loop system such that the dominant pole approximation of the system is equivalent to a second order system."
  },
  {
    "objectID": "step-response.html#time-response-of-first-order-systems",
    "href": "step-response.html#time-response-of-first-order-systems",
    "title": "Step Response",
    "section": "5 Time response of first order systems",
    "text": "5 Time response of first order systems\nA general first order system is of the form \\[\n\\dfrac{dy(t)}{dt} + a y(t) = b u(t).\n\\] By inspection, we know that the transfer function is \\[\n  \\frac{b}{s+a} = \\frac{b}{a} \\cdot \\frac{a}{s+a}\n  = K \\dfrac{a}{s+a}.\n\\]\nExamples of first order systems include circuits with only one energy storage element, e.g., an RC circuit or an RL circuit or a DC-motor when mechanical aspects such as friction and inertia are also considered.\nThe step response of a first order system is shown below.\n\n\n\n\n\n\nFigure 8: A general first order system\n\n\n\n\n\n\n\n\nviewof a_1 = Object.assign(Inputs.range([0.5, 2.5], {label: \"a\", step: 0.1, value: 1 }), {style: '--label-width:20px'})\n\nviewof K_1 = Object.assign(Inputs.range([0.5, 1.5], {label: \"K\", step: 0.1, value: 1 }), {style: '--label-width:20px'})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntransformed_first_order = first_order.map(d =&gt;({...d, output: d.output*K_1}))\n\nPZplot = function(zeros, poles, xdomain, ydomain) {\n  return Plot.plot({\n    grid: true,\n    x: { domain: xdomain},\n    y: { domain: ydomain},\n\n    marks: [\n      // Axes\n      Plot.ruleX([0]),\n      Plot.ruleY([0]),\n      // Data\n      Plot.dot(zeros, {x:\"σ\", y:\"jω\", r: 5}),\n      Plot.dot(poles, {x:\"σ\", y:\"jω\", symbol: \"times\", r: 5}),\n    ]\n  })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPZplot(\n       [], \n       [ {σ: -a_1, jω: 0} ], \n       [-3, 3],\n       [-3, 3],\n      )\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.6] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // a_1 is still 64 bit float while a is 16 bit float. So we do a crude comparison for equality. \n    Plot.line(transformed_first_order.filter(d =&gt; Math.abs(d.a - a_1) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 9: Step response of a first order system for different values of the parameters\n\n\n\nWe can compute the step response by the usual partial fraction expansion: \\[ Y(s) = K \\frac{a}{s+a} = K \\biggl[ \\frac{1}{s} - \\frac{1}{s+a}\\biggr]. \\] Taking inverse LTs, we get: \\[ y(t) = K \\bigl[ 1 - e^{-at} \\bigr] \\mathbb{1}(t). \\]\n\n5.1 Important characteristics of the step response\nThe response of a first order system has the following features:\n\nThere is no overshoot\nThe initial slope (at \\(t=0\\)) is non-zero and equal to \\(a\\).\n\n\n5.1.1 Time constant \\(τ\\)\n\nThe parameter \\(τ = 1/a\\) is called the time constant of a first order system.\nThe intial slope of the step response is \\(a\\) (i.e., \\(1/τ\\)).\nAt \\(t=τ\\), \\(c(τ) = K(1-e^{-1}) \\approx 63\\%\\) of final value.\n\n\n\n5.1.2 Rise time \\(T_r\\)\n\nThe rise time, denoted by \\(T_r\\) is the time to go from \\(10\\%\\) to \\(90\\%\\) of the final value.\nTo find time \\(t_1\\) when the response reaches \\(10\\%\\) of the final value: \\[\n  c(t_1) = 1 - e^{-a t_1} = 0.1\n  \\implies\n  t_1 = \\frac{0.11}{a}.\n\\]\nSimilarly, to find time \\(t_2\\) when the response reaches \\(90\\%\\) of the final value: \\[\n  c(t_2) = 1 - e^{-a t_2} = 0.9\n  \\implies\n  t_2 = \\frac{2.31}{a}.\n\\]\nThus, \\[\n\\bbox[5pt,border: 1px solid]{T_r = \\frac{2.31}{a} - \\frac{0.11}{a} = \\frac{2.2}{a}}\n\\]\n\n\n\n5.1.3 Settling time \\(T_s\\)\n\nThe (2%) setting time is the time required for the step response to reach 2% of its final value.\nTo find \\(T_s\\), we solve \\[\n(1-e^{-a T_s}) = 0.98\n\\implies\n\\bbox[5pt,border: 1px solid]{T_s = \\frac 4a}\n\\]\n\n\nExample 3 Consider the TF \\(G(s) = \\dfrac{100}{s+50}\\). Identify the time constant \\(τ\\), rise time \\(T_r\\), and settling time \\(T_s\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe start by writing the TF in standard form: \\[\n  G(s) = 2 \\frac{50}{s+50}.\n\\] Comparing from the standard form, we have \\(K=2\\) and \\(a=50\\). Thus,\n\n\\(τ = \\dfrac{1}{a} = 0.02\\).\n\\(T_r = \\dfrac{2.2}{a} = 2.2 τ = 0.044\\) s.\n\\(T_s = \\dfrac{4}{a} = 4 τ = 0.08\\) s."
  },
  {
    "objectID": "step-response.html#identifying-a-first-order-system-via-testing",
    "href": "step-response.html#identifying-a-first-order-system-via-testing",
    "title": "Step Response",
    "section": "6 Identifying a first-order system via testing",
    "text": "6 Identifying a first-order system via testing\nIn many applications, we may not know the TF of a system and may need to identify the TF from the measurements of the step response. We will do such an experiment in Lab 3. In such cases, we can identify that the system is first order from the following features:\n\nno overshoot\nnon-zero initial slope.\n\nFor a first order system, we need to identify two parameters: \\(K\\) and \\(a\\).\n\nThe gain \\(K\\) is equal to the final value of the step response.\nTo identify \\(a\\), we identify the time constant \\(τ=1/a\\) as the time where the step response is \\(0.63K\\).\n\nThen, the TF is \\[ G(s) = K \\frac{a}{s+a}. \\]\n\nExample 4 Identify the transfer function from the following step response\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSince there is no overshoot and the initial slope is non-zero, this is the step response of a first order system.\n\nThe final DC value of response is \\(2.5\\). Thus, \\(K = 2.5\\).\nNow we search for the time when the response is \\(0.63K = 1.575\\) which happens around \\(τ=0.25\\). Thus, \\(a = 1/τ = 4\\).\n\nHence \\(G(s) = K \\dfrac{a}{s+a} = \\dfrac{10}{s+4}\\)."
  },
  {
    "objectID": "step-response.html#types-of-second-order-systems",
    "href": "step-response.html#types-of-second-order-systems",
    "title": "Step Response",
    "section": "7 Types of second order systems",
    "text": "7 Types of second order systems\nA general second order system is of the form \\[\n\\frac{d^2 y(t)}{dt^2} + a_1 \\frac{d y(t)}{dt} + a_0 y(t) = b_0 u(t).\n\\] By inspection, we know that the transfer function is \\[\nG(s) = \\frac{b_0}{s^2 + a_1 s + a_0}\n=\nK \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}\n\\] where \\(K\\) is called the gain, \\(ω_n\\) is called the natural frequency and \\(ζ\\) is called the damping coefficient.\nExamples of second order system include circuits with two energy storage elements such as an RCL circuit or a DC-motor with voltage as an input and angular position as the output when the mechanical aspects such as inertia and friction are ignored.\nThe step response of a second order system is illustrated below in Figure 10. When \\(ζ &lt; 0\\), the system is unstable and we will not discuss that case. When \\(ζ \\ge 0\\), we observe four types of behavior depending on the value of \\(ζ\\). These are called the categories of a second order system:\n\nUndamped \\(ζ = 0\\), in which case the poles are at \\(\\pm j ω_n\\).\nUnderdamped \\(0 &lt; ζ &lt; 1\\), in which case the poles are at \\(-ω_n( ζ \\pm j\\sqrt{1 - ζ^2}).\\)\nCritically damped \\(ζ = 1\\), in which case there is a double pole at \\(-ω_n\\).\nOverdamped \\(ζ &gt; 1\\), in which case the poles are at \\(-ω_n(ζ \\pm \\sqrt{ζ^2 - 1}).\\)\n\nFigure 10 illustrates the impact of the damping coefficient on the location of the poles:\n\n\n\n\n\n\nviewof zeta_damping = Object.assign(Inputs.range([0, 2], {label: \"ζ\", step: 0.05, value: 0.5 }), {style: '--label-width:20px'})\n\n  ω_damping = 1\n\n  poles_damping = {\n    var factor\n    var poles\n\n    if (zeta_damping == 0) {\n      poles = [ { σ: 0, jω: ω_damping },\n                { σ: 0, jω: -ω_damping } ] \n    } else if (0 &lt; zeta_damping && zeta_damping &lt; 1) { \n      factor = Math.sqrt(1-zeta_damping*zeta_damping)\n      poles = [ { σ: -ω_damping*zeta_damping, jω: ω_damping*factor }, \n                { σ: -ω_damping*zeta_damping, jω: -ω_damping*factor } ] \n    } else {// zeta_damping &gt; 1\n      factor = Math.sqrt(zeta_damping*zeta_damping-1)\n      poles = [ { σ: -ω_damping*zeta_damping + ω_damping*factor, jω: 0 }, \n                { σ: -ω_damping*zeta_damping - ω_damping*factor, jω: 0 } ] \n    }\n    return poles\n  }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPZplot(\n       [], \n       poles_damping,\n       [-4.5*ω_damping, 0.5*ω_damping],\n       [-1.5*ω_damping, 1.5*ω_damping],\n      )\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.6] },\n  x: { domain: [0,8] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // zeta_damping is still 64 bit float while a is 16 bit float. So we do a crude comparison for equality. \n    Plot.line(second_order_zeta.filter(d =&gt; Math.abs(d.ζ - zeta_damping) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 10: Step response as a function of damping coefficient\n\n\n\n\nExercise 2 Indentify the category of the following second order systems:\n\n\\(\\dfrac{12}{s^2 + 8s + 12}.\\)\n\\(\\dfrac{16}{s^2 + 8s + 16}\\).\n\\(\\dfrac{20}{s^2 + 8s + 20}\\).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(ζ = \\dfrac{8}{2\\sqrt{12}} \\approx 1.1547\\). Thus, the system is overdamped.\n\\(ζ = \\dfrac{8}{2\\sqrt{16}} = 1\\). Thus, the system is critically damped.\n\\(ζ = \\dfrac{8}{2\\sqrt{20}} \\approx 0.8944\\). Thus, the system is underdamped."
  },
  {
    "objectID": "step-response.html#step-response-of-undamped-second-order-system",
    "href": "step-response.html#step-response-of-undamped-second-order-system",
    "title": "Step Response",
    "section": "8 Step response of undamped second order system",
    "text": "8 Step response of undamped second order system\nFor \\(ζ = 0\\), the TF is \\[\nG(s) = K \\frac{ω_n^2}{s^2 + ω_n^2}\n= K \\frac{ω_n^2}{ (s+j ω_n)(s - j ω_n) }.\n\\]\nThis would correspond to a LC circuit (without resistance) or a spring mass system (without damping). These are idealized models, so the step response is mainly for academic interest. Although we derive the step response below, we will not investigate the undamped system in this course.\nTo find the step response, we observe the following formula from the LT tables: \\[\n\\dfrac{a^2 + b^2}{s( (s+a)^2 + b^2 )}\n\\xleftrightarrow{\\quad \\mathcal L\\quad}\n\\biggl[ 1 - e^{-at}\\bigg( \\cos bt + \\frac{a}{b}\\sin bt \\biggr) \\biggr].\n\\IND(t)\n\\]\nChoosing \\(a = 0\\) and \\(b = ω_n\\), we get \\[\nK \\frac{1}{s} \\cdot \\frac{ω_n^2}{s^2 + ω_n^2}\n\\xleftrightarrow{\\quad \\mathcal L\\quad}\nK [1 - \\cos ω_n t] \\IND(t).\n\\]\n\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 2.2] },\n  x: { domain: [0,8] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(second_order_zeta.filter(d =&gt; Math.abs(d.ζ - 0) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 11: Step response of undamped system"
  },
  {
    "objectID": "step-response.html#step-response-of-underdamped-second-order-system",
    "href": "step-response.html#step-response-of-underdamped-second-order-system",
    "title": "Step Response",
    "section": "9 Step response of underdamped second order system",
    "text": "9 Step response of underdamped second order system\nFor the case when \\(0 &lt; ζ &lt; 1\\), we can write the TF as \\[\nG(s) = \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}\n=\n\\frac{σ^2 + ω_d^2}{(s+σ)^2 + ω_d^2}\n\\] where \\(σ = ζ ω_n\\) and \\(ω_d = \\sqrt{1 - ζ^2} ω_n\\). Note that we have \\[ σ^2 + ω_d^2 = ω_n.\\] The parameter \\(ω_n\\) is called the natural frequency. It is the frequency at which the system will oscillate when there is no damping. The parameter \\(ω_d\\) is called the damped frequency. As we will show below, it is the frequency at which the damped system oscillates.\nThe system has two complex conjugate roots. In cartesian coordinates, these roots are at \\(-σ \\pm j ω_d\\), while in polar coordinates they are at \\(ω_n e^{\\pm j(π/2 + φ)}\\), where \\(\\phi = \\sin^{-1}ζ\\). The relationship between the different quantities is shown in Figure 12.\n\n\n\n\n\n\nFigure 12: Relationship between different quantities for a second order system\n\n\n\nTo find the step response, we again recall the following LT formula from the LT tables: \\[\n\\dfrac{a^2 + b^2}{s( (s+a)^2 + b^2 )}\n\\xleftrightarrow{\\quad \\mathcal L\\quad}\n\\biggl[ 1 - e^{-at}\\bigg( \\cos bt + \\frac{a}{b}\\sin bt \\biggr) \\biggr].\n\\IND(t)\n\\] Taking \\(a = σ\\) and \\(b = ω_d\\), we get that the step response is \\[\\begin{align*}\ny(t) &= K \\biggl[ 1 - e^{-σt} \\biggl(\\cos ω_d t + \\frac{σ}{ω_d} \\sin ω_d t\\biggr)\\biggr] \\IND(t) \\\\\n&= K \\biggl[ 1 - \\frac{1}{\\sqrt{1 - ζ^2}} e^{-σ t} \\cos(ω_dt - \\phi) \\biggr] \\IND(t)\n\\end{align*}\\] where as before \\(φ = \\sin^{-1} ζ\\).\n\n9.1 Important characteristics of the step response\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 13: Step response of underdamped system for \\(ζ = 0.5\\)\n\n\n\n\nThe response of an underdamped second order system has the following features:\n\nThe initial slope (at \\(t=0\\)) is zero.\nThe output oscillates at frequency \\(ω_d\\). It overshoots and then settles back to the final DC value of \\(K\\).\nThe maximum peak, denoted by \\(M_p\\) is the maximum value of the step response.\nRather than working with maximum peak, we often use percentage overshoot which is defined as \\[\\% {\\rm OS} = \\frac{M_p - K}{K} × 100\\] which does not depend on the gain \\(K\\). We will show that \\[\n\\bbox[5pt,border: 1px solid]\n{\\% {\\rm OS} = e^{-π ζ/\\sqrt{1 - ζ^2}} × 100\n= e^{-π σ/ω_d} × 100}\n\\] where the last equality uses the fact that \\(ζ/\\sqrt{1-ζ^2} = σ/ω_d\\). We often use this relationship in the reverse, i.e., \\[\nζ = \\frac{- \\ln(\\%{\\rm OS}/100)}{\\sqrt{π^2 + \\ln^2(\\%{\\rm OS}/100)}}.\n\\]\nThe time needed to reach the maximum overshoot is called peak time and denoted by \\(T_p\\). We will show that \\[\n\\bbox[5pt,border: 1px solid]\n{T_p = \\frac{π}{ω_d} = \\frac{π}{ω_n \\sqrt{1 - ζ^2}}}\n\\]\nThe time after which the system stays within \\(Δ\\%\\) of the final value is called the \\(Δ\\%\\) settling time and denoted by \\(T_s(Δ\\%)\\). We will show that\n\n\\(T_s(1\\%) = \\dfrac{4.6}{σ}\\)\n\\(T_s(2\\%) = \\dfrac{4}{σ}\\)\n\\(T_s(5\\%) = \\dfrac{3}{σ}\\).\n\nThe time required for the response to go from \\(0.1\\) of the final value to \\(0.9\\) of the final value is called rise time and denoted by \\(T_r\\). It is difficult to get a closed form expression for \\(T_r\\). It can be verified numerically that \\(ω_n T_r\\) is approximately a constant that depends on \\(ζ\\). So we approximately compute \\(T_r\\) from a look-up table using interpolation:\n\n\n\nTable 1: \\(ω_n T_r\\) as a function of \\(ζ\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(ζ\\)\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\n\n\n\n\\(ω_n T_r\\)\n1.104\n1.203\n1.321\n1.463\n1.638\n1.854\n2.216\n2.467\n2.883\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the formulas\n\n\n\n\n\n\nAt \\(T_p\\), we should have \\(dy(t)/dt = 0\\). Recall that \\[\ny(t) = K \\biggl[ 1 - e^{-σt} \\biggl(\\cos ω_d t + \\frac{σ}{ω_d} \\sin ω_d t\\biggr)\\biggr] \\IND(t)\n\\] Thus, \\[\\begin{align*}\n\\frac{dy(t)}{dt} &= K σ e^{-σt} \\left( \\cancel{\\cos ω_d t} + \\frac{σ}{ω_d} \\sin ω_d t\\right)\n\\\\\n&\\quad\n- K e^{-σt}( -ω_d \\sin ω_d t + \\cancel{σ \\cos ω_d t} )\n\\\\\n&= K e^{-σt} \\left[ \\frac{σ^2}{ω_d} + ω_d \\right] \\sin ω_d t\n\\end{align*}\\] For \\(dy(t)/dt = 0\\), we must have \\[\nω_d t = n π, \\quad n \\in \\mathbb{N}.\n\\] From the step response, we can see that the first peak is the highest. So, we pick \\(n=1\\) and thus, \\[\nT_p = \\frac{π}{ω_d}.\n\\]\nNow, for \\(K=1\\), we have \\[\\begin{align*}\n\\%{\\rm OS}  &= y(T_p) - 1 \\\\\n&= -e^{-σt}\\biggl(\n\\underbrace{\\cos ω_d T_p}_{=-1} + \\frac{σ}{ω_d} \\underbrace{\\sin ω_d t}_{=0} \\biggr)\n\\\\\n&= e^{-σ T_p} = e^{-π σ / ω_d }\n\\end{align*}\\]\nFor the derivation of the setting time, we use the other form of \\(y(t)\\): \\[\ny(t) = K \\biggl[ 1 - \\frac{1}{\\sqrt{1 - ζ^2}} e^{-σ t} \\cos(ω_dt - \\phi) \\biggr] \\IND(t)\n\\] For simplicity, we simply consider the envelop \\[\n\\bar y(t) = K \\biggl[ 1 - \\frac{e^{-σ t}}{\\sqrt{1 - ζ^2}} \\biggr]\n\\] and consider the value of \\(T_s\\) such that \\(\\bar y(T_s) = 0.98K\\). This is equivalent to \\[\n\\frac{e^{-σ T_s}{\\sqrt{1 - ζ^2}} = 0.02.\n\\] Thus, \\[T_s = \\frac{- \\ln(0.02 \\sqrt{1 - ζ^2})}{σ}.\\] We observe that as \\(ζ\\) varies from \\(0.1\\) to \\(0.9\\), \\(T_s\\) varies from \\(3.91/σ\\) to \\(4.72/σ\\). For simplicity, we choose \\(T_s = 4/σ\\).\n\n\n\n\n\nExample 5 Consider the TF \\[ G(s) = \\frac{100}{s^2 + 15s + 100}. \\] Find \\(T_p\\), \\(\\%{\\rm OS}\\), \\(T_s(2\\%)\\), and \\(T_r\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom inspection, we get that \\(ω_n^2 = 100\\) and \\(2 ζ ω_n = 15\\). Thus, \\(ω_n = 10\\) and \\(ζ  = 0.75\\). Then we have\n\n\\(T_p = \\dfrac{π}{ω_d} = 0.475\\).\n\\(\\%{\\rm OS} = \\exp\\biggl(-\\dfrac{π σ}{ω_d}\\biggr) × 100 = 2.28\\%\\).\n\\(T_s(2\\%) = \\dfrac{4}{σ} = 0.533\\) sec.\nFrom Table 1, we get that for \\(ζ = 0.75\\), \\(ω_n T_r \\approx (2.216 + 2.467)/2 = 2.3415\\). Thus, \\(T_r = 2.3415/ω_n = 0.23415\\) sec."
  },
  {
    "objectID": "step-response.html#features-of-time-response-in-terms-of-location-of-poles",
    "href": "step-response.html#features-of-time-response-in-terms-of-location-of-poles",
    "title": "Step Response",
    "section": "10 Features of time response in terms of location of poles",
    "text": "10 Features of time response in terms of location of poles\n\n10.1 Same damped frequency\nThe peak time depends on the damped frequency. So, if we change \\(σ\\) while keeping \\(ω_d\\) fixed, the peak time of the system remains the same.\n\n\n\n\n\nviewof param_sigma = Object.assign(Inputs.range([0.1,4], {label: \"σ\", step: 0.1, value: 2 }), {style: '--label-width:20px'})\n\nω_d_sigma = 2\nT_p_sigma = Math.PI/ω_d_sigma\nM_p_sigma = 1 + Math.exp(-param_sigma*T_p_sigma)\npeak_sigma = [ {x: T_p_sigma, y: 0}, {x: T_p_sigma, y: M_p_sigma} ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPZplot(\n       [], \n       [ {σ: -param_sigma, jω: ω_d_sigma}, {σ: -param_sigma, jω: -ω_d_sigma} ], \n       [-3, 3],\n       [-5, 5],\n      )\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.6] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // param_sigma is still 64 bit float while a is 16 bit float. So we do a crude comparison for equality. \n    Plot.line(second_order_sigma.filter(d =&gt; Math.abs(d.σ - param_sigma) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n    Plot.dot([ [T_p_sigma, M_p_sigma] ], {fill: \"red\", r:4}),\n    Plot.line(peak_sigma, {x: \"x\", y: \"y\", stroke: \"red\"})\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 14: Step response of a second order system for fixed \\(ω_d\\) and different values \\(σ\\).\n\n\n\n\n\n10.2 Same \\(σ\\)\nThe settling time depends on \\(σ\\). So, if we change \\(ω_d\\) while keeping \\(σ\\) fixed, the settling time of the system remains the same.\n\n\n\n\n\nviewof param_ωd = Object.assign(Inputs.range([0.1,4], {label: \"ωd\", step: 0.1, value: 2 }), {style: '--label-width:20px'})\n\nσ_ωd = 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPZplot(\n       [], \n       [ {σ: -σ_ωd, jω: param_ωd}, {σ: -σ_ωd, jω: -param_ωd} ], \n       [-3, 3],\n       [-5, 5],\n      )\nPlot.plot({\n  grid: true,\n  clip: true,\n  y: { domain: [0, 1.6] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // param_ωd is still 64 bit float while a is 16 bit float. So we do a crude comparison for equality. \n    Plot.area([ {x1:4/σ_ωd, y1: 1.02, x2:4/σ_ωd, y2: 0.98}, \n                {x1:8, y1: 1.02, x2:8, y2: 0.98}],\n              {x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", fill:\"pink\"}),\n    Plot.line(second_order_ωd.filter(d =&gt; Math.abs(d.ωd - param_ωd) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 15: Step response of a second order system for fixed \\(ζ\\) and different values \\(ω_n\\).\n\n\n\n\n\n10.3 Same damping coefficient\nThe peak overshoot depends on the damping coefficient. So, if we change \\(ω_n\\) keeping \\(ζ\\) constant, the peak overshoot (and therefore percentage overshoot) remains the same.\n\n\n\n\n\nviewof param_ωn = Object.assign(Inputs.range([0.1,4], {label: \"ωn\", step: 0.1, value: 2 }), {style: '--label-width:20px'})\n\nζ_ωn = 0.4\nM_p_zeta = 1 + Math.exp(-Math.PI*ζ_ωn/Math.sqrt(1 - ζ_ωn**2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPZplot(\n       [], \n       [ {σ: -param_ωn*ζ_ωn, jω:  param_ωn*Math.sqrt(1 - ζ_ωn**2) },\n         {σ: -param_ωn*ζ_ωn, jω: -param_ωn*Math.sqrt(1 - ζ_ωn**2) } ],\n       [-3, 3],\n       [-5, 5],\n      )\nPlot.plot({\n  grid: true,\n  clip: true,\n  y: { domain: [0, 1.6] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    // param_ωn is still 64 bit float while a is 16 bit float. So we do a crude comparison for equality. \n    Plot.line([{x:0, y: M_p_zeta}, {x:8, y: M_p_zeta}], {x:\"x\", y:\"y\", stroke:\"red\"}),\n    Plot.line(second_order_ωn.filter(d =&gt; Math.abs(d.ωn - param_ωn) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 16: Step response of a second order system for fixed \\(ω_d\\) and different values \\(σ\\). The red dot shows the peak of the step response.\n\n\n\n\nExercise 3 For a generic second order system, where should the poles lie in the \\(s\\)-plane to meet the following specifications:\n\n\\(\\%{\\rm OS} \\le 10\\%\\)\n\\(T_s (2\\%) \\le 4\\) sec.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe \\(\\%{\\rm OS}\\) depends only on the damping coefficient \\(ζ\\). The values of \\(ζ\\) for which \\(\\%{\\rm OS} \\le 10\\%\\) is given by \\(ζ \\ge ζ_\\circ\\) where \\[\nζ_\\circ = \\frac{-\\ln(\\%{\\rm OS})}{\\sqrt{π^2 + \\ln^2(\\%{\\rm OS}/100)}}\n= 0.5912.\n\\] Thus, the angle \\(φ\\) of the poles from the \\(j ω\\) axis should be \\(φ \\ge φ_\\circ\\), where \\(φ_\\circ = \\sin^{-1}(ζ_\\circ) = 36.23^∘\\). This is shown in Figure 17.\nThe settling time \\(σ\\) only depends on the coefficient \\(σ\\). The values of \\(σ\\) for which \\(T_s \\le 4\\) is given by \\(σ \\ge σ_\\circ\\) where \\[\nσ_\\circ = \\frac{4}{T_s} = 1.\n\\] This is shown in Figure 17.\nThe region where both specs are active is the intersection of these two regions.\n\nviewof spec_choice = Inputs.radio(specs, {label: \"Spec\", value:specs[0], format: x =&gt; x.name})\n\ntheta = Math.asin(0.5912)\nspecs = [\n        { name:\"%OS\",\n          shaded:[{x1: 0, y1: 0, x2: 0, y2: 0},\n                  {x1: -4.1, y1:  4.1*Math.tan(Math.PI/2 - theta),\n                  x2: -4.1, y2: -4.1*Math.tan(Math.PI/2 - theta)} ],\n          left:[{x:0, y:0}, {x:0,y:0}]\n        },\n        {name:\"Ts\",\n          shaded:[{x1: -1, y1: 6, x2: -1, y2: -6},\n                  {x1: -4.1, y1: 6,\n                  x2: -4.1, y2: -6} ],\n          left:[{x:-1, y:6}, {x:-1,y:-6}]\n        },\n        {name:\"both\",\n          shaded:[{x1: -1, y1: 1*Math.tan(Math.PI/2 - theta),\n                   x2: -1, y2: -1*Math.tan(Math.PI/2 - theta)},\n                  {x1: -4.1, y1:  4.1*Math.tan(Math.PI/2 - theta),\n                  x2: -4.1, y2: -4.1*Math.tan(Math.PI/2 - theta)} ],\n          left:[{x: -1, y: 1*Math.tan(Math.PI/2 - theta)},\n                {x: -1, y: -1*Math.tan(Math.PI/2 - theta)} ],\n        }\n        ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  grid: true,\n  clip: true,\n  y: { domain: [-5,5] },\n  x: { domain: [-4,1] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(spec_choice.left, {x: \"x\", y: \"y\"}),\n    Plot.line(spec_choice.shaded, {x: \"x1\", y: \"y1\"}),\n    Plot.line(spec_choice.shaded, {x: \"x2\", y: \"y2\"}),\n    Plot.area(spec_choice.shaded, {x1: \"x1\", y1: \"y1\", x2: \"x2\", y2: \"y2\", fill:\"lightblue\"}),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 17: Location of poles that meet the specifications"
  },
  {
    "objectID": "step-response.html#identifying-an-underdamped-second-order-system-via-testing",
    "href": "step-response.html#identifying-an-underdamped-second-order-system-via-testing",
    "title": "Step Response",
    "section": "11 Identifying an underdamped second order system via testing",
    "text": "11 Identifying an underdamped second order system via testing\nA second order underdamped system has two characteristics:\n\nOscillatory overshoots (can be hard to see for \\(ζ\\) close to 1)\nZero initial slope.\n\nNote that all underdamped higher order systems share these characteristics, so it can be sometimes hard to distinguish a second order system from a higher order system.\nFor a second order system, we need to identify three parameters: \\(K\\), \\(ω_n\\) and \\(ζ\\).\n\nThe gain \\(K\\) is equal to the final value of the step response.\nTo find \\(ω_n\\) and \\(ζ\\), we need to find two out the three features: peak time, percentage overshoot, and setting time. Using these features, we can identify \\(ω_n\\) and \\(ζ\\).\n\nThen, the TF is given by \\[ G(s) = K \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}. \\]\n\nExample 6 Identify the transfer function from the following step response\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom the step response, we can infer that this is the step response of an underdapmper second order system, which in general is of the form: \\[\n  G(s) = K \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}.\n\\]\nWe now identify the parameters:\n\nThe final DC value of the response is \\(2\\). Thus, \\(K = 2\\).\nFrom the plot, we see that \\(T_p \\approx 0.75\\). Thus, \\[ ω_d = \\frac{π}{T_p} = 4.188. \\]\nFrom the plot, we see that \\(y_{\\text{max}} = 2.4\\). We have already identified that \\(y_{\\text{final}} = 2\\). Thus, \\[ \\% {\\rm OS} = \\frac{y_{\\text{max}} - y_{\\text{final}}}{y_{\\text{final}}} \\times 100\n= 20\\%\n\\] The formula for \\(\\%{\\rm OS}\\) can be written as \\[\n\\%{\\rm OS} = \\exp(-σ T_p) \\times 100\n\\]\nInverting the formula, we get \\[\nσ = \\frac{-\\ln(\\%{\\rm OS}/100)}{T_p} = 2.146\n\\]\nThus, \\[ ω_n^2 = σ^2 + ω_d^2 = 22.151 \\]\n\nHence, the transfer function is \\[\nG(s) = 2 \\frac{22.151}{s^2 + 4.21s + 22.151}\n= \\frac{44.302}{s^2 + 4.21s + 22.151}.\n\\]\nTo confirm, we plot the step response of the above system below. The plotted response is almost identical to the given response.\n\nusing ControlSystems, Plots\n\nG = tf([44.302],[1,4.21,22.151])\n\nT = 6 \n\nplt = plot(size=(600,300), gridalpha=0.75, minorgridalpha=0.25)\nplot!(plt, step(G, T))"
  },
  {
    "objectID": "step-response.html#impact-of-zeros",
    "href": "step-response.html#impact-of-zeros",
    "title": "Step Response",
    "section": "12 Impact of zeros",
    "text": "12 Impact of zeros\nIn the characterization of the step response of a second order underdamped system above, we assumed that the system had no zeros. The presence of a zero leads to more oscillations and the exact behavior depends on whether the zero is in the left-hand place (called minimum phase system) or right-hand plane (called non-minimum phase system), which we discuss below.\nThe impact of zeros means that we will not be able to execute our high-level idea of controller design because we cannot control the zeros of a closed loop system. So, in practice, we need to verify the control design via simulations to make sure that the specs are satisfied.\n\n12.1 Impact of zero in the left-hand plane (minimum-phase system)\nTo undersand this, we compare the response of two systems \\[\n  \\dfrac{8}{s^2 + 4s + 8}\n  \\quad\\hbox{vs}\\quad\n  8 \\cdot \\dfrac{1+\\frac{s}{z}}{s^2 + 4s + 8}\n\\]\n\n\n\n\n\n\nviewof zero = Object.assign(Inputs.range([1,6], {label: \"z\", step: 1, value: 1 }), {style: '--label-width:20px'})\n\n\n\n\n\n\nPZplot(\n       [ {σ: -zero, jω: 0} ], \n       [ {σ: -2, jω: 2}, {σ: -2, jω: -2} ],\n       [-6, 2],\n       [-3, 3],\n      )\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.8] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(second_order_min_zero.filter(d =&gt; d.z == zero), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n    Plot.line(second_order_min_zero.filter(d =&gt; d.z == 0), \n              {x:\"time\", y:\"output\", stroke: \"orange\" })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 18: Step response of a seocnd order system for different values zero. The orange curve shows the step response of a system without zero and the blue curve shows the step response of the system with zeros.\n\n\n\nThe above example shows that the presence of a zero in the LHP leads to more overshoot and as the zero moves away from the dominant pole, the response approaches that of a system with no zeros. To understand why this is the case, we can view the system as \\[\n  G(s) = K \\frac{ω_n^2 (1 + \\frac{s}{z})}{s^2 + 2 ζω_n^2 + ω_n^2}\n  =\n  K \\frac{ω_n^2}{s^2 + 2 ζω_n^2 + ω_n^2}\n  +\n  K \\frac{ω_n^2}{s^2 + 2 ζω_n^2 + ω_n^2} \\cdot \\frac{s}{z}\n  \\eqqcolon\n  G_1(s) + G_1(s) \\frac{s}{z}\n\\] where \\(G_1(s)\\) is the standard second-order system without a zero. Thus, the step response of the system is \\[\n  Y(s) = U(s) G(s)\n  =\n  U(s) G_1(s) + U(s) G_1(s) \\frac{s}{z}\n  =\n  Y_1(s) + Y_1(s) \\frac{s}{z}\n\\] where \\(Y_1(s)\\) is the step response of the TF with no zeros. Hence, \\[\n  y(t) = y_1(t) + \\frac{1}{z} \\frac{dy_1(t)}{dt}.\n\\] Since \\(y_1(t)\\) is increasing for small \\(t\\), the derivative \\(dy_1(t)/dt\\) is positive and therefore the second term leads to a faster and larger overshoot.\nThe above formula also shows that when \\(z\\) is large, then \\(y(t) \\approx y_1(t)\\).\n\n\n12.2 Impact of zero in the right-hand plane (non-minimum-phase system)\nTo understand this, we compare the response of two systems \\[\n  \\dfrac{8}{s^2 + 4s + 8}\n  \\quad\\hbox{vs}\\quad\n  8 \\cdot \\dfrac{1-\\frac{s}{z}}{s^2 + 4s + 8}\n\\]\n\n\n\n\n\n\nviewof zero_non_min = Object.assign(Inputs.range([1,6], {label: \"z\", step: 1, value: 1 }), {style: '--label-width:20px'})\n\n\n\n\n\n\nPZplot(\n       [ {σ: zero_non_min, jω: 0} ], \n       [ {σ: -2, jω: 2}, {σ: -2, jω: -2} ],\n       [-3, 6.5],\n       [-3, 3],\n      )\nPlot.plot({\n  grid: true,\n  y: { domain: [-1.6, 1.2] },\n  x: { domain: [0,5] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(second_order_non_min_zero.filter(d =&gt; d.z == zero_non_min), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n    Plot.line(second_order_non_min_zero.filter(d =&gt; d.z == 0), \n              {x:\"time\", y:\"output\", stroke: \"orange\" })\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 19: Step response of a seocnd order system for different values zero. The orange curve shows the step response of a system without zero and the blue curve shows the step response of the system with zeros.\n\n\n\nThe above example shows that a zero in the RHP leads to undershoot and as the zero moves to infinity, the response approaches that of a system with no zeros.\nTo understand why this is the case, we can follow the same analysis as for the minimal phase system to conclude that \\[\n  y(t) = y_1(t) - \\frac{1}{z} \\frac{dy_1(t)}{dt}.\n\\] Since \\(y_1(t)\\) is increasing for small \\(t\\), the derivative \\(dy_1(t)/dt\\) is positive and therefore the second term leads to an undershoot. The above formula also shows that when \\(z\\) is large, then \\(y(t) \\approx y_1(t)\\)."
  },
  {
    "objectID": "step-response.html#step-response-of-critically-damped-second-order-system",
    "href": "step-response.html#step-response-of-critically-damped-second-order-system",
    "title": "Step Response",
    "section": "13 Step response of critically damped second order system",
    "text": "13 Step response of critically damped second order system\nFor \\(ζ = 1\\), the TF is given by \\[ G(s) = \\frac{ω_n^2}{(s+ω_n)^2}. \\] From the LT tables, we get that \\[ y(t) = [ 1 - e^{-ω_n t}(1 + ω_n t) ] \\IND(t). \\]\nCritically damped systems have the fastest settling time among all second order system. The response can be visually differentiated from that of a second order system by noticing that the initial slope (at \\(t=0\\)) is zero.\n\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.3] },\n  x: { domain: [0,8] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(second_order_zeta.filter(d =&gt; Math.abs(d.ζ - 1) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 20: Step response of critically system for \\(ζ = 1.0\\)."
  },
  {
    "objectID": "step-response.html#step-response-of-overdamped-second-order-system",
    "href": "step-response.html#step-response-of-overdamped-second-order-system",
    "title": "Step Response",
    "section": "14 Step response of overdamped second order system",
    "text": "14 Step response of overdamped second order system\nFor \\(ζ &gt; 1\\), the system has two poles at \\[\np_1 = ω_n(ζ + \\sqrt{ζ^2 - 1})\n\\quad\\text{and}\\quad\np_2 = ω_n(ζ - \\sqrt{ζ^2 - 1}).\n\\]\nThe TF can be written as \\[ G(s) = \\frac{ω_n^2}{(s+p_1)(s+p_2)}.\\]\nUsing partial fraction expansion, we can show that \\[\nY(s) = \\frac{1}{s}\\cdot G(s)\n= \\frac{1}{s} - \\frac{ω_n^2}{2 (p_1 - p_2)} \\biggl[ \\frac{1/p_1}{s+p_1} + \\frac{1/p_2}{s+p_2} \\biggr].\n\\] Therefore, \\[\ny(t) = \\biggl[ 1 - \\frac{ω_n^2}{2\\sqrt{ζ^2 - 1}} \\biggl( \\frac{1}{p_1} e^{-p_1t} + \\frac{1}{p_2} e^{-p_2t} \\biggr) \\biggr] \\IND(t).\n\\]\n\nPlot.plot({\n  grid: true,\n  y: { domain: [0, 1.3] },\n  x: { domain: [0,8] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(second_order_zeta.filter(d =&gt; Math.abs(d.ζ - 1.5) &lt;= 0.01), \n              {x:\"time\", y:\"output\", stroke: \"darkblue\" }),\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 21: Step response of overdamped system for \\(ζ = 1.5\\)."
  },
  {
    "objectID": "routh-hurwitz.html",
    "href": "routh-hurwitz.html",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "",
    "text": "Consider a unity feedback system as shown in Figure 1. In such a setting, we are often interested in finding the values of \\(K\\) for which the system is stable.\nIn the above example, we could identify the roots of the closed loop transfer function and thereby determine the values of gain \\(K\\) for which the system is stable. However, we can only factorize lower order polynomials; for polynomials with degree greater than \\(t\\), we need to resort to numerical methods. Doing so, makes it difficult to find the range of values of \\(K\\) for which a polynomial is stable.\nHowever, for determine the stability of a system, we don’t need to find the roots of the denominator polynomial; we simply need to verify that all the roots are in the OLHP. The following result shows that we can determine if roots lie in the ORHP without factorizing a polynomial.\nBut this is a necessary condition but not a sufficient condition. So, we know that the polynomial \\[ D(s) = s^5 + 4 s^4 + 10s^3 - s^2 + 2 s + 1\\] is unstable because one of the coefficients are negative. Moreover, we know that the polynomial \\[ D(s) = s^4 + 4s^3 + s + 1\\] is unstable because the coefficient of the \\(s^2\\) term is \\(0\\). But this necessary condition doesn’t tell us if \\[ D(s) = s^5 + 4 s^4 + 10s^3 + s^2 + 2 s + 1\\] is stable or not.\nThe Routh-Hurwitz criterion is a simple algebraic procedure which determines whether a polynomial is stable. The first step is generating what is called a Routh Array."
  },
  {
    "objectID": "routh-hurwitz.html#generating-the-routh-array",
    "href": "routh-hurwitz.html#generating-the-routh-array",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "1 Generating the Routh Array",
    "text": "1 Generating the Routh Array\nConsider a polynomial \\[ D(s) = a_n s^n + \\cdots + a_0. \\]\nThe Routh array is a (non-rectangular) array with \\(n+1\\) rows, indexed by \\(s^n\\), \\(s^{n-1}\\), \\(\\dots\\), \\(s^0\\).\n\nStep 1. Fill the first two rows of the Routh array with the coefficients of \\(D(s)\\) going in the zigzag pattern as shown below. We stop when we have used all the coefficients. Any unfilled entries in the Routh array are assumed to be zero.\n\n\n\n\n\\(s^n\\)\n\n\n\\(a_{n}\\)\n\n\n\\(a_{n-2}\\)\n\n\n\\(\\cdots\\)\n\n\n\n\n\\(s^{n-1}\\)\n\n\n\\(a_{n-1}\\)\n\n\n\\(a_{n-3}\\)\n\n\n\\(\\cdots\\)\n\n\n\n\nStep 2. This is a recursive step, where we take two filled rows, say row \\(s^{m+2}\\) and \\(s^m\\) and use that to fill row \\(s^m\\), for all \\(m \\in \\{n-2, \\dots, 0\\}\\). Each entry is the negative determinant of a \\(2 \\times 2\\) matrix constructed from the entries in the previous two rows (i.e., row \\(s^{m+2}\\) and \\(s^{m+1}\\) when we are filling in row \\(s^m\\)) divided by the first entry in row \\(s^{m+1}\\) (provided that entry is not zero!). The first column of \\(2 \\times 2\\) matrix is the first column of the previous two rows; the second column of the \\(2 \\times 2\\) matrix is the columns above and to the right.\n\nNote that in each row, we eventually end up with zeros at which time we stop filling the row. We repeat this procedure until we have filled all rows until row \\(s^0\\).\n\nWe always follow the above method to fill in the Routh array, irrespective of the size of the polynomial. We illustrate this via some examples.\n\nExample 2 Find the Routh Array of \\[ D(s) = s^4 + 2 s^3 + 3 s^2 + 4s + 5. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe follow the procedure described above. The first column of the table is highlight.\n\n\n$s^{4}$$1$$3$$5$$s^{3}$$2$$4$$s^{2}$$\\displaystyle -\\frac{\\DET{ 1 & 3 \\\\ 2 & 4}}{2} = 1$$\\displaystyle -\\frac{\\DET{ 1 & 5 \\\\ 2 & 0}}{2} = 5$$s^{1}$$\\displaystyle -\\frac{\\DET{ 2 & 4 \\\\ 1 & 5}}{1} = -6$$s^{0}$$\\displaystyle -\\frac{\\DET{ 1 & 5 \\\\ -6 & 0}}{-6} = 5$"
  },
  {
    "objectID": "routh-hurwitz.html#interpreting-the-routh-array",
    "href": "routh-hurwitz.html#interpreting-the-routh-array",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "2 Interpreting the Routh Array",
    "text": "2 Interpreting the Routh Array\nWe start with the basic case when there is no zero in the first column (as is the case in Example 2). In this case, we look at the first column and count the number of sign changes (which are highlighted by arrows above). For example, for Example 2, we have two sign changes in the first column, as shown below.\n\nWhen there are no zeros in the first column, the polynomial has no roots on the \\(j ω\\)-axis. Moreover,\n\nNo. of roots in the ORHP = no. of sign changes\nNo. of roots in the OLHP = degree of polynomial \\(-\\) no. of sign changes.\n\nSo, for Example 2, we have\n\nNo. of roots in the ORHP = 2 (no. of sign changes)\nNo. of roots in the OLPH = 4 (degree of polynomial) \\(-\\) 2 (no. of sign changes) = 2.\n\nWe can verify this by factorizing \\(D(s)\\), which gives\n\nD = Polynomial([5,4,3,2,1], :s)\nprintln(\"D(s) = \", D)\nroots(D)\n\nD(s) = 5 + 4*s + 3*s^2 + 2*s^3 + s^4\n\n\n4-element Vector{ComplexF64}:\n -1.2878154795576484 - 0.8578967583284913im\n -1.2878154795576484 + 0.8578967583284913im\n  0.2878154795576478 - 1.416093080171908im\n  0.2878154795576478 + 1.416093080171908im\n\n\n\nExample 3 Find the location of the poles of a TF with with denominator given by \\[ D(s) = s^4 + 5 s^3 + s^2 + 10s + 1. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute the Routh Array\n\n\n$s^{4}$$1$$1$$1$$s^{3}$$5$$10$$s^{2}$$\\displaystyle -\\frac{\\DET{ 1 & 1 \\\\ 5 & 10}}{5} = -1$$\\displaystyle -\\frac{\\DET{ 1 & 1 \\\\ 5 & 0}}{5} = 1$$s^{1}$$\\displaystyle -\\frac{\\DET{ 5 & 10 \\\\ -1 & 1}}{-1} = 15$$s^{0}$$\\displaystyle -\\frac{\\DET{ -1 & 1 \\\\ 15 & 0}}{15} = 1$\n\n\nWe now look at the signs of the terms in the first column:\n\n\nTermSign$s^{4}$$+$$s^{3}$$+$$s^{2}$$-$$s^{1}$$+$$s^{0}$$+$\n\n\nNote that there are two sign changes in the first column. Thus, we have\n\nNo. of roots in ORHP = 2 (no. of sign changes)\nNo. of roots in OLHP = 4 (degree of poly) \\(-\\) 2 (no. of sign changes) = 2.\n\nWe can verify this by factorizing \\(D(s)\\), which gives\n\n\nD(s) = 1 + 10*s + s^2 + 5*s^3 + s^4\n\n\n4-element Vector{ComplexF64}:\n   -5.173143012444715 + 0.0im\n -0.10051275725870919 + 0.0im\n  0.13682788485171066 - 1.3800281123446885im\n  0.13682788485171066 + 1.3800281123446885im\n\n\n\n\n\n\n2.1 An optimization\nSince we only care about the signs of the coefficient, we can multiply or divide all elements in a row by a positive number without changing the result. This can sometimes lead to simpler calculations.\n\nExample 4 Find the location of the poles of a TF with with denominator given by \\[ D(s) = s^6 + 4s^5 + 3s^4 + 2s^3 + s^2 + 4s + 4. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute the Routh Array\n\n\n$s^{6}$$1$$3$$1$$4$$s^{5}$$\\cancel{4} 2$$\\cancel{2} 1$$\\cancel{4} 2$$s^{4}$$\\displaystyle -\\frac{\\DET{ 1 & 3 \\\\ 2 & 1}}{2} = \\frac{5}{2}$$\\displaystyle -\\frac{\\DET{ 1 & 1 \\\\ 2 & 2}}{2} = 0$$\\displaystyle -\\frac{\\DET{ 1 & 4 \\\\ 2 & 0}}{2} = 4$$s^{3}$$\\displaystyle -\\frac{\\DET{ 2 & 1 \\\\ \\frac{5}{2} & 0}}{\\frac{5}{2}} = 1$$\\displaystyle -\\frac{\\DET{ 2 & 2 \\\\ \\frac{5}{2} & 4}}{\\frac{5}{2}} = \\frac{-6}{5}$$s^{2}$$\\displaystyle -\\frac{\\DET{ \\frac{5}{2} & 0 \\\\ 1 & \\frac{-6}{5}}}{1} = 3$$\\displaystyle -\\frac{\\DET{ \\frac{5}{2} & 4 \\\\ 1 & 0}}{1} = 4$$s^{1}$$\\displaystyle -\\frac{\\DET{ 1 & \\frac{-6}{5} \\\\ 3 & 4}}{3} = \\frac{-38}{15}$$s^{0}$$\\displaystyle -\\frac{\\DET{ 3 & 4 \\\\ \\frac{-38}{15} & 0}}{\\frac{-38}{15}} = 4$\n\n\nWe now look at the signs of the terms in the first column:\n\n\nTermSign$s^{6}$$+$$s^{5}$$+$$s^{4}$$+$$s^{3}$$+$$s^{2}$$+$$s^{1}$$-$$s^{0}$$+$\n\n\nNote that there are two sign changes in the first column. Thus, we have\n\nNo. of roots in ORHP = 2 (no. of sign changes)\nNo. of roots in OLHP = 6 (degree of poly) \\(-\\) 2 (no. of sign changes) = 4.\n\nWe can verify this by factorizing \\(D(s)\\), which gives\n\n\nD(s) = 4 + 4*s + s^2 + 2*s^3 + 3*s^4 + 4*s^5 + s^6\n\n\n6-element Vector{ComplexF64}:\n -3.2643574436966434 + 0.0im\n -0.8858022358397655 + 0.0im\n -0.6045963281166988 - 0.993535028893593im\n -0.6045963281166988 + 0.993535028893593im\n  0.6796761678849015 - 0.7488138087285154im\n  0.6796761678849015 + 0.7488138087285154im"
  },
  {
    "objectID": "routh-hurwitz.html#special-cases-zero-in-the-first-column",
    "href": "routh-hurwitz.html#special-cases-zero-in-the-first-column",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "3 Special cases: Zero in the first column",
    "text": "3 Special cases: Zero in the first column\nWe cannot follow the usual method to construct the Routh array if there is zero in the first column. For example, consider \\[ D(s) = s^5 + 2s^4 + 3s^3 + 6s^2 + 5s + 3. \\]\n\n\n\n\\(s^5\\)\n\n\n\\(1\\)\n\n\n\\(3\\)\n\n\n\\(5\\)\n\n\n\n\n\\(s^4\\)\n\n\n\\(2\\)\n\n\n\\(6\\)\n\n\n\\(3\\)\n\n\n\n\n\\(s^3\\)\n\n\n\\(0\\)\n\n\n\\(7/2\\)\n\n\n\\(3\\)\n\n\n\nIf there is zero in the first column but the entire row is not zero, we proceed as follows\n\nReplace the zero in the first column by an \\(ε\\) and continue to construct the Routh array as a function of \\(ε\\).\nCount the number of sign changes as \\(ε \\to 0^{+}\\) (\\(ε\\) goes to zero from above). Let this number be \\(k_{+}\\).\nCount the number of sign changes as \\(ε \\to 0^{-}\\) (\\(ε\\) goes to zero from below). Let this number be \\(k_{-}\\).\n\nThen, we have\n\nNo. of roots in the ORHP = \\(\\min\\{k_{+}, k_{-}\\}\\)\nNo. of roots on the \\(j ω\\)-axis = \\(|k_{+} - k_{-}|\\).\nNo. of roots in the OLHP = \\(\\text{degree of polynomial} - \\max\\{k_{+}, k_{-}\\}\\).\n\n\nExample 5 Find the location of the poles of a TF with with denominator given by \\[ D(s) = s^5 + 2s^4 + 3s^3 + 6s^2 + 5s + 3. \\]\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first compute the Routh Array\n\n\n$s^{5}$$1$$3$$5$$s^{4}$$2$$6$$3$$s^{3}$$\\displaystyle -\\frac{\\DET{ 1 & 3 \\\\ 2 & 6}}{2} = \\cancel{0} \\epsilon$$\\displaystyle -\\frac{\\DET{ 1 & 5 \\\\ 2 & 3}}{2} = \\frac{7}{2}$$s^{2}$$\\displaystyle -\\frac{\\DET{ 2 & 6 \\\\ \\epsilon & \\frac{7}{2}}}{\\epsilon} = \\frac{-7 + 6 \\epsilon}{\\epsilon}$$\\displaystyle -\\frac{\\DET{ 2 & 3 \\\\ \\epsilon & 0}}{\\epsilon} = 3$$s^{1}$$\\displaystyle -\\frac{\\DET{ \\epsilon & \\frac{7}{2} \\\\ \\frac{-7 + 6 \\epsilon}{\\epsilon} & 3}}{\\frac{-7 + 6 \\epsilon}{\\epsilon}} = \\frac{\\frac{-49}{2} + 21 \\epsilon - 3 \\epsilon^{2}}{-7 + 6 \\epsilon}$$s^{0}$$\\displaystyle -\\frac{\\DET{ \\frac{-7 + 6 \\epsilon}{\\epsilon} & 3 \\\\ \\frac{\\frac{-49}{2} + 21 \\epsilon - 3 \\epsilon^{2}}{-7 + 6 \\epsilon} & 0}}{\\frac{\\frac{-49}{2} + 21 \\epsilon - 3 \\epsilon^{2}}{-7 + 6 \\epsilon}} = 3$\n\n\nWe now look at the signs of the terms in the first column:\n\nsign_changes(RA)\n\nTerm$ ε \\to 0^{+}$$ ε \\to 0^{-}$$s^{5}$$+$$+$$s^{4}$$+$$+$$s^{3}$$+$$-$$s^{2}$$-$$+$$s^{1}$$+$$+$$s^{0}$$+$$+$\n\n\nIn both cases, we have two sign changes in the first column. Therefore, \\(k_{+} = k_{-} = 2\\). Hence, we have\n\nNo. of roots in the ORHP = 2\nNo. of roots on the \\(j ω\\)-axis = 0\nNo. of roots in the OLHP = \\(5 - 2 = 3\\)\n\nWe can verify this by factorizing \\(D(s)\\), which gives\n\n\nD(s) = 3 + 5*s + 6*s^2 + 3*s^3 + 2*s^4 + s^5\n\n\n5-element Vector{ComplexF64}:\n -1.6680888389741944 + 0.0im\n -0.5088331416337463 - 0.7019951317695377im\n -0.5088331416337463 + 0.7019951317695377im\n 0.34287756112084433 - 1.5082901611666297im\n 0.34287756112084433 + 1.5082901611666297im\n\n\n\n\n\n\n3.1 Reciprocal polynomial\nIf we get a zero in the first column, we can follow another method but this method is not guaranteed to work. To understand this, we need the notion of a reciprocal polynomial: for a polynomial \\(D(s)\\) of degree \\(n\\), the reciprocal polynomial is \\(s^n D(\\frac 1s)\\). For instance, for Example 5, we have\n\\[\\begin{align*}\ns^5 D\\left(\\frac 1s\\right) &=\ns^5 \\left[ \\frac{1}{s^5} + \\frac{2}{s^4} + \\frac{3}{s^3} + \\frac{6}{s^2} + \\frac{5}{s} + 3 \\right] \\\\\n3 s^5 + 5 s^4 + 6 s^3 + 3 s^2 + 2s + 1\n\\end{align*}\\]\n\n\n\n\n\n\nWhy look at the reciprocal polynomial.\n\n\n\nObserve that if \\(D(s)\\) factorizes as \\[\nD(s) = (s+p_1)(s+p_2) \\cdots (s+p_n)\n\\] then the factorization of \\(s^nD(\\frac 1s)\\) is given by \\[\\begin{align*}\ns^n D(s) &= s^n \\left( \\frac 1s + p_1 \\right) \\left( \\frac 1s + p_2 \\right) \\cdots \\left( \\frac 1s + p_n \\right) \\\\\n&= (p_1 p_2 \\cdots p_n) \\left(s + \\frac 1{p_1} \\right) \\left( s + \\frac{1}{p_2}\\right) \\cdots \\left(s + \\frac{1}{p_n} \\right).\n\\end{align*}\\] Thus, if \\(p_i\\) is a root of \\(D(s)\\), then \\(\\dfrac{1}{p_i}\\) is a root of \\(s^n D(\\frac 1s)\\).\nThe key point is that \\(p_i\\) and \\(1/p_i\\) have the same sign. In particular if \\[\np_i = a_i + j b_i \\implies \\frac {1}{p_i} = \\frac{a}{|p_i|} - j\\frac{ b_i}{|p_i|}\n\\] So, the polynomial and its reciprocal have same number of roots in the OLHP, ORHP, and \\(j ω\\)-axis.\n\n\n\nExample 6 Solve Example 5 using the reciprocal polynomial.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have already computed the reciprocal polynomial above. We now construct the Routh array\n\n\n$s^{5}$$3$$6$$2$$s^{4}$$5$$3$$1$$s^{3}$$\\displaystyle -\\frac{\\DET{ 3 & 6 \\\\ 5 & 3}}{5} = \\frac{21}{5}$$\\displaystyle -\\frac{\\DET{ 3 & 2 \\\\ 5 & 1}}{5} = \\frac{7}{5}$$s^{2}$$\\displaystyle -\\frac{\\DET{ 5 & 3 \\\\ \\frac{21}{5} & \\frac{7}{5}}}{\\frac{21}{5}} = \\frac{4}{3}$$\\displaystyle -\\frac{\\DET{ 5 & 1 \\\\ \\frac{21}{5} & 0}}{\\frac{21}{5}} = 1$$s^{1}$$\\displaystyle -\\frac{\\DET{ \\frac{21}{5} & \\frac{7}{5} \\\\ \\frac{4}{3} & 1}}{\\frac{4}{3}} = \\frac{-7}{4}$$s^{0}$$\\displaystyle -\\frac{\\DET{ \\frac{4}{3} & 1 \\\\ \\frac{-7}{4} & 0}}{\\frac{-7}{4}} = 1$\n\n\nWe now look at the signs of the terms in the first column:\n\n\nTermSign$s^{5}$$+$$s^{4}$$+$$s^{3}$$+$$s^{2}$$+$$s^{1}$$-$$s^{0}$$+$\n\n\nThus, we see that there are two sign changes in the first column. Therefore,\n\nNo. of roots in the ORHP = 2 (no. of sign changes)\nNo. of roots in the OLHP = 5 (degree of polynomial) \\(-\\) 2 (no. of sign changes) = 3. which matches with what we had observed in the solution of Example 5."
  },
  {
    "objectID": "routh-hurwitz.html#special-cases-entire-row-of-zeros",
    "href": "routh-hurwitz.html#special-cases-entire-row-of-zeros",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "4 Special cases: Entire row of zeros",
    "text": "4 Special cases: Entire row of zeros\nSometimes, we get an entire row of zero. For instance, consider \\[\nD(s) = s^5 + 7s^4 + 6 s^3 + 42 s^2 + 8 s + 56.\n\\]\nIn this case, the Routh array ends with a row of zero.\n\n\n\n\\(s^5\\)\n\n\n\\(1\\)\n\n\n\\(6\\)\n\n\n\\(8\\)\n\n\n\n\n\\(s^4\\)\n\n\n\\(\\cancel{7} 1\\)\n\n\n\\(\\cancel{42} 6\\)\n\n\n\\(\\cancel{56} 8\\)\n\n\n\n\n\\(s^3\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\\(0\\)\n\n\n\n\n\n\n\n\n\nWhen do we get a row of zeros\n\n\n\nSuppose the row \\(s^{m-1}\\) is zero in the Routh array. This means that the polynomial \\(P^m\\) (which is the polynomial above the row of zeros) divides \\(D(s)\\).\nFor instance, in the above example, row \\(s^3\\) is zero. The above claim means that \\[\nP_4(s) = s^4 + 6 s^2 + 8\n\\] divides \\(D(s)\\). This is indeed the case and we can verify that \\[\nD(s) = \\underbrace{(s+7)}_{\\text{remainder poly.}}\n       \\underbrace{(s^4 + 6s^2 + 8)}_{\\text{divisor poly.}}.\n\\]\n\n\nAs argumed The rows that we have encountered until the row of all zeros corresponds to the remainder polynomial. We interpret is the same way we did the normal case.\nObserve that the divisor polynomial is an even polynomial. A key feature of an even polynomial is that it has an equal number of roots in the OLHP and the ORHP. The number of sign changes is equal to the number of roots in the ORHP. So, the number of roots in OLHP are the same. The remaining roots must be on the \\(j ω\\)-axis.\n\n\n\n\n\n\nRoots of even polynomial\n\n\n\nAn even polynomial has the same number of roots in the ORHP and OLHP.\nConsider an even polynomial\n\\[P^{2m}(s) = a_{2m} s^{2m} + a_{2m -2 } s^{2m -2} + \\cdots + a^0.\\]\nWe can write this as a polynomial in \\(z = s^2\\) as\n\\[ a_{2m} z^m + a_{2m -2} z^{m-1} + \\cdots + a_0. \\]\nLet \\(p_1, p_2, \\dots, p_m\\) be the roots of this polynomial. Then, the roots of \\(P^{2m}\\) are \\(\\pm \\sqrt{p_1}, \\pm \\sqrt{p_2}, \\dots, \\pm \\sqrt{p_m}\\).\nNow observe the following:\n\nIf \\(p_i\\) is real and positive, then \\(+ \\sqrt{p_i}\\) is positive and \\(- \\sqrt{p_i}\\) is negative. Thus, one root is in the ORHP and one is in the OLHP.\nIf \\(p_i\\) is real and negative, then \\(\\pm \\sqrt{p_i} = \\pm j \\sqrt{|p_i|}\\), which lie on the \\(j ω\\)-axis. Thus, there are no roots in the ORHP or OLHP.\nIf \\(p_i\\) is complex and say equal to \\(a_i + j b_i\\), then there must be another root \\(a_i - j b_i\\). Now let \\(\\pm (c_i + j d_i)\\) denote the square root of \\(a_i + j b_i\\). Then, the square roots of \\(a_i - j b_i\\) are \\(\\pm (c_i - j d_i)\\). Thus, out of the four roots, two are in the the ORHP and two are in the OLHP.\n\nThus, in all cases, no. of roots in the ORHP = no. of roots in the OLHP.\n\n\n\nThe rows that we have encountered until the row of all zeros corresponds to the remainder polynomial. We interpret is the same way we did the normal case.\nWe first compute the Routh Array\n\n\n$s^{5}$$1$$6$$8$$s^{4}$$\\cancel{7} 1$$\\cancel{42} 6$$\\cancel{56} 8$$s^{3}$$\\displaystyle -\\frac{\\DET{ 1 & 6 \\\\ 1 & 6}}{1} = \\cancel{0} \\cancel{4} 1$$\\displaystyle -\\frac{\\DET{ 1 & 8 \\\\ 1 & 8}}{1} = \\cancel{0} \\cancel{12} 3$$s^{2}$$\\displaystyle -\\frac{\\DET{ 1 & 6 \\\\ 1 & 3}}{1} = 3$$\\displaystyle -\\frac{\\DET{ 1 & 8 \\\\ 1 & 0}}{1} = 8$$s^{1}$$\\displaystyle -\\frac{\\DET{ 1 & 3 \\\\ 3 & 8}}{3} = \\frac{1}{3}$$s^{0}$$\\displaystyle -\\frac{\\DET{ 3 & 8 \\\\ \\frac{1}{3} & 0}}{\\frac{1}{3}} = 8$\n\n\nWe now look at the signs of the terms in the first column:\n\n\nTermSign$s^{5}$$+$$s^{4}$$+$$s^{3}$$+$$s^{2}$$+$$s^{1}$$+$$s^{0}$$+$\n\n\nSince we have a row of zeros, we split our analysis into two cases. The remainder polynomial, which is rows \\(s^5\\) to \\(s^4\\) and has no sign change, and the divisor polynomial, which is rows \\(s^4\\) to \\(s^0\\) and has no sign changes either. So, we have\n\n\n\nPolynomial\ndegree\nno. of sign changes\nORHP\nOLHP\n\\(j ω\\)-axis\n\n\n\n\nRemainder\n1\n0\n0\n1\n0\n\n\nDivisor\n4\n0\n0\n0\n4\n\n\n\\(D(s)\\)\n5\n\n0\n1\n4\n\n\n\nWe can verify this by factorizing \\(D(s)\\), which gives (there are four roots on the \\(j ω\\) axis but we get small error in the location of the roots due to numerical accuracy)\n\n\nD(s) = 56 + 8*s + 42*s^2 + 6*s^3 + 7*s^4 + s^5\n\n\n5-element Vector{ComplexF64}:\n     -7.000000000000012 + 0.0im\n  6.938893903907228e-17 - 1.414213562373094im\n  6.938893903907228e-17 + 1.414213562373094im\n 3.0531133177191805e-16 - 1.9999999999999993im\n 3.0531133177191805e-16 + 1.9999999999999993im\n\n\n\nExample 7 Consider \\[\nD(s) = (s^2 + 2)(s+2) = s^3 + s^2 + 2s + 2.\n\\] Use Routh Hurwitz to find the location of the roots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe write the Routh Array\n\n\n$s^{3}$$1$$2$$s^{2}$$1$$2$$s^{1}$$\\displaystyle -\\frac{\\DET{ 1 & 2 \\\\ 1 & 2}}{1} = \\cancel{0} 2$$s^{0}$$\\displaystyle -\\frac{\\DET{ 1 & 2 \\\\ 2 & 0}}{2} = 2$\n\n\nWe now look at the signs of the terms in the first column:\n\n\nTermSign$s^{3}$$+$$s^{2}$$+$$s^{1}$$+$$s^{0}$$+$\n\n\nSince we have a row of zeros, we split our analysis into two cases. The remainder polynomial, which is rows \\(s^3\\) to \\(s^2\\) and has no sign change, and the divisor polynomial, which is rows \\(s^2\\) to \\(s^0\\) and has no sign changes either. So, we have\n\n\n\nPolynomial\ndegree\nno. of sign changes\nORHP\nOLHP\n\\(j ω\\)-axis\n\n\n\n\nRemainder\n1\n0\n0\n1\n0\n\n\nDivisor\n2\n0\n0\n0\n2\n\n\n\\(D(s)\\)\n3\n\n0\n1\n2"
  },
  {
    "objectID": "routh-hurwitz.html#stability-in-state-space",
    "href": "routh-hurwitz.html#stability-in-state-space",
    "title": "Routh-Hurwitz Stability Criterion",
    "section": "5 Stability in state space",
    "text": "5 Stability in state space"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Outline",
    "section": "",
    "text": "Instructor\n\n\nAditya Mahajan\nOffice Hours: TBD\n\n\nTeaching Assistants\n\n\nTBD\n\n\nLectures\n\n\n10:05am–11:25am Tuesday, Thursday (ENGMC 11)\n\n\nTutorials\n\n\n8:35am–10:25am Friday, (ENGMC 11)\n\n\n\nEveryone must be registered for the tutorial. Tutorials start from the second week of classes.\n\nLabs\n\n\n1:35pm-3:25pm Monday (TR 4090)\n\n\n\n\n3:35pm–5:25pm Monday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Tuesday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Wednesday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Friday (TR 4090)\n\n\n\nEveryone must be registered for one lab. Labs start from the second week of classes.\nThe labs have to be done in groups of two. The groups will be formed in the first week of classs.\n\nPrerequisites\n\n\nECSE 206 (Fundamentals of Signals and Systems)\nECSE 210 (Electrical Circuits 2)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered.\n\n\n\n\n\n\n\n\nAttribute\nDescription\nStatus\n\n\n\n\nKB\nKnowledge Base for Engineering\nDeveloped\n\n\nPA\nProblem Analysis\nDeveloped\n\n\nIN\nInvestigation\nDeveloped\n\n\nDE\nDesign\nIntroduced\n\n\n\nUpon the successful completion of this course, the student students will have demonstrated the ability to:\n\nApply time- and frequency-domain tools to analyze linear time-invariant systems (KB, PA)\nDesign controllers based on state-space methods and lead-lag compensators to meet time-response specifications (KB, PA, DE)\nUse Matlab to analyze and synthesize controllers for linear time-invariant systems (DE, IN)\n\n\n\n\n\n\n\n\n\n\n\nWeek\nMaterial Covered\n\n\n\n\n1\nReview of LTI Systems and Laplace Transforms  Reading: Nise Ch. 2\n\n\n2\nState-space modeling, canoncial forms, transfer function of state space models  Reading: William and Lawrence Ch. 1 and Ch. 2\n\n\n3\nPole Zero Plot Step response of first and second order systems, dominant pole approximation  Reading: Nise Ch. 4\n\n\n4\nMatrix exponential and solution of matrix differential equations, time response of state space models\n\n\n5\nPole placement, controllability, and state feedback  Reading: William and Lawrence Ch. 3 and Ch. 7\n\n\n6\nLuenberger observer, observability, output feedback, and separation principle  Reading: William and Lawrence Ch. 4 and Ch. 8\n\n\n7\nReview and Mid-Term\n\n\n8\nRouth-Hurwitz stability criterion. Special cases: zero in first column, row of zeros.  Reading: Nise Ch 6\n\n\n9\nBlock diagrams, rules for simplifying block diagrams, block diagram implementations of canonical forms of state space models\n\n\n10\nSystem type, steady state error, and disturbance rejection  Reading: Nise Ch. 7\n\n\n11\nBode and Nyquist Plots  Reading: Nise Ch. 10\n\n\n12\nNyquist stability criterion, gain and phase margins  Reading: Nise Ch. 10\n\n\n13\nLead-Lag compensators\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLab\n\n\n\n\n1\nNo Labs\n\n\n2\nLab 1 : Introduction to Matlab\n\n\n3\nLab 2 : Transfer functions and step response\n\n\n4\nLab 3 : Eigenvalues and eigenvectors\n\n\n5\nLab 4 : System identification via Step Response\n\n\n6\nLab 5 : State-Feedback Control (in simulation)\n\n\n7\nLab 6 : State-Feedback Control (in hardware)\n\n\n8\nLab 7 : Luenberger Observer for State Estimation\n\n\n9\nLab 8 : Output-Feedback Control\n\n\n10\nLab 9 : Block Diagram Reduction using Matlab\n\n\n11\nLab 10 : System identification via Bode Plots\n\n\n12\nLab 11 : Nyquist stability criterion\n\n\n13\nLab 12 : Lead-Lag compensator design\n\n\n\n\n\n\n\nTextbook\n\n\nN. Nise, Control Systems Engineering, 7th Ed., Wiley\nBoth paper and electronic versions of the textbook are available from the McGill bookstore. The electronic version of the book is also available from the publisher’s website and Google books.\n\n\n\n\nR.L. Williams II and D.A. Lawrence, Linear State‐Space Control Systems, Wiley.\nThe electronic version of the book is available the library.\n\n\nReference Books\n\n\nG.F. Franklin, J. Powell, A. Emani-Naeini, Feedback Control of Dynamical Systems, 7th Ed., Pearson.\nK. Oagata, Modern Control Engineering, 5th Ed., Pearson.\n\n\n\n\n\n\n\nAssignments (20%) Weekly homework assignments. Typically, each assignment will consist of four questions, out of which one or two randomly selected questions will be grader. The lowest two homework assignments will be dropped.\nLaboratory (20%) In-person weekly labs, to be conducted in groups of two. At the end of each lab, each group has to submit a lab report by filling in a Matlab Livescript template provided as part of the lab assignment.\nMid Term (20%) Closed book in-class exam. Oct 10 (during class time)\nFinal Exam (40%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term.\n\n\n\n\n\nAssignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\nDue to paucity of grading hours, only one or two randomly selected questions will be graded in each assignment.\nThe lowest two assignments and labs will be dropped. There will be no make-up for missed assignments and labs, even if it is for a valid reason. The whole point of dropping the lowest two assignments/labs is to reduce the administrative overhead of keeping track of such missed assignments/labs.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)\n\n\n\n\n\nThe course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures.\n\n\n\n\nAs the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students."
  },
  {
    "objectID": "index.html#general-information",
    "href": "index.html#general-information",
    "title": "Course Outline",
    "section": "",
    "text": "Instructor\n\n\nAditya Mahajan\nOffice Hours: TBD\n\n\nTeaching Assistants\n\n\nTBD\n\n\nLectures\n\n\n10:05am–11:25am Tuesday, Thursday (ENGMC 11)\n\n\nTutorials\n\n\n8:35am–10:25am Friday, (ENGMC 11)\n\n\n\nEveryone must be registered for the tutorial. Tutorials start from the second week of classes.\n\nLabs\n\n\n1:35pm-3:25pm Monday (TR 4090)\n\n\n\n\n3:35pm–5:25pm Monday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Tuesday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Wednesday (TR 4090)\n\n\n\n\n1:35pm–3:25pm Friday (TR 4090)\n\n\n\nEveryone must be registered for one lab. Labs start from the second week of classes.\nThe labs have to be done in groups of two. The groups will be formed in the first week of classs.\n\nPrerequisites\n\n\nECSE 206 (Fundamentals of Signals and Systems)\nECSE 210 (Electrical Circuits 2)\n\n\nCommunication\n\nUse the discussion board on myCourses for all questions related to the course. Only personal emails related to medical exceptions for missing a deliverable will be answered."
  },
  {
    "objectID": "index.html#graduate-attributes-and-learning-outcomes",
    "href": "index.html#graduate-attributes-and-learning-outcomes",
    "title": "Course Outline",
    "section": "",
    "text": "Attribute\nDescription\nStatus\n\n\n\n\nKB\nKnowledge Base for Engineering\nDeveloped\n\n\nPA\nProblem Analysis\nDeveloped\n\n\nIN\nInvestigation\nDeveloped\n\n\nDE\nDesign\nIntroduced\n\n\n\nUpon the successful completion of this course, the student students will have demonstrated the ability to:\n\nApply time- and frequency-domain tools to analyze linear time-invariant systems (KB, PA)\nDesign controllers based on state-space methods and lead-lag compensators to meet time-response specifications (KB, PA, DE)\nUse Matlab to analyze and synthesize controllers for linear time-invariant systems (DE, IN)"
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Course Outline",
    "section": "",
    "text": "Week\nMaterial Covered\n\n\n\n\n1\nReview of LTI Systems and Laplace Transforms  Reading: Nise Ch. 2\n\n\n2\nState-space modeling, canoncial forms, transfer function of state space models  Reading: William and Lawrence Ch. 1 and Ch. 2\n\n\n3\nPole Zero Plot Step response of first and second order systems, dominant pole approximation  Reading: Nise Ch. 4\n\n\n4\nMatrix exponential and solution of matrix differential equations, time response of state space models\n\n\n5\nPole placement, controllability, and state feedback  Reading: William and Lawrence Ch. 3 and Ch. 7\n\n\n6\nLuenberger observer, observability, output feedback, and separation principle  Reading: William and Lawrence Ch. 4 and Ch. 8\n\n\n7\nReview and Mid-Term\n\n\n8\nRouth-Hurwitz stability criterion. Special cases: zero in first column, row of zeros.  Reading: Nise Ch 6\n\n\n9\nBlock diagrams, rules for simplifying block diagrams, block diagram implementations of canonical forms of state space models\n\n\n10\nSystem type, steady state error, and disturbance rejection  Reading: Nise Ch. 7\n\n\n11\nBode and Nyquist Plots  Reading: Nise Ch. 10\n\n\n12\nNyquist stability criterion, gain and phase margins  Reading: Nise Ch. 10\n\n\n13\nLead-Lag compensators"
  },
  {
    "objectID": "index.html#lab-schedule",
    "href": "index.html#lab-schedule",
    "title": "Course Outline",
    "section": "",
    "text": "Week\nLab\n\n\n\n\n1\nNo Labs\n\n\n2\nLab 1 : Introduction to Matlab\n\n\n3\nLab 2 : Transfer functions and step response\n\n\n4\nLab 3 : Eigenvalues and eigenvectors\n\n\n5\nLab 4 : System identification via Step Response\n\n\n6\nLab 5 : State-Feedback Control (in simulation)\n\n\n7\nLab 6 : State-Feedback Control (in hardware)\n\n\n8\nLab 7 : Luenberger Observer for State Estimation\n\n\n9\nLab 8 : Output-Feedback Control\n\n\n10\nLab 9 : Block Diagram Reduction using Matlab\n\n\n11\nLab 10 : System identification via Bode Plots\n\n\n12\nLab 11 : Nyquist stability criterion\n\n\n13\nLab 12 : Lead-Lag compensator design"
  },
  {
    "objectID": "index.html#course-material",
    "href": "index.html#course-material",
    "title": "Course Outline",
    "section": "",
    "text": "Textbook\n\n\nN. Nise, Control Systems Engineering, 7th Ed., Wiley\nBoth paper and electronic versions of the textbook are available from the McGill bookstore. The electronic version of the book is also available from the publisher’s website and Google books.\n\n\n\n\nR.L. Williams II and D.A. Lawrence, Linear State‐Space Control Systems, Wiley.\nThe electronic version of the book is available the library.\n\n\nReference Books\n\n\nG.F. Franklin, J. Powell, A. Emani-Naeini, Feedback Control of Dynamical Systems, 7th Ed., Pearson.\nK. Oagata, Modern Control Engineering, 5th Ed., Pearson."
  },
  {
    "objectID": "index.html#evaluation",
    "href": "index.html#evaluation",
    "title": "Course Outline",
    "section": "",
    "text": "Assignments (20%) Weekly homework assignments. Typically, each assignment will consist of four questions, out of which one or two randomly selected questions will be grader. The lowest two homework assignments will be dropped.\nLaboratory (20%) In-person weekly labs, to be conducted in groups of two. At the end of each lab, each group has to submit a lab report by filling in a Matlab Livescript template provided as part of the lab assignment.\nMid Term (20%) Closed book in-class exam. Oct 10 (during class time)\nFinal Exam (40%) Closed book, in-person exam. Will be scheduled by the exam office and the dates will be announced later.\nThe final exam will cover all the material seen in the class during the term."
  },
  {
    "objectID": "index.html#marking-policy",
    "href": "index.html#marking-policy",
    "title": "Course Outline",
    "section": "",
    "text": "Assignments must be submitted electronically on myCourses as a PDF. You may write the assignments on paper and then scan them as a PDF (there are several such apps available for all phone platforms), or write on a tablet and convert to PDF, or type using a word processor.\nThere will no make-up examination for students who miss a mid-term.\n\nStudent who miss the exam due to a valid reason (see Faculty of Engineering policy) should notify the instructor within a week of the exam and provide necessary documentation.\nIf, and only if, proper documentation for a missed exam is presented, the marks for the missed exam will be shifted to the final exam.\nStudents who miss the mid-term exam for any other reason (e.g., no medical note, going to the exam at the wrong time, or on the wrong day, etc.) will get zero marks on the exam.\n\nAny request for reevaluation of a mid-term or an assignment must be made in writing within a week of its return. Note that requesting a re-grade will mean that you WHOLE assignment or exam will be re-graded.\nDue to paucity of grading hours, only one or two randomly selected questions will be graded in each assignment.\nThe lowest two assignments and labs will be dropped. There will be no make-up for missed assignments and labs, even if it is for a valid reason. The whole point of dropping the lowest two assignments/labs is to reduce the administrative overhead of keeping track of such missed assignments/labs.\n\n\nRight to submit in English or French written work that is to be graded.\n\nIn accord with McGill University’s Charter of Students’ Rights, students in this course have the right to submit in English or in French any written work that is to be graded.\n\nAcademic Integrity\n\nMcGill University values academic integrity. Therefore all students must understand the meaning and consequences of cheating, plagiarism and other academic offences under the Code of Student Conduct and Disciplinary Procedures (see McGill’s guide to academic honesty for more information).\nL’université McGill attache une haute importance à l’honnêteté académique. Il incombe par conséquent à tous les étudiants de comprendre ce que l’on entend par tricherie, plagiat et autres infractions académiques, ainsi que les conséquences que peuvent avoir de telles actions, selon le Code de conduite de l’étudiant et des procédures disciplinaires (pour de plus amples renseignements, veuillez consulter le guide pour l’honnêteté académique de McGill.)"
  },
  {
    "objectID": "index.html#course-delivery",
    "href": "index.html#course-delivery",
    "title": "Course Outline",
    "section": "",
    "text": "The course is taught in a “chalk and board” style; there will be no power point presentations. All students are expected to attend lectures and take notes. Partial notes on some of the material will be provided, but are not a substitute for the material covered in class.\n© Instructor-generated course materials (e.g., handouts, notes, summaries, exam questions) are protected by law and may not be copied or distributed in any form or in any medium without explicit permission of the instructor. Note that infringements of copyright can be subject to follow up by the University under the Code of Student Conduct and Disciplinary Procedures."
  },
  {
    "objectID": "index.html#additional-notes",
    "href": "index.html#additional-notes",
    "title": "Course Outline",
    "section": "",
    "text": "As the instructor of this course I endeavor to provide an inclusive learning environment. However, if you experience barriers to learning in this course, do not hesitate to discuss them with me or contact the office of Student Accessibility and Achievement.\nEnd-of-course evaluations are one of the ways that McGill works towards maintaining and improving the quality of courses and the student’s learning experience. You will be notified by e-mail when the evaluations are available. Please note that a minimum number of responses must be received for results to be available to students."
  },
  {
    "objectID": "safety-margins.html",
    "href": "safety-margins.html",
    "title": "Gain and Phase margins",
    "section": "",
    "text": "In practice, we never know the transfer function perfectly. So, it is important to understand the effect of model uncertainty on system stability. This is called the robustness of the stability of the system\nUsing Nyquist plots, we can define two qualitative measures of how “close” the system is from unstability. The usual practice is to measure this closeness in terms of gain margin (GM) and phase margin (PM).\nIn this section, we will restrict to minimal phase systems, that is systems that don’t have any poles or zeros in the ORHP. The general idea is that for a minimal phase system, the closer the Nyquist plot is to the \\(-1 + j0\\) point, the closer the system is to unstability. So, we can use the closeness of \\(G(jω)\\) to the \\(-1 + j0\\) point as a measure of stability of the system.\nThe GM and PM are easier to compute proxies for the distance of \\(G(jω)\\) from the \\(-1 + j0\\) point. The reason that GM and PM are used as a proxy is that a minimal phase system is stable if and only if both GM and PM are positive (this can be inferred from Nyquist stability criterion). However, GM and PM are a crude proxy for robustness, but we will not study more nuanced techniques in this course."
  },
  {
    "objectID": "safety-margins.html#the-gain-and-phase-margins",
    "href": "safety-margins.html#the-gain-and-phase-margins",
    "title": "Gain and Phase margins",
    "section": "1 The gain and phase margins",
    "text": "1 The gain and phase margins\n\n\n\n\n\n\nThe gain margin (GM) is the factor by which the gain can be increased before the system becomes unstable. It is the reciprocal of the magnitude of \\(|G(jω)|\\) at when the Nyquist plot intersects the negative real-axis.\n\n\n\nFormally, let \\(ω_{\\rm pc}\\) denote the phase crossover frequency, i.e., the frequency at which \\(\\angle G(jω) = -180^∘\\) (so the Nyquist plot intersects the negative real-axis at \\(ω_{\\rm pc}\\). Then, \\[\n  \\text{GM} = \\frac{1}{|G(jω_{\\rm pc})|}\n\\] Typically, we express GM in decibels (dB), so \\[\n  \\text{GM} = - 20\\log |G(jω_{\\rm pc})|\\, \\text{dB}\n\\]\n\n\n\n\n\n\nThe phase margin (PM) is the amount by which the phase can be increased before the system becomes unstable. It is the angle (as measured from the negative real-axis) at which the Nyquist plot intersects a circle of unit radium.\n\n\n\nFormally, let \\(ω_{\\rm gc}\\) denote the gain crossover frequency, i.e., the frequency at which \\(|G(jω)| = 1\\) (so the Nyquist plot intersects the unit circle at \\(ω_{\\rm gc}\\)). Then, \\[\n   \\text{PM} = 180^∘ + \\angle G(jω_{\\rm gc})\n\\] For non-minimal phase systems, there may be multiple frequencies at which \\(|G(jω)|=1\\) or \\(\\angle G(jω) = 180^∘\\). In such systems, we need to look at the highest and the lowest crossover frequencies. However, we will not study such systems in this course."
  },
  {
    "objectID": "safety-margins.html#some-examples",
    "href": "safety-margins.html#some-examples",
    "title": "Gain and Phase margins",
    "section": "2 Some examples",
    "text": "2 Some examples\nAlthough GM and PM are defined based on Nyquist stability criterion, we will use Bode plots to determine them. We present some examples to show how to “read” GM and PM from the Bode plot.\n\nExample 1 Find the crossover frequencies and GM and PM of the following system: \\[\n  G(s) = \\dfrac{100}{s(s+1)(s+100)}\n\\] using the straight-line approximation Bode plots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first draw the straight line approximation of the Bode plot.\n\nBode plotGM-PM Calcs\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first compute the GM. From the plot we see that the phase plot intersects the \\(-180 ^∘\\) line at \\(ω_{\\rm pc} = 10\\) rad/s (the yellow circle). At \\(ω_{\\rm pc} = 10\\), we have \\(|G(j ω_{\\rm pc})| = -40\\) dB. Thus, \\({\\rm GM} = 40\\) dB.\nWe now compute the PM. From the plot, we can see that the gain plot intersects the 0dB line at \\(ω = 1\\) (the green circle). Thus, \\(ω_{\\rm gc} = 1\\) rad/s. At \\(ω_{\\rm gc} = 1\\), we have \\(\\angle G(j ω_{\\rm gc}) = -135 ^∘\\). Thus, \\(\\text{PM} = 180 ^∘ - 135 ^∘ = 45 ^∘\\).\n\n\n\n\nExample 2 Find the crossover frequencies and GM and PM of the following system: \\[\n  G(s) = \\dfrac{10}{(s+1)(s+100)}\n\\] using the straight-line approximation Bode plots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first draw the straight line approximation of the Bode plot.\n\nBode plotGM-PM Calcs\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first compute the GM. From the plot we see that the phase plot intersects the \\(-180 ^∘\\) line at \\(ω_{\\rm pc} = 10^3\\) rad/s (the yellow point). At \\(ω_{\\rm pc} = 10^3\\), we have \\(|G(j ω_{\\rm pc})| = -100\\) dB. Thus, \\({\\rm GM} = 100\\) dB.\nWe now compute the PM. From the plot, we can see that the gain plot does not intersect the 0dB line!. Thus, the PM is infinity.\nNote: The above calculations are done using straight-line approximations. If we use the actual Bode plot, then the phase plot never actually reaches \\(-180 ^∘\\), so the GM is also infinity. Nonetheless, in this course, we will only work with straight-line approximations.\n\n\n\n\nExample 3 Find the crossover frequencies and GM and PM of the following system: \\[\n  G(s) = \\dfrac{100}{(s+10)}\n\\] using the straight-line approximation Bode plots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe first draw the straight line approximation of the Bode plot.\n\nBode plotGM-PM Calcs\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe first compute the GM. From the plot we see that the phase plot does not intersect the \\(-180 ^∘\\). Therefore, the GM is infinity.\nWe now compute the PM. From the plot, we can see that the gain plot intersects the 0dB line at \\(ω = 10^2\\). Thus, \\(ω_{\\rm gc} = 10^2\\) rad/s (the green circle). At \\(ω_{\\rm gc} = 10^2\\), we have \\(\\angle G(j ω_{\\rm gc}) = -90 ^∘\\). Thus, \\(\\text{PM} = 180 ^∘ - 90 ^∘ = 90 ^∘\\)."
  },
  {
    "objectID": "safety-margins.html#relationship-between-phase-margin-and-damping-ratio",
    "href": "safety-margins.html#relationship-between-phase-margin-and-damping-ratio",
    "title": "Gain and Phase margins",
    "section": "3 Relationship between phase margin and damping ratio",
    "text": "3 Relationship between phase margin and damping ratio\nWe can relate the PM of an open loop system with the damping ratio of the unity feedback closed loop system. This enables us to evaluate the percentage overshoot of the closed loop system the PM of the open loop system.\nConsider a unity feedback system with open-loop transfer function \\[\nG(s) = \\frac{ω_n^2}{s(s+2 ζ ω_n)}.\n\\] The closed-loop transfer function of this system is \\[\nT(s) = \\frac{G(s)}{(1 + G(s)} = \\frac{ω_n^2}{s^2 + 2 ζ ω_n s + ω_n^2}.\n\\] In order to find the PM, we first find the gain crossover frequency \\(ω_{\\rm gc}\\). We know that \\(|G(j ω_{\\rm gc})| = 1\\). Thus, \\[\n1 = |G(j ω_{\\rm gc})|^2 =\n\\frac{ω_n^2}{ω_{\\rm gc}^4 + 2 ζ^2 ω_n^2 ω_{\\rm gc}^2}\n\\] This gives a quadatic in \\(ω_{\\rm gc}\\). Solving for the positive root, we get \\[\nω_{\\rm gc} = ω_c \\sqrt{ 2 ζ^2 + \\sqrt{ 1 + 4 ζ^4 } }.\n\\] The phase at this frequency is \\[\n\\angle G(j ω_{\\rm gc}) = -90 - \\tan^{-1} \\frac{ω_{\\rm gc}}{2 ζ ω_n }\n\\] Thus, \\[\n\\bbox[5pt,border: 1px solid]\n{\\begin{align*}\n\\text{PM} &= 180 + \\angle G(j ω_{\\rm gc})\n= 90 - \\tan^{-1} \\frac{ω_{\\rm gc}}{2 ζ ω_n }\n= \\tan^{-1} \\frac{2 ζ ω_n}{ω_{\\rm rc}}\n\\\\\n&= \\tan^{-1} \\frac{2 ζ}{ \\sqrt{ -2 ζ^2 + \\sqrt{1 + 4 ζ^4} } }.\n\\end{align*}}\n\\]\nThis is a complicated looking formula, but we can simply plot it and read the values from the plot, shown in Figure 1.\n\n\n\n\n\n\nPlot.plot({\n    grid: true,\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.line(PM_values, {x:\"zeta\", y:\"PM\"})\n  ]\n})\n\n\n\n\n\n\n\n\nFigure 1: Phase margin as a function of damping ratio\n\n\n\n\nFigure 1 shows that PM increases with damping ratio. Thus, increasing the PM makes the response less oscillatory.\n\nExample 4 Find the PM needed to ensure that the percentage overshoot is less than 10%."
  },
  {
    "objectID": "nyquist-stability.html",
    "href": "nyquist-stability.html",
    "title": "Nyquist Stability Criterion",
    "section": "",
    "text": "Nyquist stability criterion is a graphical method to check the stability of a feedback control system. Consider the following closed loop transfer function\nFor simplicity, we do not write the argument \\((s)\\) in the discussion below and write the closed loop TF as \\[ T(s) = \\frac{G}{1 + GH}. \\]\nObserve that \\(T(s)\\) is stable if\nNow suppose \\[\nG(s) = \\frac{N_G(s)}{D_G(s)}\n\\quad\\text{and}\\quad\nH(s) = \\frac{N_H(s)}{D_H(s)}\n\\] Then, \\[ GH = \\frac{N_G N_H}{D_G D_H}. \\] Hence, \\[ 1 + GH = \\frac{D_G D_H + N_G N_H}{D_G D_H}. \\] Thus, \\(1 + GH\\) and \\(GH\\) have the same poles.\nWe now ask the following question: If we know the number of poles of \\(1 + GH\\) in ORHP (e.g., by knowing the number of poles of GH in ORHP), can we fine the number of zeros of \\(1 + GH\\) in ORHP? If so, we can determine the stability of \\(T(s)\\).\nIt turns our that there is an answer to this question using what is called Cauchy’s Argument Principle, which we explain below."
  },
  {
    "objectID": "nyquist-stability.html#cauchys-argument-principle",
    "href": "nyquist-stability.html#cauchys-argument-principle",
    "title": "Nyquist Stability Criterion",
    "section": "1 Cauchy’s Argument Principle",
    "text": "1 Cauchy’s Argument Principle\nWe can think of any transfer function \\(G\\) as a mapping from \\(\\mathbb{C}\\) (the set of complex numbers) to \\(\\mathbb{C}\\).\n\nExample 1 Consider the transfer function \\[\nG(s) = \\frac{s+1}{s+2}.\n\\] This is a function from \\(\\mathbb{C}\\) to \\(\\mathbb{C}\\) that maps every point in the “\\(s\\)-plane” to a point in the “\\(G\\)-plane”. For instance, \\[\nG \\colon -1 + j \\mapsto \\frac{j}{1 + j} = \\frac{j+1}{2}.\n\\]\nPlot.plot({\n  grid: true,\n  clip: true,\n  y: { domain: [-2, 2] },\n  x: { domain: [-2, 2] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([{σ: -1, jω: 1}], {x: \"σ\", y: \"jω\", r: 5, fill: \"black\"}),\n  ]\n})\nPlot.plot({\n  grid: true,\n  clip: true,\n  y: { domain: [-2, 2] },\n  x: { domain: [-2, 2] },\n  marks: [\n    // Axes\n    Plot.ruleX([0]),\n    Plot.ruleY([0]),\n    // Data\n    Plot.dot([{σ: 0.5, jω: 0.5}], {x: \"σ\", y: \"jω\", r: 5, fill: \"black\"}),\n  ]\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) \\(s\\)-plane\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \\(G\\)-plane\n\n\n\n\n\n\n\nFigure 2: A transfer function maps every point in the \\(s\\)-plane to a point in the \\(G\\)-plane\n\n\n\n\nIf we exclude the poles of \\(G\\), then \\(G\\) is a continuous function. Thus, if we take any curve in the \\(s\\)-plane that doesn’t pass through a pole of \\(G\\), then \\(G\\) will map that to a curve in the \\(G\\)-plane. As an instance, Figure 3 shows how a curve in \\(s\\)-plane gets mapped to a curve in the \\(G\\)-plane for Example 1\n\n\n\n\n\n\n\nFigure 3: A curve in \\(s\\)-plane for Example 1\n\n\n\n\nA curve which starts and ends at the same point is called a contour. So if we take a contour in \\(s\\)-plane that doesn’t pass through a pole of \\(G\\), then \\(G\\) will map that contour to a countor in the \\(G\\)-plane. As an instance, Figure 4 shows how a contour in \\(s\\)-plane gets mapped to a countour in the \\(G\\)-plane for Example 1.\n\n\n\n\n\n\n\nFigure 4: A contour in \\(s\\)-plane for Example 1 that does not encircle any poles or zeros\n\n\n\n\nWe will think of contours as having a direction and, by convention, assume that the contour in the \\(s\\)-plane is clockwise. Then, depending on the TF \\(G\\), the contour in the \\(G\\)-plane may be clockwise or counter-clockwise. As can be seen in Figure 4, a clockwise contour in \\(s\\)-plane gets mapped to a clockwise contour in the \\(G\\)-plane.\nAnother important aspect is whether the contour in the \\(G\\)-plane encircles the origin. Let’s consider a few other contours in the \\(s\\)-plane and their mappings in the \\(G\\)-plane in Figure 5, Figure 6, and Figure 7.\n\n\n\n\n\n\n\nFigure 5: A contour in \\(s\\)-plane for Example 1 that encircles a zero\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: A contour in \\(s\\)-plane for Example 1 that encircles a poles and a zero\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: A contour in \\(s\\)-plane for Example 1 that encircles a pole\n\n\n\n\nWe can understand what is happening using the Cauchy’s Argument Principle.\n\n\n\n\n\n\nCauchy’s Argument Principle\n\n\n\n\\[ N = Z - P \\]\nwhere\n\n\\(N\\): number of encirclements of origin in the \\(G\\)-plane in clockwise direction (encirclements in the counter-clockwise direction are counted with negative signs)\n\\(Z\\): number of zeros inside the contour in the \\(s\\)-plane\n\\(P\\): number of poles inside the contour in the \\(s\\)-plane\n\n\n\nWe check Cauchy’s Argument principle for the examples presented earlier.\n\n\n\nContour\n\\(N\\)\n\\(Z\\)\n\\(P\\)\n\n\n\n\nFigure 4\n0\n0\n0\n\n\nFigure 5\n1\n1\n0\n\n\nFigure 6\n0\n1\n1\n\n\nFigure 7\n-1\n0\n1"
  },
  {
    "objectID": "nyquist-stability.html#nyquist-stability-criterion",
    "href": "nyquist-stability.html#nyquist-stability-criterion",
    "title": "Nyquist Stability Criterion",
    "section": "2 Nyquist Stability Criterion",
    "text": "2 Nyquist Stability Criterion\nNow suppose that we start with the contour in \\(s\\)-plane shown in Figure 8 and the limit as \\(R \\to ∞\\). Then, the contour encloses the entire \\(s\\)-plane.\n\n\n\n\n\n\n\nFigure 8: A contour in \\(s\\)-plane\n\n\n\n\nThe \\(G\\)-map of this contour traces the function \\(G(s)\\) for \\(s\\) ranging from \\(0 - j ∞\\) to \\(0 + j ∞\\) (and then a circle at infinity). This is precisely the Nyquist plot of \\(G\\)!\nNow let’s go back to our feedback control system:\n\n\n\n\n\n\nFigure 9: A feedback control system\n\n\n\nConsider the TF \\(1 + GH\\). For example, for \\(G(s) 1/(s+2)\\) and \\(H(s) = 1\\), the map of \\(1 + GH = 1 + 1/(s+2)\\) is shown in Figure 10\n\n\n\n\n\n\n\nFigure 10: Mapping of the contour in \\(s\\)-plane to the \\(1 + GH\\)-plane. Note that there is slight “kink” in the curve in the \\(1+GH\\) plane near \\(1 + j0\\) which corresponds to the semi-circle of radius \\(R\\) in the \\(s\\)-plane. This kink will collapse to a point as \\(R \\to ∞\\).\n\n\n\n\nThen by Cauchy’s argument principle, we have \\[\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Zeros of $1 + GH$} \\\\\n\\text{in ORHP}\n\\end{array}} =\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{c}\n\\text{Number of clockwise} \\\\\n\\text{encirclements of origin}\\\\\n\\text{in NP of $1 + GH$}\n\\end{array}}\n+\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{c}\n\\text{Number of poles} \\\\\n\\text{of $1+GH$}\\\\\n\\text{in the ORHP}\n\\end{array}}\n\\]\nThus, we have answered our original question. The only catch is that we need to draw the Nyquist plot of \\(1 + GH\\) (rather than \\(GH\\)). But this can be remidied easily by the observation that the Nyquist plot of \\(1 + GH\\) is the Nyquist plot of \\(GH\\) shifted by \\(1\\). Therefore, \\[\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Number of clockwise encirclements}\\\\\n\\text{of \\textbf{origin} in the} \\\\\n\\text{Nyquist plot of $1 + GH$}\n\\end{array}} =\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Number of clockwise encirclements}\\\\\n\\text{of \\textbf{$-1 + j0$} in the} \\\\\n\\text{Nyquist plot of $GH$}\n\\end{array}}\n\\]\nMoreover, we have already shown that \\[\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Zeros of $1 + GH$} \\\\\n\\text{in ORHP}\n\\end{array}} =\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Number of poles} \\\\\n\\text{of $T$ in ORHP}\n\\end{array}}\n\\] and \\[\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{c}\n\\text{Number of poles} \\\\\n\\text{of $1+GH$}\\\\\n\\text{in the ORHP}\n\\end{array}} =\n\\bbox[5pt,border: 1px solid]\n{\\begin{array}{@{}c@{}}\n\\text{Number of poles}\\\\\n\\text{of $GH$ in ORHP}\n\\end{array}}\n\\]\nThis gives us the Nyquist formula.\n\n\n\n\n\n\nNyquist formula\n\n\n\n\\[ Z = N + P \\] where\n\n$Z = $ number of zeros of \\(1 + GH\\) in ORHP (which equals poles of \\(T\\) in ORHP)\n$N = $ number of clockwise encirclements of \\(-1\\) in the Nyquist plot of \\(GH\\)\n$P = $ number of poles of \\(1 + GH\\) in ORHP (which equals number of poles of \\(GH\\) in ORHP)\n\n\n\nThus, we can proceed as follows:\n\nCount the number of poles of \\(GH\\) in the ORHP (e.g., using Routh-Hurwitz). This gives the value of \\(P\\)\nDraw the Nyquist plot of \\(GH\\) and count the number of clockwise encirclements of \\(-1\\). This gives the value of \\(N\\).\n\\(Z = N + P\\) gives the number of poles of \\(T\\) in the ORHP\nSystem is stable if and only if \\(Z = 0\\)."
  },
  {
    "objectID": "nyquist-stability.html#examples",
    "href": "nyquist-stability.html#examples",
    "title": "Nyquist Stability Criterion",
    "section": "3 Examples",
    "text": "3 Examples\n\nExample 2 Consider \\[\nGH(s) = \\dfrac{K}{(s-2)(s+6)(s+8)}\n\\] for different values of \\(K\\).\n\n\n\n\n\n\n\nCondition for stability\n\n\n\n\n\nFrom observation we not that system \\(GH\\) is stable and \\(P = 0\\). Thus, for stability (i.e., \\(Z = 0\\)), we want \\(N = 0\\), i.e., the Nyquist plot should have zero encirclements of \\(-1\\).\n\n\n\n\n\\(K=50\\)\\(K=100\\)\\(K=336\\)\\(K=500\\)\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\nExample 3 Consider \\[\nGH(s) = K \\dfrac{(s+1)(s+2)}{(s-3)(s-4)}\n\\] for different values of \\(K\\).\n\n\n\n\n\n\n\nCondition for stability\n\n\n\n\n\nFrom observation we not that system \\(GH\\) is unstable and \\(P = 2\\). Thus, for stability (i.e., \\(Z = 0\\)), we want \\(N = -2\\), i.e., the Nyquist plot should have two (counter-clockwise) encirclements of \\(-1\\).\n\n\n\n\n\\(K=1\\)\\(K=10\\)\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane. The kink around \\(10+j\\) in the \\(GH\\)-plane will become a point as \\(R \\to ∞\\) in the \\(s\\)-plane.\n\n\n\n\n\n\n\nMapping of the contour in \\(s\\)-plane to \\(GH\\) plane. The kink around \\(10+j\\) in the \\(GH\\)-plane will become a point as \\(R \\to ∞\\) in the \\(s\\)-plane."
  },
  {
    "objectID": "nyquist-stability.html#values-of-gain-k-for-stability",
    "href": "nyquist-stability.html#values-of-gain-k-for-stability",
    "title": "Nyquist Stability Criterion",
    "section": "4 Values of gain \\(K\\) for stability",
    "text": "4 Values of gain \\(K\\) for stability\n\n\n\n\n\n\nRather than redrawing the Nyquist plot for different values of \\(K\\), we can draw the Nyquist plot for \\(K=1\\) and count the encirclements of \\(-1/K\\). This also enables us to find the values of \\(K\\) for which the system is stable.\n\n\n\nWe illustrate this via some examples.\n\nExample 4 Find the values of \\(K\\) for which the following system is stable. \\[\nGH(s) = \\dfrac{K}{(s+1)(s+2)}.\n\\] The Nyquist plot for \\(K=1\\) is shown in Figure 11.\n\n\n\n\n\n\n\nFigure 11: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst observe that \\(P=0\\). Therefore, for stability (i.e., for \\(Z=0\\)), we need \\(N=0\\). The Nyquist plot intersets the \\(σ\\)-axis at \\(0\\) and \\(0.5\\). So, we have the following cases.\n\n\n\nCase\nRange for \\(K\\)\n\\(N\\)\nStable?\n\n\n\n\n\\(-∞ &lt; -\\dfrac 1K &lt; 0\\)\n\\((0, ∞)\\)\n\\(0\\)\nYes\n\n\n\\(0 &lt; -\\dfrac 1K &lt; \\dfrac 12\\)\n\\((-∞,-2)\\)\n\\(1\\)\nNo\n\n\n\\(\\dfrac 12 &lt; -\\dfrac 1K &lt; ∞\\)\n\\((-2,0)\\)\n\\(0\\)\nYes\n\n\n\nThus, the system is stable for \\(K &gt; 0\\) or \\(K \\in (-2,0)\\).\n\n\n\n\nExample 5 Find the values of \\(K\\) for which the following system is stable. \\[\nGH(s) = \\dfrac{6K}{(s+1)(s+2)(s+3)}.\n\\] The Nyquist plot for \\(K=1\\) is shown in Figure 12, which intersets the \\(σ\\)-axis at \\(-0.1\\), \\(0\\), \\(1\\).\n\n\n\n\n\n\n\nFigure 12: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst observe that \\(P=0\\). Therefore, for stability (i.e., for \\(Z=0\\)), we need \\(N=0\\). The Nyquist plot intersets the \\(σ\\)-axis at \\(-0.1\\), \\(0\\), \\(1\\). So, we have the following cases.\n\n\n\nCase\nRange for \\(K\\)\n\\(N\\)\nStable?\n\n\n\n\n\\(-∞ &lt; -\\dfrac 1K &lt; -\\dfrac 1{10}\\)\n\\((0, 10)\\)\n\\(0\\)\nYes\n\n\n\\(-\\dfrac{1}{10} &lt; -\\dfrac 1K &lt; 0\\)\n\\((10, ∞)\\)\n\\(2\\)\nNo\n\n\n\\(0 &lt; -\\dfrac 1K &lt; 1\\)\n\\((-∞,-1)\\)\n\\(1\\)\nNo\n\n\n\\(1 &lt; -\\dfrac 1K &lt; ∞\\)\n\\((-1,0)\\)\n\\(0\\)\nYes\n\n\n\nThus, the system is stable for \\(K &gt; 10\\) or \\(K \\in (-1,0)\\).\n\n\n\n\nExample 6 Find the values of \\(K\\) for which the following system is stable. \\[\nGH(s) = \\dfrac{K(s-1)}{s^3 + s^2 - s + 2}\n= \\dfrac{K(s-1)}{(s+2)(s^2-s+1)}\n\\] The Nyquist plot for \\(K=1\\) is shown in Figure 12, which intersets the \\(σ\\)-axis at \\(-2/3\\), \\(-1/2\\), \\(0\\).\n\n\n\n\n\n\n\nFigure 13: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst observe that \\(s^2 - s + 1\\) has two ORHP. Thus, \\(P=0\\) and for stability (i.e., for \\(Z=0\\)), we need \\(N=-2\\). The Nyquist plot intersets the \\(σ\\)-axis at \\(-2/3\\), \\(-1/2\\), \\(0\\). So, we have the following cases.\n\n\n\n\n\n\n\n\n\nCase\nRange for \\(K\\)\n\\(N\\)\nStable?\n\n\n\n\n\\(-∞ &lt; -\\dfrac 1K &lt; -\\dfrac 2{3}\\)\n\\((0, 1.5)\\)\n\\(0\\)\nNo\n\n\n\\(-\\dfrac{2}{3} &lt; -\\dfrac 1K &lt; -\\dfrac{1}{2}\\)\n\\((1.5, 2)\\)\n\\(-2\\)\nYes\n\n\n\\(-\\dfrac{1}{2} &lt; -\\dfrac 1K &lt; 0\\)\n\\((2,∞)\\)\n\\(-1\\)\nNo\n\n\n\\(0 &lt; -\\dfrac 1K &lt; ∞\\)\n\\((-∞,0)\\)\n\\(0\\)\nNo\n\n\n\nThus, the system is stable for \\(K \\in (1.5, 2)\\)."
  },
  {
    "objectID": "nyquist-stability.html#poles-on-the-j-ω-axis",
    "href": "nyquist-stability.html#poles-on-the-j-ω-axis",
    "title": "Nyquist Stability Criterion",
    "section": "5 Poles on the \\(j ω\\)-axis",
    "text": "5 Poles on the \\(j ω\\)-axis\nRecall that the Nyquist stability criterion relies on Cauchy’s argument principle, which is valid when the contour in the \\(s\\)-plane doesn’t pass through a pole. This assumption is violated if there is a pole on the \\(jω\\)-axis, as illustrated in Figure 14 which shows the Nyquist plot of \\(GH(s) = 1/(s(s+1))\\).\nThe TF has no poles in the ORHP, so \\(P = 0\\). For stability (i.e., \\(Z=0\\)), we need \\(N = 0\\), i.e., there should be zero encirclements of \\(-1\\) in the \\(GH\\)-plane.\n\n\n\n\n\n\n\nFigure 14: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\nIn this case, the Nyquist plot is not a closed curve so we cannot count the number of encirclements of \\(-1\\). We need to fix this manually. Consider the contour shown in Figure 15, which skips the pole at origin by travesing a small semi-circle \\(r e^{j θ}\\), \\(θ \\in [-π/2, π/2]\\), and then take \\(r \\to 0\\)\n\n\n\n\n\n\n\nFigure 15: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane. Although it is not visitble at this scale, the contour skips the pole at origin by going around a semi-circle of radius \\(0.2\\).\n\n\n\n\nHowever, standard software for drawing Nyquist plot does not circumvent the poles. In this section, we will learn how to determine how to close the contour of Figure 14.\nRecall that the transfer function is \\[\n  GH(s) = \\dfrac{1}{s(s+1)}.\n\\] For any point along the small circle around the pole, we have \\(s = r e^{j θ}\\). Thus, \\[\n  GH(r e^{j θ}) = \\dfrac{1}{r e^{j θ}(1 + re^{j θ})}\n  \\approx\n  \\dfrac{1}{r e^{j θ}}\n\\] Thus, we have the following:\n\n\n\n\\(θ\\)\n\\(\\lvert GH(re^{j θ}) \\rvert\\)\n\\(\\angle GH(r e^{j θ})\\)\n\n\n\n\n\\(-90^∘\\)\n\\(\\dfrac 1r\\)\n\\(90^∘\\)\n\n\n\\(-45^∘\\)\n\\(\\dfrac 1r\\)\n\\(45^∘\\)\n\n\n\\(0^∘\\)\n\\(\\dfrac 1r\\)\n\\(0^∘\\)\n\n\n\\(45^∘\\)\n\\(\\dfrac 1r\\)\n\\(-45^∘\\)\n\n\n\\(90^∘\\)\n\\(\\dfrac 1r\\)\n\\(-90^∘\\)\n\n\n\nThis is consistent with the contour in Figure 15. Thus, there are no encirclements of \\(-1\\) in the \\(GH\\)-plane. Hence, \\(N = 0\\), which implies, \\(Z = 0\\). Hence, the system is stable.\nWe now do a few practise exercises.\n\nExercise 1 Consider \\(GH = \\dfrac{K}{s(s+1)(2s+1)}\\). The Nyquist plot for \\(K=1\\) is shown in Figure 16. Find the values of \\(K\\) for which the closed loop system is stable.\n\n\n\n\n\n\n\nFigure 16: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\nNote that the Nyquist plot interests the \\(σ\\)-axis at two points: \\(s = 0\\) and \\(s = -2/3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe simplify \\(GH(s)\\) along the contour \\(s = re^{j θ}\\): \\[\n  GH(re^{j θ}) = \\dfrac{K}{re^{j θ}(re^{j θ} + 1)(2re^{j θ} + 1)}\n  \\approx\n  \\dfrac{K}{re^{j θ}}.\n\\] Thus, we have the following\n\n\n\n\\(θ\\)\n\\(\\lvert GH(re^{j θ}) \\rvert\\)\n\\(\\angle GH(r e^{j θ})\\)\n\n\n\n\n\\(-90^∘\\)\n\\(\\dfrac Kr\\)\n\\(90^∘\\)\n\n\n\\(-45^∘\\)\n\\(\\dfrac Kr\\)\n\\(45^∘\\)\n\n\n\\(0^∘\\)\n\\(\\dfrac Kr\\)\n\\(0^∘\\)\n\n\n\\(45^∘\\)\n\\(\\dfrac Kr\\)\n\\(-45^∘\\)\n\n\n\\(90^∘\\)\n\\(\\dfrac Kr\\)\n\\(-90^∘\\)\n\n\n\nThe the two arms of the Nyquist plot in Figure 16 are connected via a semi-circle in closewise direction, as shown in Figure 17\n\n\n\n\n\n\n\nFigure 17: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane. Although it is not visitble at this scale, the contour skips the pole at origin by going around a semi-circle of radius \\(0.2\\).\n\n\n\n\nFirst observe that \\(P = 0\\). Therefore, for stability (i.e. \\(Z = 0\\)), we need \\(N = 0\\). The Nyquist plot intersects the \\(σ\\)-axis at \\(0\\) and \\(-2/3\\). So, we have the following cases.\n\n\n\nCase\nRange for \\(K\\)\n\\(N\\)\nStable?\n\n\n\n\n\\(-∞ &lt; -\\dfrac 1K &lt; -\\dfrac{2}{3}\\)\n\\((0, 1.5)\\)\n\\(0\\)\nYes\n\n\n\\(-\\dfrac{2}{3} &lt; -\\dfrac 1K &lt; 0\\)\n\\((1.5, ∞)\\)\n\\(2\\)\nNo\n\n\n\\(0 &lt; -\\dfrac 1K &lt; ∞\\)\n\\((-∞,0)\\)\n\\(1\\)\nNo\n\n\n\nThus, the system is stable for \\(K \\in (0, 1.5)\\).\n\n\n\n\nExercise 2 Consider \\(GH = \\dfrac{K}{s^2(s+1)}\\). Find the values of \\(K\\) for which the closed loop system is stable.\n\n\n\n\n\n\n\nFigure 18: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane.\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe simplify \\(GH(s)\\) along the contour \\(s = re^{j θ}\\): \\[\n  GH(re^{j θ}) = \\dfrac{K}{r^2e^{j 2θ}(re^{j θ} + 1)}\n  \\approx\n  \\dfrac{K}{r^2e^{j 2θ}}.\n\\] Thus, we have the following\n\n\n\n\\(θ\\)\n\\(\\lvert GH(re^{j θ}) \\rvert\\)\n\\(\\angle GH(r e^{j θ})\\)\n\n\n\n\n\\(-90^∘\\)\n\\(\\dfrac K{2r^2}\\)\n\\(180^∘\\)\n\n\n\\(-45^∘\\)\n\\(\\dfrac K{2r^2}\\)\n\\(90^∘\\)\n\n\n\\(0^∘\\)\n\\(\\dfrac K{2r^2}\\)\n\\(0^∘\\)\n\n\n\\(45^∘\\)\n\\(\\dfrac K{2r^2}\\)\n\\(-90^∘\\)\n\n\n\\(90^∘\\)\n\\(\\dfrac K{2r^2}\\)\n\\(-180^∘\\)\n\n\n\nThe the two arms of the Nyquist plot in Figure 18 are connected via a circle in closewise direction, as shown in Figure 19.\n\n\n\n\n\n\n\nFigure 19: Mapping of the contour in \\(s\\)-plane to \\(GH\\) plane. Although it is not visitble at this scale, the contour skips the pole at origin by going around a semi-circle of radius \\(0.2\\).\n\n\n\n\nThus, irrespective of the value of \\(K\\), \\(N=2\\). Since \\(P = 0\\), we have \\(Z = 0\\). Thus, the system is not stable for any values of \\(K\\).\n\n\n\n\n\n\n\n\n\nWhat about other poles on the \\(j ω\\) axis\n\n\n\nConsider the TF \\[\nGH(s) = \\dfrac{1}{(s^2 + 1)(s+1)}.\n\\]\nIn this case, there are two poles on the \\(j ω\\) axis. We can follow the same idea as before and circumvent them via small semi-circles of radius \\(r\\) at \\(\\pm j\\). However, we will not study this case in detail in class."
  }
]